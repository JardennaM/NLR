











Real-time detection of drones at large distances with 25 megapixel cameras



















 





















































Sign In


View Cart 


        Help
    









  

Email or Username
Forgot your username?




Password
Forgot your password?


 Show




Remember Email/Username on this computer


Remember Password






Please wait...




                No SPIE account? Create an account 
Institutional Access:Sign in with your institutional credentials

 













                    CONFERENCE PROCEEDINGS
                



        Papers
    



        Presentations
    







        Journals




        Advanced Photonics
    





        Journal of Applied Remote Sensing
    



        Journal of Astronomical Telescopes, Instruments, and Systems
    



        Journal of Biomedical Optics
    



        Journal of Electronic Imaging
    



        Journal of Medical Imaging
    



        Journal of Micro/Nanolithography, MEMS, and MOEMS
    



        Journal of Nanophotonics
    



        Journal of Photonics for Energy
    



        Neurophotonics
    



        Optical Engineering
    





        Ebooks
    




Advanced Search >








                            Home
                        
>

                            Proceedings
                        
>

                            Volume 10799
                        
>
Article





















        Translator Disclaimer
    




                        You have requested a machine translation of selected content from our databases. This functionality is provided solely for your convenience and is in no way intended to replace human translation. Neither SPIE nor the owners and publishers of the content make, and they explicitly disclaim, any express or implied representations or warranties of any kind, including, without limitation, representations and warranties as to the functionality of the translation feature or the accuracy or completeness of the translations.
                    

                        Translations are not retained in our system. Your use of this feature and the translations is subject to all use restrictions contained in the Terms and Conditions of Use of the SPIE website.
                    









4 October 2018
Real-time detection of drones at large distances with 25 megapixel cameras



 Thomas Perschke;                         Konrad Moren;                         Thomas Müller 


Author Affiliations +

 Thomas Perschke,1 Konrad Moren,1 Thomas Müller1 
 1Fraunhofer IOSB (Germany)
				




Proceedings Volume 10799, Emerging Imaging and Sensing Technologies for Security and Defence III; and Unmanned Sensors, Systems, and Countermeasures; 107990N (2018) https://doi.org/10.1117/12.2324678Event: SPIE Security + Defence, 2018, Berlin, Germany














                                ARTICLE
                            





                                SECTIONS
 


1. INTRODUCTION
2. METHOD
2.1 Hardware Setup
2.1.1 Camera
2.1.2 Framegrabber
2.1.3 FPGA board
2.1.4 PC
2.1.5 System setup
2.1.6 GPGPU
2.1.7 Software stack
2.2 Algorithm
2.2.1 Detection
2.2.2 Clustering
2.3 FPGA Implementation
2.3.1 Camera interface
2.3.2 Detection implementation
2.3.3 Clustering implementation
2.4 GPGPU Implementation
2.4.1 Detection implementation
2.4.2 Clustering implementation
2.5 Data distribution
2.6 Tracking
3. RESULT AND DISCUSSION
3.1 FPGA Performance
3.2 GPGPU Performance
3.2.1 Experimental Design
3.2.2 Performance Results
3.3 Tracking Perfomance
3.4 Range limitations
4. CONCLUSION





                                FIGURES & TABLES
                            





                                REFERENCES
                            





                                CITED BY
                            




DOWNLOAD PDF 

SAVE TO MY LIBRARY














 
 PERSONAL SIGN IN
 Full access may be available with your subscription

 

Email or Username
Forgot your username?




Password
Forgot your password?


Show




Remember Email/Username on this computer


Remember Password





                    No SPIE account? Create an account 
Institutional Access:Sign in with your institutional credentials

 









 SUBSCRIBE TO DIGITAL LIBRARY



50 downloads per 1-year subscription


Members: $195


Non-members: $335
ADD TO CART



25 downloads per 1 - year subscription


Members: $145


Non-members: $250
ADD TO CART





 PURCHASE SINGLE ARTICLE



Includes PDF, HTML & Video, when available


Members: $14.40


Non-members: $18.00
ADD TO CART














This will count as one of your downloads.
You will have access to both the presentation and article (if available).



DOWNLOAD NOW







 
 This content is available for download via your institution's subscription. To access this item, please sign in to your personal account.
 

Email or Username
Forgot your username?




Password
Forgot your password?


 Show




Remember Email/Username on this computer


Remember Password





                No SPIE account? Create an account 

 








My Library









                            You currently do not have any folders to save your paper to! Create a new folder below.
                        


                            Create New Folder
                        






                            SAVE >
                        








 

                    Folder Name
                




                    Folder Description
                





                    SAVE
                
















Abstract

To detect drones in real-time at large distances, using high-resolution visual(VIS) cameras, requires a careful system design. In our setup we use four 10-bit 25 Megapixels@25fps cameras to detect drones. This means a theoretical raw data throughput of about 33 GB/sec for the system from the VIS cameras alone. We implemented a small-object detection algorithm on two different platforms. The algorithm is based on a point-detector with a subsequent clustering step. One platform is a Xilinx Kintex-7 FPGA, the other a Nvidia GeForce GTX 1080 GPU. We explain, how the small-object detection algorithm, based on a software reference implementation, has been ported to a FPGA version. It is shown, that the FPGA implementation of the point-detector reaches the optimal throughput and we show, that the clustering can be done in a streaming fashion. Using the FPGA implementation, the camera can be used in free-running mode, processing the camera data in real time. The CUDA implementation of the algorithm shows, that the computing capabilities of modern GPGPUs allow an easy port of the algorithm to this platform to reach real-time performance with 68fps. The use of four high-resolution cameras requires a careful overall system design. We present our hardware system design and the central data distribution system. As the small-object detections may be used by different processes like the tracking process or display processes, a fast, safe and robust distribution system must be provided. We describe our approach using the Boost Interprocess library to provide a portable data distribution system.





1.INTRODUCTIONThe availability of low priced unmanned aerial vehicles(UAVs) make them a ubiquitous phenomenon today. These drones are used by private users to film the environment, they are used by fireman and save and rescue organizations for disaster management, agricultural industries can check the status of the crop and there are many more applications. Unfortunately, one is also confronted with negative aspects. Drones can endanger the civilian air traffic on airports, they can be used to violate the private-sphere of us citizens, or even worse, can be used for terrorist attacks.1 This led to an increasing market of drone detection and drone thread neutralization technologies.A SANDIA report2 gives an comprehensive list of the current technologies to detect and identify low, slow, small (LSS) UAVs which we show here as overview of current technologies.1. Radaradvantages:3D position information, large range detections over kilometer distance, well established technology, easy hemisphere coveragedisadvantages:Possible bad radar cross sections of drones, active system2. Passive optics(a) visual(VIS) imagersadvantages:broad range of cameras, high pixel resolution, flexible optics solutionsdisadvantages:2D position information, needs active illumination, atmospheric effects, susceptible to clutter(b) short-wave infrared (SWIR) imagersadvantages:night vision capability, uncooled microbolometer cameras are relatively cheapdisadvantages:2D position information, atmospheric effects, lower resolution, drone signature often low(c) mid-wave infrared (MWIR) and long-wave onfrared (LWIR) imagersadvantages:night vision capabilitydisadvantages:2D position information, lower resolution, expensive cooled cameras and optics3. Active optics(a) LIDAR, gated-viewingadvantages:3D position information, long range detections over kilometerdisadvantages:active system, difficult 2π coverage, clutter4. Acousticsadvantages:inexpensive, passivedisadvantages:unknown detection range, may require signature data base, susceptible to wind5. EM emissionsadvantages:inexpensive, most drones emit detectable signalsdisadvantages:cannot detect quiet threadThe vastly varying characteristics of drones like material composition, size and speed makes it unlikely, that it will be enough to use a single detection/identification technology. A careful selection of technologies is required to achieve a high detection/identification performance with low false alarm rates. As many drones can achieve high speeds, it will be extremely advantageous, to detect them at large distances. This paper will show, how visual cameras can be used for this task.A recent Sandia report3 evaluated VIS, SWIR, MWIR and LWIR imagers for their applicability to drone detection and assessment. They identify the LWIR and VIS as best spectral bands. However, the report is skeptical that the clutter problem for VIS systems makes them suitable drone detection devices.There are different video drone detection approaches. Ganti et al.4 use blob detection and SURF features with a 640x480 pixel camera. Rozantsev et al.5 detect drones against complex backgrounds in small field-of-view scenes of a moving camera for a classification task. Hu et al.6 developed a drone detection algorithm using horizon finding, motion feature extraction, blob analysis and coherence analysis. A different approach is the use of active laser range gated viewing sensors7 in combination with passive color cameras to get a 3D position information.These approaches assume, that the drone shows some features in the image. In this paper we want to explore, how visual cameras can be useful in drone detection systems, if the drone is in a distance range, where it appears almost point-like.2.METHODUsing passive VIS or infrared cameras for drone detection has some challenges. In the general case, the system must cover a 2π hemnisphere while maintaining a high enough instantaneous field of view(IFOV). This can be realized with a few high-resolution cameras with wide-angle lenses or with a larger number of low-resolution cameras.8 To reduce the required hardware resources, our system uses the former approach.We use the optical setup for two different tasks. The first task is the detection and tracking of far away point-like objects as potential thread. The far-range is theoretically limited by the the IFOV and the drone dimensions. However, it is highly likely, that the atmospheric conditions will reduce the effective detection range. The second task is the detection of extended objects in the near- and mid-range. A possible detection and tracking algorithm for this range is shown by T. Müller.9Our visual system is part of a larger drone detection system with a visual verification sensor on a pan-tilt unit, laser range finders, acoustic detectors and EM detectors. The tracked objects are distributed to the main system, where objects can be tracked with a visual zoom-camera on a pan-tilt-unit and classified in the sensor fusion system layer.From the image processing point of view, the system must be scalable and provide enough processing power. It should provide real-time processing without large latency.2.1Hardware Setup2.1.1CameraTo cover a 2π hemnisphere, we use four Adimec S-25A30/CL CameraLink cameras. The sensor is an On Semiconductor VITA 25K CMOS image sensor with 5120 x 5120 active pixels. The theoretical maximum frame rate is 25Hz at full resolution. Using a 10bit pixel bit-depth, this corresponds to a raw data rate of 3.3 Gbyte/sec. The relevant camera parameters are shown in table 1.Table 1:Camera parameters according to product specification sheet.10TypeMonochromeArchitecture5T CMOS with global shutter, microlensesPixelsize4.5μm × 4.5μmActive pixels5120 × 5120Video outputFull Camera Link 8 tap 10 bitsInterface clock85 MHzFrame rate (full frame)25 HzThe lens is a Walimex pro 10/2,8 wide-angle lens, covering 95° in horizontal and vertical direction. The cameras are mounted in plastic boxes, looking into the sky under 45°. In theory, it should be possible to detect a drone like the DJI-Inspire in a distance of 500 meters with this setup. The optical distortions make it necessary to calibrate the individual cameras.The cameras can be synchronized with an FPGA-based trigger-generator. A CameraLink splitter distributes the camera data to a FPGA-board and a frame-grabber.2.1.2FramegrabberA CameraLink frame-grabber is needed to provide the images on the host to track extended objects9 and for visualization. The selected grabber is a Silicon Software mE5 marathon ACL PCIe card supporting Full frame CameraLink.2.1.3FPGA boardOne camera data stream from the CameraLink splitter is directed into an Alpha Data ADM-XRC-7K1 FPGA board. The FPGA is a Xilinx Virtex-7 Kintex K325T on a XMC carrier, which is used with an ADC-PCIE-XMC PCIex8 carrier.11,12 The physical CameraLink interface is realized with a XRM2-CLINK-MINI-FULL Full CameraLink adapter.13 The data transfer from the FPGA to the host PC can use up to four DMA-channels for efficient data transfers.2.1.4PCAs PC, we use the Dell Precision Tower 5810 XCTO Base. It has a Intel Xeon E5-1620 v4 Quad-Core Prozessor with 3.5Ghz base frequency and 16GB 2.4GHz DDR4-RAM. The Operating System is Arch-Linux.2.1.5System setupTo have a scalable design, every camera is processed by its own PC-System with a FPGA-board and a frame-grabber. The final setup consists of four PCs. The coordinated readout of the PCs is done with top-processes communicating over UDP. It is also possible to display the camera images locally on the computer. Figure 1 (a) shows the architecture of the system.Figure 1:Realized system architectures.2.1.6GPGPUAs the implementation of the algorithm on a FPGA is very time-consuming and difficult to change, it was evaluated, if the FPGA board can be replaced by a General Purpose Graphical Processing Unit(GPGPU) for a more flexible system. A GPGPU is much easier to program than a FPGA, but it comes at the cost of a higher power consumption. Figure 1 (b) shows the architecture of such a system. The changes to the system are minor, as the GPGPU reads the images from the shared memory filled from the frame-grabber and writes the results to the shared memory also used for the FPGA-based system. Therefore, the processes using the data from the shared memories need not to be altered.2.1.7Software stackThe cluster data from the FPGA-Board are read out with the Alpha Data ADM3 SDK. The data transfer is done over the four available DMA channels. As the SDK supported DMA transfers need a fixed length to complete, the outgoing data are padded if necessary, in order to get the result data after the end of a frame is reached. To give applications access to the cluster data, the data are written into a shared memory ring-buffer. The ring-buffer can hold the data from a fixed number of images in its sub-buffers. In the shared memory section, there is also a header section to handle the concurrent access to the data. The header contains information about the ring-buffer and data properties and the status of the write access. The header must be accessed by a process-wide named mutex. Only the process transferring the data from the FPGA is allowed to write data, all other processes should only read data. To wait for newly written data, the read processes can use events implemented with process-wide condition-variables. Every sub-buffer has a corresponding event, on which a reading process can wait for new data. Before filling a sub-buffer with new data, the writer process resets the event. After writing the new data, the process updates the header section and signals the event. A reading process can use the header data to check, if the sub-buffer it was reading was overwritten in that time, so that it has the possibility to discard the results created from this data. The ring-buffer API is implemented with the help of the Boost Interprocess14 library.For the tracking of extended objects, the tracking algorithm can use a similar shared memory scheme to read the image data from the frame-grabber. The connection to the rest of the drone detection system is done over the Ethernet network using the ActiveMQ15 messaging library.2.2AlgorithmThe detection algorithm has two main parts. First, the detection of small, point-like objects which have defined boundaries against the local background. This covers bright hotspot- and dark coldspot-detections. Second, locally connected detections are combined into clusters.2.2.1DetectionDetections are defined by their boundaries against the local background. The shape and the number of boundaries are algorithmic parameters. We use quadratic boundaries around the pixels of the image. The algorithm first selects the extrema on the individual boundaries and finally the extrema over all boundaries, which maximizes the difference of the pixel intensities on the boundary and the middle pixel. This difference, defined as the hotspot or coldspot value, defines the quality measure of a detection. The minimum quality of the detections is selected by a threshold parameter. The number of the considered boundaries defines the maximum size of the detection. Objects with larger sizes are therefore implicitly rejected.To gain flexibility, it is possible to divide an image in virtual sub-images and to define individual thresholds for the hotspots and coldspots inside the sub-images. This can be used to define higher thresholds for high-clutter regions like trees in direct sun-light.2.2.2ClusteringIf detections of one sort are connected over a local 8-neighborhood, they are combined into clusters. This is similar to search for connected components excluding the background component. The cluster properties are the weighted mean positions, the number of detections in the cluster and a quality number. As the detection algorithm limits the size of the extracted objects, clusters have a maximum extension. This fact can be used to speed up the processing.2.3FPGA ImplementationThe implementation on the FPGA was done with the Xilinx Vivado 2015.4 Tool-chain. The only external core used is a Full-Cameralink-Core from Alpha-Data.2.3.1Camera interfaceA Alpha-Data Full-Cameralink-Core is used to deserialize the incoming 10bit pixel data of the camera. The output of the core are eight pixels per 85 MHz Cameralink clock cycle. The algorithm core is running with 255 MHz. As the core requires a pixel-wise input, the eight pixels must be serialized in an intermediate step. This means, that a minimum spare time is necessary between two consecutive rows send into the core. To achieve this, the image is divided in four sub-images of size 1280x5120. Every sub-image is processed by its own algorithm core.Field trials with this setup have shown, that it will be advantageous to bin the image by an factor of two in vertical and horizontal direction. A second version of the camera interface uses the vertical binning feature of the camera, while the vertical binning is done on the FPGA by averaging each two consecutive rows. The pixel serializer for the algorithm cores is adapted accordingly. The rest of the design is unchanged.2.3.2Detection implementationThe algorithm works row-wise on the incoming data. The incoming pixels are divided into horizontal and vertical boundary components and delayed accordingly to calculate the hotspot or coldspot value for every pixel of the image. Pixels at the image boundaries are handled by zero-padding and taking into account the maximum possible detection size at the boundary. For performance reasons, two cores are used per sub-image. The coldspot core is a duplicate of the hotspot core with inverted incoming pixel values.2.3.3Clustering implementationThe clustering algorithm works on a binary image where the detections form the foreground. The detection core delivers the binary image row-wise. In a first step, a candidate-search core identifies contiguous detections in a row and extracts the necessary information needed to link them with the previously found candidates and to calculate the cluster properties. The candidates are put into a FIFO. A cluster algorithm checks if the new cluster candidates are part of a cluster found in the previous rows. If the candidate is part of such a cluster, the cluster information is updated accordingly. If no previous cluster can be found, the cluster candidate forms a new cluster. After matching the new candidate clusters, the algorithms checks if clusters must be merged. As the detection algorithm defines a maximum size of the clusters, already found clusters can be written out if they were not touched for several processed lines. The resulting cluster data of a sub-image are transferred to the host over one of the four available DMA channels on the FPGA board.2.4GPGPU ImplementationIn this section we briefly describe the detection and clustering implementations. Both computationally expensive methods are offloaded and computed on a GPGPU with Cuda.2.4.1Detection implementationThe detector is a data-parallel algorithm as the hotspot and coldspot property can be calculated independently with respect to the image pixels. Algorithm 1 presents our implementation with Cuda.Algorithm 1The Naive Detector algorithm. All symbols with the subscripts D,H stand for the device and host variables respectively. The symbols without any subscript are implicit device variables.The naive, non-optimized version of the detector receives an input image on the host side and transfers it to the GPGPUs device memory. Then, the program creates a single Cuda thread for each result pixel. In the next step, each thread reads the neighboring pixels and calculates the hotspot and coldspot properties. All detections found are stored in a result image. The proposed thread-mapping in this method is not efficient on GPGPUs. The threads from the same CUDA-warp need to load many cache-lines from the GPGPU global memory and repeatedly read the data for the calculations. Such a naive approach under-utilizes the GPGPU bandwidth. To improve the bandwidth utilization, we used shared block-memory and implemented a more efficient thread mapping to get an optimized detector version as shown in Algorithm 2.Similarly to the naive version, the algorithm operates on the image received from the host. But now, the program allocates threads differently and uses shared block-memory. For each thread-block, the algorithm reads pixels from the global memory and stores them in the shared memory. This operation is done only once. In the next step, thread blocks reuse those data to find detections. The strategy with the shared block-memory reduces the number of global memory requests and significantly increases the overall algorithm performance. The results are shown in section 3.2Algorithm 2The optimized detector algorithm. All symbols with the subscripts D,H stand for the device and host variables respectively. The symbols without any subscript are implicit device variables.2.4.2Clustering implementationAfter the detection phase, the clustering algorithm creates clusters from the hotspots and coldspots, respectively. The clustering module is offloaded to the GPGPU and implemented with Cuda. Our Cuda implementation is based on the iterative label-equivalence-resolving method which is similar to the CPU algorithm described by16 and.17 Algorithm 3 in the appendix presents our Cuda implementation.The implemented method executes the task of labeling by creating one Cuda-thread for each image pixel. Each thread will examine the neighboring pixels and assign proper labels in the labels-list. If the label is next to a lower label and if the label is lower than the value currently in the list, it will put that lower label into the equivalence list. This method records only the lowest label, that each label is equivalent to. This reduces the memory complexity and keeps the number of iterations need to resolve equivalences low. During our tests, the 5120x5120 images required only 12 or less iterations. The scanning is repeated until the label it looks up in the list is equivalent to itself. Once the value is found, the method overwrites the equivalence list with the found value. After the equivalence lists have been updated, the final stage of the labeling process can take place. One thread for each pixel will look up the table in the equivalence lists and then assign the equivalent label to the labeled resulting image.2.5Data distributionTo distribute the cluster data to different processes, the data are written to a ring-buffer in a shared-memory region. Using the Boost Interprocess Library, mutual access to the ring-buffer meta-data, like the last written buffer, and events to wait for new data are provided. The API has the same interface like the corresponding native Window functions for event handling.2.6TrackingFirst of all, the subsequent tracking process reads all cluster data associated with the current camera image from host shared memory. For the first camera image and in cases where no trajectories are present, each detected coldspot and hotspot is used to generate a new trajectory of length one, i.e. consisting of one point, using the mean pixel position of the cluster as point position. If trajectories are present, for each trajectory the nearest coldspot or hotspot position with respect to the most recent trajectory point is computed. If this nearest distance is below a reasonable threshold t := f · d, the trajectory is lengthened by a new point having the pixel position of the found cluster. d is an estimation of the detected object’s diameter in the image in pixels. It is computed by solving the Gauss’s Circle Problem.11,12 It is advantageous to consider d in the formula for t due to the fact that nearer drones (having a greater d) generally move faster in the image than further ones (smaller d). f is a user-defined tuning factor which is chosen so that t is reasonable, i.e. matches the observations when flying with drones in different distances. In that way, a balance is found to capture drone flights even at higher velocities on the one hand, but without jumping to clutter, which is too far away, when a drone detection is missing in an image for some reason, on the other hand. We found f = 8 to be a good choice. Table 2 illustrates the values of d and t for some cluster sizes.Table 2:Estimated object diameter d and distance threshold t in pixels depending on different cluster sizes.cluster size1245781321313651618196116126151d12233456789101112131415t816162424324048566472808896104112120All remaining coldspots or hotspots that could not be assigned in this way to an existing trajectory form a new trajectory of length one.Trajectories that cannot be continued in the above described procedure with a compatible cluster are not deleted immediately from the set of present trajectories but marked to be old at first. A deletion is done not until a trajectory remains old over four successive images. In this manner, trajectories are smoothly continued when detections from drones are missing for one, two, or three images and reappear afterwards.The detections of a drone may change over time from coldspots to hotspots and vice versa due to background changes or light reflectance reasons. If there are dark and bright clouds in the sky background, the appearance of a moving drone can naturally vary in this way. Figure 5 (d) shows an example, where the reflection of sunlight from the drone leads to a hotspot detection (red cross). Therefore, we do not distinguish between hotspots and coldspots in the trajectory generation process in order to capture such drone flights preferably with only one trajectory.Trajectories of length one are not shown to the user as system alarm. This is done for trajectories with a length of up to three, too, if longer trajectories are also currently present. This filters out most false alarms stemming from sensor noise and other noise artifacts. Similarily, detections stemming from static scene background can be filtered out, too, by computing bounding boxes for the trajectories. If a long trajectory has a bounding box size below a small threshold ϵ (e.g. ϵ = 3 pixels), the detection stems from a static scene part or a static object in the scene. Since tree leaves in the wind often produce alarms which cannot be handled by this mechanism due to their movement, the user can manually generate an image mask in which all scene parts are just painted by the user with a paint software that have to be masked out in the tracking process. Another option is the use of the virtual sub-images with according thresholds.3.RESULT AND DISCUSSION3.1FPGA PerformanceThe FPGA resources used for the implementation of the detection and cluster algorithms are shown in table 3. The resource utilization is defined as the percentage of used resources in relation to the available resources on a specific FPGA chip. It can be observed, that the internal memory blocks, the Block RAM Tiles, are the most critical resources. The internal memory is used for delaying image rows and to hold intermediate and final results. Except for a lower Block RAM utilization by about 8%, the version working on the binned image needs the same resources as the version working on the full resolution image.Table 3:Resource utilization Xilinx Kintex 7k325tffg900-2.ResourceAvailableunbinned version [%]binned version [%]Slice LUTS20380027.6627.52Slice Registers40760022.2222.08Block RAM Tile44586.8578.76DSPs8407.627.62Table 4 shows the clock frequency used to run the algorithmic core and the estimated power consumption. The clock frequency is 255 MHz. It is selected as multiple of the CameraLink frequency of 85 MHz. The design can run at the full camera speed of 25 FPS. The detection core has a throughput of one result per incoming pixel and a latency of a few ten cycles. The cluster core is able to handle the maximum theoretical detection rate, which is a detection at every second pixel. The power consumption of the cores is estimated with the Power Report Tool of the Xilinx Vivado 2015.4 tool-chain. As result, the power consumption of the FPGA is between 5 and 10 Watt.Table 4:FPGA system properties.Propertyunbinned versionbinned versionCore clock frequency255 MHz255 MHzPower consumption5-10W5-10W3.2GPGPU Performance3.2.1Experimental DesignAll tests have been performed on an Desktop-PC equipped with a Nvidia GTX 1080 TI GPU. We have used Nvidia Cuda-Toolkit 9.1 with the Graphic-driver 390.48. Also, we used the nvvprof utility to exploit the GPU power consumption and profile the execution parameters. Besides, we tested 100 times with two different images: N = 6553600, 26214400 where N is the number of image pixels. For all tests we recorded the profiling information and calculated the averaged values.3.2.2Performance ResultsFirst, we evaluated the execution time of the detector algorithm. Figure 2 shows the execution time of the two detector versions. One observes, that the optimized version reduces the execution time by more than 50%. Additionally, we measured the GPGPU power consumption with the Nvidia profiling tool nvvprof. The power consumption of the optimized version is also reduced, by around 30 Watt, independent of the image resolution. The reason for the increased throughput and the simultaneous decrease of the dissipated power is the reduction of global memory transactions. The naive detector version requires 1766646872 global memory loads, where as the optimized version requires only 33702136 loads. Additionally, we measured the execution time of the clustering functions. Figure 3 presents the execution time of all offloaded functions to the GPGPU. As can be seen, the detector function is the most computationally intensive operation in our system. The clustering functions require only a few millisecond to create the clusters. We observe in our tests that the offloaded calculations the GPU satisfies a real-time processing requirements independently of the image resolution. The final optimized version achieve the throughput of 68fps for the 5120x5120 images.Figure 2:Profiling results for naive and optimized detector kernel functions. The left figure presents the execution times in microseconds. The right figure shows the power consumption in Watt.Figure 3:The figure presents the execution times of all system function in microseconds. The profiling results are generated with two different image sizes.3.3Tracking PerfomanceFigure 4 shows an example that illustrates the final result of the presented detection and tracking approach and how detection alarms are transmitted to the user. The tree on the lower left in the image was masked out as described at the end of the preceding section in order to suppress false alarms there. A flying drone is captured over time and the flight path (trajectory) visualized in form of a light blue image overlay. The tracking process has an uncritical computation time of around 6 milliseconds per image.Figure 4:Result of the presented detection and tracking process. The trajectory of a captured flying drone is overlayed in light blue color.3.4Range limitationsThe first field trials with the setup have shown, that detections at the theoretical maximum range of about 500m are difficult to achieve due to the atmospheric conditions and the noise characteristic of the camera. Figure 5 (a) shows a DJI-Inspire drone (nearly at the center of the image) in an range between 100m and 200m. The contrast between background and drone surface is very weak as can be better seen in the zoomed view in Figure 5 (b). Figure 5 (c) shows another example, where the detector is still able to find the drone even at low contrast. However, under such conditions, the detection of point-like drones is severely limited, as the drone is relatively large in the useful distances. The detector works mainly in the cases, where substructures like lights on the drone generate detections. To use the detector under such conditions, the binned version of the detector was implemented. With this version drones can be detected at ranges starting from 30-40m. Under most conditions it works reliable up to 100-200m. In the future, we want to investigate the detection ranges for different environmental conditions.Figure 5:Detection on drones. Blue cross: coldspot; red cross: hotspot)4.CONCLUSIONThe presented system to detect and track drones in the far-range is able to handle the raw data load of 33GB/s from the visual sensors. The image data of a camera can be processed in real-time with a FPGA with an power consumption below 10 Watt additional to the power consumption of the host system. A FPGA-based system approach is very inflexible to changes in the algorithm. Therefore, the detection algorithm was implemented on GPGPU. It was shown, that real-time performence can be easily achieved with 68 FPS for the investigated GPGPU. This comes at the cost of an power consumption of 120 Watt for the binned image and 186 Watt for the full frame image. The distribution of the detection data can be scalable and safe by using the Boost Interprocess library.Under poor atmospheric condition, the system can be still useful when working on binned image data. The system is currently investigated for distance ranges between 30 and 200 meter. In the future, we want to make more field trails to investigate the full ranges under different environmental conditions.We think, that visual sensors can be a valuable part of a multi-sensor drone detection system.AppendicesAPPENDIX A.CLUSTER ALGORITHMAlgorithm 3The Cuda Connected Component algorithm (CCCL).REFERENCES[1] Warrick, J., “Use of weaponized drones by ISIS spurs terrorism fears.” Washington Post February 21, 2017 https://www.washingtonpost.com/world/national-security/use-of-weaponized-drones-by-isis-spurs-terrorism-fears/2017/02/21/9d83d51e-f382-11e6-8d72-263470bf0401_story.html?utm_term=.fe9c4c7d5b9f (2017). (Accessed: 8 August 2018).Google Scholar
        [2] Birch, G. C., Griffin, J. C., and Erdmann, M. K., “UAS Detection, Classification, and Neutralization: Market Survey 2015.” SANDIA REPORT SAND2015–6365 (2015).Google Scholar
        [3] Birch, G. C. and Woo, B. L., “Counter Unmanned Aerial Systems Testing: Evaluation of VIS, SWIR, MWIR, and LWIR passive imagers.” SANDIA REPORT SAND2017–0921 (2017).Google Scholar
        [4] Ganti, S. R. and Kim, Y., “Implementation of detection and tracking mechanism for small uas,” International Conference on Unmanned Aircraft Systems (ICUAS),June 7-10 2016 (2016). https://doi.org/10.1109/ICUAS.2016.7502513Google Scholar
        [5] Rozantsev, A., Lepetit, V., and Fua, P., “Flying objects detection from a single moving camera,” in [IEEE Converence on Computer Vision and Pattern Recognition (CVPR)], 4128–4136 (2015).Google Scholar
        [6] Hu, S., Goldman, G. H., and Borel-Donohue, C. C., “Detection of unmanned aerial vehicles using a visible camera system,” in [Applied Optics], Vol. 56, Issue 3, B214–B221 (2017).Google Scholar
        [7] Christnacher, F., Hengy, S., Laurenzis, M., Matwyschuk, A., Naz, P., Schertzer, S., and Schmitt, G., “Optical and acoustical uav detection,” in [Electro-Optical Remote Sensing X], Proc. SPIE 9988 99880B (2016).Google Scholar
        [8] Liu, H., Wei, Z., Chen, Y., Pan, J., Lin, L., and Ren, Y., “Drone detection based on an audio-assisted camera array,” in [2017 IEEE Third International Conference on Multimedia Big Data (BigMM)], 402–406 (2017).Google Scholar
        [9] Müller, T., “Robust drone detection for day/night counter-uav with static vis and swir cameras,” in [Ground/air Multisensor Interoperability, Integration, and Networking for Persistent ISR VIII], Tien Pham, M. A. K., ed., Proc. SPIE 10190 (2017).Google Scholar
        [10] “Product Specfication Sapphire.” S25A30CLS1011 Ver. 1.1 (2014).Google Scholar
        [11] “Adm-xrc-7k1 datasheet revision 1.1,16th april 2012.” https://www.alpha-data.com/pdfs/adm-xrc-7k1.pdf. (Accessed: 9 August 2018).Google Scholar
        [12] “Adc-pcie-xmc datasheet revision 1.2,7th december 2015.” https://www.alpha-data.com/pdfs/adc-pcie-xmc.pdf. (Accessed: 9 August 2018).Google Scholar
        [13] “Xrm2-clink-mini datasheet revision 1.3,29th may 2018.” https://www.alpha-data.com/pdfs/xrm2-clink-mini.pdf. (Accessed: 9 August 2018).Google Scholar
        [14] https://github.com/boostorg/interprocess.Google Scholar
        [15] http://activemq.apache.org/.Google Scholar
        [16] He, L., Chao, Y., Suzuki, K., and Wu, K., “Fast connected-component labeling,” Pattern Recogn. 42, 1977–1987 (Sept. 2009). https://doi.org/10.1016/j.patcog.2008.10.013Google Scholar
        [17] Hawick, K. A., Leist, A., and Playne, D. P., “Parallel graph component labelling with gpus and cuda,” Parallel Comput. 36, 655–678 (Dec. 2010). https://doi.org/10.1016/j.parco.2010.07.002Google Scholar
        [18] Weisstein, E. W., “Gauss’s Circle Problem.” http://mathworld.wolfram.com/GausssCircleProblem.html. (Accessed: 8 August 2018).Google Scholar
        [19] Hilbert, D. and Cohn-Vossen, S., [Geometry and the Imagination], Ams Chelsea Publishing, New York (1999).Google Scholar
        





                    © (2018) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.
                





Citation
Download Citation



Thomas Perschke, Konrad Moren, and Thomas Müller
"Real-time detection of drones at large distances with 25 megapixel cameras", Proc. SPIE 10799, Emerging Imaging and Sensing Technologies for Security and Defence III; and Unmanned Sensors, Systems, and Countermeasures, 107990N (4 October 2018); doi: 10.1117/12.2324678; https://doi.org/10.1117/12.2324678











Access the abstract







PROCEEDINGS
15 PAGES



DOWNLOAD PDF



                        SAVE TO MY LIBRARY
                    










SHARE









GET CITATION















                        <
                        Previous Article
|
Next Article
                        >

                







Advertisement











Advertisement












KEYWORDS





                        Cameras
                    






                        Detection and tracking algorithms
                    






                        Sensors
                    






                        Field programmable gate arrays
                    






                        Image processing
                    






                        Imaging systems
                    






                        Data processing
                    



 


                        Visualization
                    






                        Atmospheric optics
                    






                        Image resolution
                    






Show All Keywords







RELATED CONTENT






Ghost removing for HDR real-time video stream generation

Proceedings of SPIE (April 28 2016)


High performance camera module for fast quality inspection in industrial...

Proceedings of SPIE (February 16 2007)


Hardware accelerator design for tracking in smart camera

Proceedings of SPIE (September 30 2011)


General relationship for optimal tracking performance

Proceedings of SPIE (October 28 1996)


On road obstacle detection and tracking system using robust global...

Proceedings of SPIE (April 15 2010)


An airborne real-time hyperspectral target detection system

Proceedings of SPIE (May 12 2010)


Real-time reconfigurable foveal target acquisition and tracking system

Proceedings of SPIE (July 14 1999)














Subscribe to Digital Library





Receive Erratum Email Alert













                    Erratum Email Alerts notify you when an article has been updated or the paper is withdrawn.
                



                    Visit My Account to manage your email alerts.
                











  

Email or Username
Forgot your username?




Password
Forgot your password?


  Show




Remember Email/Username on this computer


Remember Password





                No SPIE account? Create an account 
Institutional Access:Sign in with your institutional credentials

 









                The alert successfully saved.
            



                Visit My Account to manage your email alerts.
            



                CLOSE
            







                The alert did not successfully save. Please try again later.
            



                CLOSE
            











        Thomas Perschke, Konrad Moren, Thomas Müller, "Real-time detection of drones at large distances with 25 megapixel cameras," Proc. SPIE 10799, Emerging Imaging and Sensing Technologies for Security and Defence III; and Unmanned Sensors, Systems, and Countermeasures, 107990N (4 October 2018);
        


Include:



 Citation Only


 Citation & Abstract





Format:



 RIS (Zotero)


 EndNote


 BibTex


 Medlars


 ProCite


 Ref Works







                DOWNLOAD CITATION
            
















Access provided by  Univ. of Amsterdam














Site Map

 
        Home
    

 
        Conference Papers
    

 
        Conference Presentations
    

 
        Journals
    

 
        eBooks
    

 
        About
    

 
        Subscriptions
    




Information for Authors

 
        Proceedings Authors
    

 
        Journal Authors
    

 
        eBook Authors
    




Information for Reviewers

 
        Reviewer Guidelines
    



Information for Librarians

 
        Resources
    

 
        Subscriptions
    




Contact & Support
+1 888 902 0894(United States)+1 360 685 5580(International)
Hours:8:00 am to 5:00 pm PST
Help | Contact Us


Connect
 

 

 











        SPIE Privacy Policy
    
|

        Terms of Use
    


						© 2019 SPIE
					













                    CONFERENCE PROCEEDINGS
                



        Papers
    



        Presentations
    







        Journals




        Advanced Photonics
    





        Journal of Applied Remote Sensing
    



        Journal of Astronomical Telescopes, Instruments, and Systems
    



        Journal of Biomedical Optics
    



        Journal of Electronic Imaging
    



        Journal of Medical Imaging
    



        Journal of Micro-Nanolithography, MEMS, and MOEMS
    



        Journal of Nanophotonics
    



        Journal of Photonics for Energy
    



        Neurophotonics
    



        Optical Engineering
    





        Ebooks
    





Help |
                Advanced Search >













Keywords/Phrases


Keywords




in

All Fields
Abstract
Author Name
Affiliation
DOI / ISSN / ISBN
Figure & Table Captions
Keywords
Title
Volume Title

Remove



AND
OR
NOT





in

All Fields
Abstract
Author Name
Affiliation
DOI / ISSN / ISBN
Figure & Table Captions
Keywords
Title
Volume Title

Remove



AND
OR
NOT





in

All Fields
Abstract
Author Name
Affiliation
DOI / ISSN / ISBN
Figure & Table Captions
Keywords
Title
Volume Title

Remove



+ Add another field

Search In:




Proceedings


Volume







Journals +


Volume



Issue



Page





Advanced Photonics

Journal of Applied Remote Sensing

Journal of Astronomical Telescopes  Instruments  and Systems

Journal of Biomedical Optics

Journal of Electronic Imaging

Journal of Medical Imaging

Journal of Micro/Nanolithography, MEMS, and MOEMS

Journal of Nanophotonics

Journal of Photonics for Energy

Neurophotonics

Optical Engineering

SPIE Reviews




eBooks +



Field Guide Series

Press Monograph

Spotlight

Tutorial Text

Other Press



Publication Years


Range






Single Year





Clear Form