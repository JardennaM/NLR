











Potential of lidar sensors for the detection of UAVs























 





















































Sign In


View Cart 


        Help
    









  

Email or Username
Forgot your username?




Password
Forgot your password?


 Show




Remember Email/Username on this computer


Remember Password






Please wait...




                No SPIE account? Create an account 
Institutional Access:Sign in with your institutional credentials

 













                    CONFERENCE PROCEEDINGS
                



        Papers
    



        Presentations
    







        Journals




        Advanced Photonics
    





        Journal of Applied Remote Sensing
    



        Journal of Astronomical Telescopes, Instruments, and Systems
    



        Journal of Biomedical Optics
    



        Journal of Electronic Imaging
    



        Journal of Medical Imaging
    



        Journal of Micro/Nanolithography, MEMS, and MOEMS
    



        Journal of Nanophotonics
    



        Journal of Photonics for Energy
    



        Neurophotonics
    



        Optical Engineering
    





        Ebooks
    




Advanced Search >








                            Home
                        
>

                            Proceedings
                        
>

                            Volume 10636
                        
>
Article





















        Translator Disclaimer
    




                        You have requested a machine translation of selected content from our databases. This functionality is provided solely for your convenience and is in no way intended to replace human translation. Neither SPIE nor the owners and publishers of the content make, and they explicitly disclaim, any express or implied representations or warranties of any kind, including, without limitation, representations and warranties as to the functionality of the translation feature or the accuracy or completeness of the translations.
                    

                        Translations are not retained in our system. Your use of this feature and the translations is subject to all use restrictions contained in the Terms and Conditions of Use of the SPIE website.
                    









10 May 2018
Potential of lidar sensors for the detection of UAVs



 Marcus Hammer;                         Marcus Hebel;                         Björn Borgmann;                         Martin Laurenzis;                         Michael Arens 


Author Affiliations +

 Marcus Hammer,1 Marcus Hebel,1 Björn Borgmann,1 Martin Laurenzis,2 Michael Arens1 
 1Fraunhofer-Institut für Optronik, Systemtechnik und Bildauswertung (Germany)						 2Institut Franco-Allemand de Recherches de Saint-Louis (France)
				




Proceedings Volume 10636, Laser Radar Technology and Applications XXIII; 1063605 (2018) https://doi.org/10.1117/12.2303949Event: SPIE Defense + Security, 2018, Orlando, Florida, United States














                                ARTICLE
                            





                                SECTIONS
 


1. INTRODUCTION
2. DATASET
2.1 Sensor platform MODISSA
2.2 Field trial
3. METHODOLOGY
3.1 Segmentation of single 360° scans
3.2 Merging of 3D point clouds and 3D clustering
3.3 Tracking of 3D clusters
4. EXPERIMENTAL RESULTS
4.1 Field Trial
4.2 Detection range
4.3 Resolution on target
4.4 Detection rate
5. CONCLUSIONS AND FUTURE WORK





                                FIGURES & TABLES
                            





                                REFERENCES
                            





                                CITED BY
                            




DOWNLOAD PDF 

SAVE TO MY LIBRARY














 
 PERSONAL SIGN IN
 Full access may be available with your subscription

 

Email or Username
Forgot your username?




Password
Forgot your password?


Show




Remember Email/Username on this computer


Remember Password





                    No SPIE account? Create an account 
Institutional Access:Sign in with your institutional credentials

 









 SUBSCRIBE TO DIGITAL LIBRARY



50 downloads per 1-year subscription


Members: $195


Non-members: $335
ADD TO CART



25 downloads per 1 - year subscription


Members: $145


Non-members: $250
ADD TO CART





 PURCHASE SINGLE ARTICLE



Includes PDF, HTML & Video, when available


Members: $14.40


Non-members: $18.00
ADD TO CART














This will count as one of your downloads.
You will have access to both the presentation and article (if available).



DOWNLOAD NOW







 
 This content is available for download via your institution's subscription. To access this item, please sign in to your personal account.
 

Email or Username
Forgot your username?




Password
Forgot your password?


 Show




Remember Email/Username on this computer


Remember Password





                No SPIE account? Create an account 

 








My Library









                            You currently do not have any folders to save your paper to! Create a new folder below.
                        


                            Create New Folder
                        






                            SAVE >
                        








 

                    Folder Name
                




                    Folder Description
                





                    SAVE
                
















Abstract

The number of reported incidents caused by UAVs, intentional as well as accidental, is rising. To avoid such incidents in future, it is essential to be able to detect UAVs. LiDAR systems are well known to be adequate sensors for object detection and tracking. In contrast to the detection of pedestrians or cars in traffic scenarios, the challenges of UAV detection lie in the small size, the various shapes and materials, and in the high speed and volatility of their movement. Due to the small size of the object and the limited sensor resolution, a UAV can hardly be detected in a single frame. It rather has to be spotted by its motion in the scene. In this paper, we present a fast approach for the tracking and detection of (low) flying small objects like commercial mini/micro UAVs. Unlike with the typical sequence -track-after-detect-, we start with looking for clues by finding minor 3D details in the 360° LiDAR scans of scene. If these clues are detectable in consecutive scans (possibly including a movement), the probability for the actual detection of a UAV is rising. For the algorithm development and a performance analysis, we collected data during a field trial with several different UAV types and several different sensor types (acoustic, radar, EO/IR, LiDAR). The results show that UAVs can be detected by the proposed methods, as long as the movements of the UAVs correspond to the LiDAR sensor’s capabilities in scanning performance, range and resolution. Based on data collected during the field trial, the paper shows first results of this analysis.





1.INTRODUCTIONThe worldwide availability of low-cost UAVs that are easy to operate leads to rising problems, reaching from mislead toys up to abusive use of UAVs for spying or other hostile activities. Especially with regard to sensible areas like airports, company premises or military facilities, options for the detection, tracking and monitoring of undesired UAVs are needed.Not only the small size of UAVs renders their detection and tracking a complicated task. Their high acceleration capacity, their high top speed and their great maneuverability in all three dimensions make it hard to track the UAVs or to predict their paths reliably. Nevertheless, most of the known standard surveillance techniques can be applied for UAV detection:– There are various approaches with cameras in the visible [1] [2] and the SWIR [3] spectra. For all camera-based algorithms, the main challenge is the separation of the object from the background. Furthermore, the detection results are mainly dependent on the quality of the images, e.g., the resolution and the focus on the object.– Another well-proven technique for the detection of (flying) objects is the use of radar sensors. Noetel et al. [4] describe two types of millimeter wave radars for the detection and tracking of small UAVs. The UAV detection with radar is limited mainly by the low radar cross-section of most UAVs.– The detection of UAVs with acoustical sensors is described by Christnacher et al. in [5]. Based on acoustic antenna arrays, the acoustic signatures of flying UAVs are measured continuously. With this hardware setup, a wide range of UAV sounds (up to 6.8 kHz) can be detected and tracked.– The use of active imaging in the form of gated viewing for UAV detection and tracking is presented by Christnacher et al. in [5] as well. The main advantage of gated viewing in contrast to the imaging with CCD cameras is the option to set the gate around the object and to suppress the foreground and the background. For this purpose, the distance to the UAV has to be known initially.3D point clouds acquired by LiDAR sensors (light detection and ranging) have proven to be a good basis for object detection, e.g., for persons [6], cars [7] or ships [8]. Today’s available scanning LiDAR systems represent a compromise between acceptable resolution, realizable refresh rate and field of view (FOV). Since UAVs usually have a small laser radar cross-section (LRCS) [9], the sparse resolution of most LiDAR sensors seems to make them rather unsuitable for UAV detection. On the other hand, LiDAR-based detection methods show many advantages: the separation of the object from fore- or background is comparatively easy, the exact 3D position of the object is known instantaneously after its detection, and the sensor is not dependent on daylight conditions. In this paper, we assess the potential of LiDAR sensors for the detection of UAVs.2.DATASETFor our experiments we used data which we acquired with the measurement vehicle MODISSA [10] during a joint field trial in cooperation with the Institute Franco-Allemand de Recherches de Saint-Louis (ISL) and the Fraunhofer Institute for High Frequency Physics and Radar Techniques FHR [11].2.1Sensor platform MODISSAThe vehicle shown in Figure 1 is equipped with several scanning LiDAR sensors, various cameras for omnidirectional monitoring, an EO- and a SWIR camera on a pan-tilt head and an inertial measurement unit (IMU) and two GNSS (GPS, GLONASS) receivers. We used the two LiDAR sensors Velodyne HDL-64E mounted on the roof in the front of the vehicle in combination with the two LiDAR sensors Velodyne VLP-16 PUCK mounted on the roof in the back of the car. Each HDL-64E is capable of performing 1.3 million measurements per second in a range up to 120 meters. Its vertical FOV is 26.9°, divided into 64 scanlines resulting in a vertical resolution of 0.4°. Due to the rotating sensor head, the horizontal field of view covers 360° with a resolution of about 0.17° at a typical rotation frequency of 10 Hz. This means that every point cloud could consist of approximately 130 000 measurements. However, since not every emitted laser pulse results in the detection of a returning echo (e.g., measurements directed to the sky or water surfaces), the resulting point clouds are usually smaller. The smaller VLP-16 has a vertical FOV of 30° generated by 16 scanlines, leading to a vertical resolution of 2°. At a rotation frequency of 10 Hz, the horizontal resolution is about 0.2° and a single 360° scan could consist of up to 30 000 measurements (3D points). The recording of the LiDAR measurements is time-synchronized and the geometric constellation of the four sensors is known, so the four individual point clouds can easily be merged together.Figure 1:Sensor platform MODISSA equipped with several sensors including four 360° LiDAR scanners.The two HDL-64E sensors are mounted to the sensor carrier with a tilt of 25° downwards and an angle of 45° outwards (cf. Figure 1), the VLP-16 are mounted to it with a tilt of 15° downwards and 45° outwards. This special configuration leads to an enlarged FOV in upwards directions (e.g., towards the facades of buildings). Otherwise, if a HDL-64E is horizontally mounted as intended by the manufacturer for applications like autonomous driving, its FOV would cover +2° to -25° in elevation, which would be inappropriate for UAV detection. With our setup, elevation angles up to +27° are realized, at least in parts of the surrounding.2.2Field trialA field trial took place on a testing ground near Baldersheim (France). Its main goal was the acquisition of realistic sensor data in order to assess the capabilities of different sensor technologies for the detection, tracking and identification of small unmanned aerial vehicles (mini/micro UAVs) flying at low altitude. An additional aspect of these activities was the investigation of a sensor network, comprising of a small radar system, distributed acoustic antennas, passive/active EO/IR imaging and static as well as vehicle-mounted LIDAR sensors [11]. The various sensors were distributed to different measurement locations within the structured environment. The sensor vehicle MODISSA was used both stationary and moving. Within this setup, different types of small UAVs (see Table 1) were used for different scenarios like an ascending/descending UAV, an approaching UAV, a UAV that flies attacks, UAV in high altitude patrol, UAV in fast fly-by and multiple UAVs in various maneuvers. All the different sensor systems recorded the data of the scenarios. In the sensor fusion approach, it is investigated how the fusion of the individual detection results can enhance the overall detection rate. A more detailed description of the field trial and first results of the other sensors and the sensor fusion are given in [11]. In this paper, we focus on the analysis of the LiDAR data.Table 1:UAVs used in the field trialNo.Typewidthlengthmodel1quadcopter25 cm25 cmDJI Phantom 32quadcopter32 cm32 cmParrot Bebop 23flying wing115 cm58 cmParrot Disco4quadcopter24 cm24 cmDJI MAVIC ProFigure 2:Different mini/micro UAVs that were used during the field trial (see Table 1 for the types).3.METHODOLOGYThe proposed object detection and tracking algorithm takes advantage of the dense, ordered structure of the point clouds corresponding to single 360° rotations of the laser scanner (organized clouds). In the following, we call that a single scan or frame, consisting of n rows or scanlines (n=64 for the HDL-64E, n=16 for the VLP-16) and m columns (m depending on the pulse repetition rate and the rotation frequency of the scanner head). With the Velodyne sensors, single frames are generated such that for every emitted laser pulse a data value is recorded, even if it is NaN since no actual measurement was possible (too far, low reflectance etc.). Due to this ordered data structure, for each point the range value of its neighboring points within the same scanline can easily be determined, similar to a range image.3.1Segmentation of single 360° scansAs we have shown in an earlier work [12] and as it is known from literature [13], a fast geometric 3D segmentation is possible if the 3D data are given as organized point clouds like it is described above, i.e., if the range of neighboring pixels in rows of the range image is known. At first, this requires a 2D segmentation where neighboring points of a row are clustered as long as no changes in range exceeding a given threshold are found. All line segments (that means at least two neighboring points) with a width less than the maximum width of a target object (we assumed up to 1.2 m for the examined small UAVs, see Table 1) are further examined as possible target-related 2D clusters. Due to the high variety (a 2D cluster could contain between two and over 200 points), a high number of 2D clusters is detected for each sensor, most of them being artefacts of rough ground in higher distance.3.2Merging of 3D point clouds and 3D clusteringThe four time-synchronized LiDAR sensors of MODISSA are mounted to the sensor carrier with a fixed, known relative orientation, such that the individual point clouds can be transferred to a common coordinate system. Therefore, it is easy to merge the 2D cluster we get from the individual segmentation runs into one large 2D cluster set. All these 2D clusters are further merged to 3D clusters if they overlap horizontally and do not exceed a maximum vertical distance. 3D clusters with an overall width in x-y direction or a height (z dimension) wider than the maximum dimensions of a UAV are rejected. The number of 3D clusters typically varies between a few and more than several hundred potential objects per frame. Assuming that a UAV has to keep a minimum distance to the scene (walls, trees or the ground), we find 3D clusters that are isolated in the point cloud, i.e., 3D clusters that have no or only very few other points in their direct neighborhood. This detection of isolated clusters is done with an occupancy grid which is set up with all points of all four sensors and a grid size of 0.5 m. As a result, the number of potential UAV objects can typically be reduced from several hundred to less than twenty for each frame.Because of the small size of a UAV and the limited resolution of the LiDAR scanner, the number of points per UAV is low, even on short distances. Due to this fact, a further geometric analysis of the point cloud objects (the remaining 3D clusters) is not very promising. A better approach for a further classification of the object (UAV/non-UAV) is the analysis of its movement. This requires a tracking of the 3D clusters.3.3Tracking of 3D clustersWe record the continuous data stream of each Velodyne LiDAR sensor in such a way that it is subdivided into single 360° scans which we call frames. An object detected in the last frame should occur in the current frame at a position near its last position or the extrapolated next position. In order to achieve that, a track list is set up with all detected objects of the first frame. For all subsequent frames, the coordinates of all detected 3D objects are compared with the objects in the track list. Each new object is assigned to the track list object with the minimum distance. If there is no track list object with a distance below a threshold, the new object is set as a new track list object. For each track list object, a prediction of its further movement is calculated using a Kalman filter. For each track list object, the current speed and the overall motion can be calculated and objects without any movement can be classified as non-UAV.4.EXPERIMENTAL RESULTS4.1Field TrialDuring the field trial, we recorded over 1 hour of data of eight different scenarios, of which 75 % are supplemented with ground truth data, provided by GPS-receivers attached or belonging to the UAVs. Due to the limited FOV of the LiDAR sensors, most scans do not contain a UAV object. A UAV was detected in 4704 of 39480 frames (12 %). For a quantitative evaluation of our methods (precision and recall), only the scenarios with ground truth information are considered. In 28224 frames, 2961 UAVs should have been detectable, meaning that the UAV was in the FOV and in a range of less than 50 m. For the evaluation of the detection range and the resolution on the target all frames are taken into account.4.2Detection rangeThe maximum range in which a UAV can be detected is a crucial parameter for the use of a panoramic scanning LiDAR system. The Velodyne sensors are optimized for applications like autonomous driving, not for long-range UAV detection. The maximum range of the HDL-64E is specified with 120 m for a target with 80 % reflectivity. Due to the small LRCS of most UAV, it is to be expected that the maximum range for UAV detection is much shorter, even under good other circumstances. In the data sets we detected 4704 UAVs in ranges between 3.5 m and 50 m (preset maximum detection range). The histogram of the detection ranges is shown in Figure 3 (left side), the prominent maximum values result from UAVs hovering a longer time at the same point. Other than that, the distribution of the detection ranges is dependent on the scenario and the flight path of the UAV. But it can be assumed that the probability of a detection decreases with increasing distances over 30 m. In contrast to that, the false-positive rate (objects or parts of objects erroneously detected as UAVs) increases in distances over 40 m (Figure 3, right side).Figure 3:Left: histogram of the detection range of all UAVs, right: histogram of the detection range of all false positives.4.3Resolution on targetThe detection range is not only limited by the LRCS, but by the resolution of the sensor and the size of the target to be detected. For the HDL-64E rotating at 10 Hz, the horizontal resolution is about 0.17°. This corresponds to a horizontal sampling resolution of 0.15 m on a target in 50 m distance. The vertical resolution of 0.4° leads to a gap of 0.35 m between two scanlines at that distance. Aiming at objects of the size of a mini/micro UAV (see Table 1), it seems doubtful to expect a detection range of more than 50 m. For the VLP-16 scanners, the values for the horizontal resolution are comparable to those of the HDL-64E, but due to the 2° angle between two scanlines, the vertical gap of 0.35 m is reached already at a distance of 10 m. When analyzing the number of points on a detected UAV (see Figure 4) it can be seen that only a small number of UAVs are represented by more than 20 points and most of the detections are performed with less than ten points on the object.Figure 4:Left: histogram of the number of points on the object (correctly detected UAVs), right: histogram of the number of points on the object (false positives).This sparse point density on the targets prevents a further classification of the 3D clusters by exact types of UAVs. Even if a UAV is scanned in low distance and therefore higher point density, it is hard to identify an individual type of UAV. The correlation between detection range and points on the target is also confirmed in Figure 5, where the number of points is plotted against the detection range. If the detection range is above 30 m, there are less than 10 points to be expected on the target. To explain the false positives: apparently single isolated objects in the scene like poles or trees are responsible for the high number of erroneously detected UAVs.Figure 5:Number of points on the object vs. detection range, left: all UAVs, right: false positives.4.4Detection rateIt is hard to give precise numbers like precision and recall for the detection rate, because such calculations would require accurate ground truth information. The ground truth recorded for the UAV’s trajectories is derived from their onboard GPS sensor, but the accuracy of that data is limited due to the low update rate of the GPS readings, given the high speed of the UAVs. On the other hand, the overall FOV of the LiDAR sensors in our system is quite complex, due to the different tilt angles of the sensors (see Section 2.1). In detail, the height of the UAV is crucial for its detectability. The height values measured by GPS are generally more blurred than the horizontal values and while scanning 360° horizontally, the vertical FOV of the LiDAR sensors is strictly limited. Furthermore, due to the number of sensors on our sensor platform MODISSA, the FOV of individual sensors can be restricted by other sensors in the line of sight. Therefore, it is hard to decide if a UAV (with blurred GPS height) should have been detectable by a LiDAR sensor.Although the ground truth information may be imprecise that way, we calculated a detection rate or recall (true positives (TP) divided by the sum of true positives and false negatives (FN)) as well as the precision (TP divided by the sum of TP and false positives (FP)) for our system of four LiDAR sensors. In this analysis, we considered only those measurements that had supplementary ground truth data available. In four scenarios, 2961 UAVs should have been visible in the FOV of the sensors, 1489 of them were detected correctly (TP). 1472 UAVs were visible in the FOV but were not detected (FN) and 3556 objects were erroneously taken as UAVs (FP). Hence the precision can be estimated to 29.5 %. This result is quite poor, but it is linked to the high number of FP which has to be seen in connection with the number of measurements considered (28 224 frames) and the trend to misdetections in ranges above 40 m mentioned above (see Section 4.2 and Figure 3). The recall is estimated to 50.3 %, which should be seen as a lower bound, given the imperfect ground truth information. More than 50 % of all FN are detected in a distance of more than 30 m (Figure 6). Hence 3 m to 30 m seems to be the reasonable range for a stable UAV detection with this kind of sensors.Figure 6:Left: histogram of the detection range of undetected UAVs (FN), right: detected UAVs in scenario 4In a consideration of all measurements, the false positive rate can be estimated to 9.2 % (3651 FP in 39476 frames). That means there is less than one false positive in ten frames (one second). A UAV was detected and tracked precisely as long as it stayed within the FOV. On the right side of Figure 6, all detections of a single UAV in a single scenario are visualized. It can be seen that a new track (new color) is set up and followed every time the UAV enters the FOV.5.CONCLUSIONS AND FUTURE WORKIn this paper, we investigated the potential of 360° LiDAR sensors for UAV detection. We have shown that it is possible to detect UAVs with a high probability, as long as the UAV is in the FOV of the sensor and the distance between sensor and UAV is not too big. A remarkable finding was the reduction of the detection rate for distances above 30 m, which can be traced back to the low point density on targets. In addition, the gap between scanlines allows UAVs to remain undetected in these distances.The major advantage of LiDAR-based UAV detection is the automatically generated 3D coordinate of the UAV. Especially for applications like close-range facility protection and for countermeasures, an accurate trajectory of the UAV is needed. Besides that, LiDAR is robust against changes in illumination and environmental conditions. We participated in a field trial with various sensor technologies; the individual results will be analyzed and compared in the near future. We expect that a fusion of the results of the different sensors can further improve the detection rate. Although a set of different mini/micro UAVs was utilized in the field trial, the presented results do not pay attention to the differences between the UAV types. The impact of the shape, structure and material of different aircrafts should be further examined.6.6.REFERENCES[1] Ganti, S. R., Kim, Y., “Implementation of Detection and Tracking Mechanism For Small UAS,” International Conference on Unmanned Aircraft Systems (ICUAS), Arlington, VA USA, (2016). 10.1109/ICUAS.2016.7502513Google Scholar
        [2] Hu, S., Goldman, G. H., Borel-Donohue, C. C., “Detection of unmanned aerial vehicles using a visible camera system,” Applied Optics Research, Vol. 56, No. 3, (2017).Google Scholar
        [3] Müller, T., “Robust drone detection for day/night counter-UAV with static VIS and SWIR cameras,” Proc. SPIE 10190, Ground/Air Multisensor Interoperability, Integration, and Networking for Persistent ISR VIII, (2017).Google Scholar
        [4] Noetel, D., Johannes, W., Caris, M., Hommes, A., Stanko, S., “Detection of MAVs (Micro Aerial Vehicles) based on millimeter wave radar,” Proc. SPIE 9993, Millimetre Wave and Terahertz Sensors and Technology IX, 999308, (2016).Google Scholar
        [5] Christnacher, F., Hengy, S., Laurenzis, M., Matwyschuk, A., Naz, P., Schertzer, S., Schmitt, G., “Optical and acoustical UAV detection,” Proc. SPIE 9988, Electro-Optical Remote Sensing X, 99880B, (2016).Google Scholar
        [6] Spinello, L., Luber, M., Arras, K. O., “Tracking People in 3D Using a Bottom-Up Top-Down Detector,” IEEE Int. Conf. on Robotics and Automation (ICRA), (2011). 10.1109/ICRA.2011.5980085Google Scholar
        [7] Li, B., Zhang, T., Xia, T., “Vehicle Detection from 3D Lidar Using Fully Convolutional Network,” In Robotics: Science and Systems, (2014).Google Scholar
        [8] Armbruster, W. Hammer, M., “Segmentation, classification, and pose estimation of maritime targets in flash-ladar imagery,” SPIE Proceedings, Volume 8542, Signal and Image Processing I, (2012).Google Scholar
        [9] Laurenzis, M., Bacher, E., Christnacher F., “Experimental and Rendering-based Investigation of Laser Radar Cross-Sections of small Unmanned Aerial Vehicles,” Optical Engineering, 56 (12), 124106, (2017). 10.1117/1.OE.56.12.124106Google Scholar
        [10] https://www.iosb.fraunhofer.de/servlet/is/42840/Google Scholar
        [11] Laurenzis M., Hengy, S., Hammer, M., Hommes, A., Johannes, W., Rassy, O., Bacher, E., Schertzer, S., Poyet, J.-M., “An adaptive sensing approach for detection of small UAV: first investigation of static sensor network and moving sensor platform,” Proc. SPIE 10646 Paper 10646-27, (2018).Google Scholar
        [12] Hammer, M., Hebel, M., Arens, M., “Person detection and tracking with a 360° LiDAR system,” Proc. SPIE 10434, 104340L, (2017).Google Scholar
        [13] Jiang, X., Bunke, H., “Fast Segmentation of Range Images into Planar Regions by Scan Line Grouping,” Machine Vision and Applications 7 (2), pp. 115–122, (1994). 10.1007/BF01215806Google Scholar
        





                    © (2018) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.
                





Citation
Download Citation



Marcus Hammer, Marcus Hebel, Björn Borgmann, Martin Laurenzis, and Michael Arens
"Potential of lidar sensors for the detection of UAVs", Proc. SPIE 10636, Laser Radar Technology and Applications XXIII, 1063605 (10 May 2018); doi: 10.1117/12.2303949; https://doi.org/10.1117/12.2303949











Access the abstract







PROCEEDINGS
7 PAGES



DOWNLOAD PDF



                        SAVE TO MY LIBRARY
                    










SHARE









GET CITATION















                        <
                        Previous Article
|
Next Article
                        >

                







Advertisement











Advertisement












KEYWORDS





                        Unmanned aerial vehicles
                    






                        Sensors
                    






                        LIDAR
                    






                        Clouds
                    






                        Target detection
                    






                        Image segmentation
                    






                        Cameras
                    











RELATED CONTENT






Aerial vehicles collision avoidance using monocular vision

Proceedings of SPIE (October 20 2016)


Lidar-based detection and tracking of small UAVs

Proceedings of SPIE (October 03 2018)


Measuring laser reflection cross sections of small unmanned aerial vehicles...

Proceedings of SPIE (May 04 2017)


3D sensing and imaging for UAVs

Proceedings of SPIE (October 15 2015)


Drogue tracking using 3D flash lidar for autonomous aerial refueling

Proceedings of SPIE (June 08 2011)


Integration of 3D and 2D imaging data for assured navigation...

Proceedings of SPIE (May 08 2009)


Signal processing for imaging and mapping ladar

Proceedings of SPIE (October 05 2011)














Subscribe to Digital Library





Receive Erratum Email Alert













                    Erratum Email Alerts notify you when an article has been updated or the paper is withdrawn.
                



                    Visit My Account to manage your email alerts.
                











  

Email or Username
Forgot your username?




Password
Forgot your password?


  Show




Remember Email/Username on this computer


Remember Password





                No SPIE account? Create an account 
Institutional Access:Sign in with your institutional credentials

 









                The alert successfully saved.
            



                Visit My Account to manage your email alerts.
            



                CLOSE
            







                The alert did not successfully save. Please try again later.
            



                CLOSE
            











        Marcus Hammer, Marcus Hebel, Björn Borgmann, Martin Laurenzis, Michael Arens, "Potential of lidar sensors for the detection of UAVs," Proc. SPIE 10636, Laser Radar Technology and Applications XXIII, 1063605 (10 May 2018);
        


Include:



 Citation Only


 Citation & Abstract





Format:



 RIS (Zotero)


 EndNote


 BibTex


 Medlars


 ProCite


 Ref Works







                DOWNLOAD CITATION
            
















Access provided by  Univ. of Amsterdam














Site Map

 
        Home
    

 
        Conference Papers
    

 
        Conference Presentations
    

 
        Journals
    

 
        eBooks
    

 
        About
    

 
        Subscriptions
    




Information for Authors

 
        Proceedings Authors
    

 
        Journal Authors
    

 
        eBook Authors
    




Information for Reviewers

 
        Reviewer Guidelines
    



Information for Librarians

 
        Resources
    

 
        Subscriptions
    




Contact & Support
+1 888 902 0894(United States)+1 360 685 5580(International)
Hours:8:00 am to 5:00 pm PST
Help | Contact Us


Connect
 

 

 











        SPIE Privacy Policy
    
|

        Terms of Use
    


						© 2019 SPIE
					













                    CONFERENCE PROCEEDINGS
                



        Papers
    



        Presentations
    







        Journals




        Advanced Photonics
    





        Journal of Applied Remote Sensing
    



        Journal of Astronomical Telescopes, Instruments, and Systems
    



        Journal of Biomedical Optics
    



        Journal of Electronic Imaging
    



        Journal of Medical Imaging
    



        Journal of Micro-Nanolithography, MEMS, and MOEMS
    



        Journal of Nanophotonics
    



        Journal of Photonics for Energy
    



        Neurophotonics
    



        Optical Engineering
    





        Ebooks
    





Help |
                Advanced Search >













Keywords/Phrases


Keywords




in

All Fields
Abstract
Author Name
Affiliation
DOI / ISSN / ISBN
Figure & Table Captions
Keywords
Title
Volume Title

Remove



AND
OR
NOT





in

All Fields
Abstract
Author Name
Affiliation
DOI / ISSN / ISBN
Figure & Table Captions
Keywords
Title
Volume Title

Remove



AND
OR
NOT





in

All Fields
Abstract
Author Name
Affiliation
DOI / ISSN / ISBN
Figure & Table Captions
Keywords
Title
Volume Title

Remove



+ Add another field

Search In:




Proceedings


Volume







Journals +


Volume



Issue



Page





Advanced Photonics

Journal of Applied Remote Sensing

Journal of Astronomical Telescopes  Instruments  and Systems

Journal of Biomedical Optics

Journal of Electronic Imaging

Journal of Medical Imaging

Journal of Micro/Nanolithography, MEMS, and MOEMS

Journal of Nanophotonics

Journal of Photonics for Energy

Neurophotonics

Optical Engineering

SPIE Reviews




eBooks +



Field Guide Series

Press Monograph

Spotlight

Tutorial Text

Other Press



Publication Years


Range






Single Year





Clear Form