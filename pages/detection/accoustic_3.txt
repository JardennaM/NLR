url: https://www.researchgate.net/publication/317978772_Drone_detection_by_acoustic_signature_identification








    






 














































(PDF) Drone detection by acoustic signature identification


We use cookies to make interactions with our website easy and meaningful, to better understand the use of our services, and to tailor advertising. For further information, including about cookie settings, please read our Cookie Policy . By continuing to use this site, you consent to the use of cookies.Got itWe value your privacyWe use cookies to offer you a better experience, personalize content, tailor advertising, provide social media features, and better understand the use of our services.To learn more or modify/prevent the use of cookies, see our Cookie Policy and Privacy Policy.Accept CookiestopSee all ›2 CitationsSee all ›17 ReferencesSee all ›2 FiguresDownload citationShare  Facebook Twitter LinkedIn RedditDownload full-text PDFDrone detection by acoustic signature identificationArticle (PDF Available) · January 2017 with 2,489 ReadsDOI: 10.2352/ISSN.2470-1173.2017.10.IMAWM-168  Cite this publication Andrea Bernardini3.29Fondazione Ugo Bordoni Federica Mangiatordi6.79Fondazione Ugo Bordoni Emiliano Pallotti6.41Fondazione Ugo Bordoni Licia CapodiferroAbstractIn the last years, the ductility and easiness of usage of unmanned aerial vehicles (UAV) and their affordable cost have increased the drones use by industry and private users.
However, drones carry the potential of many illegal activities from smuggling illicit material, unauthorized reconnaissance and surveillance of targets and individuals, to electronic and kinetic attacks in the worse threatening scenarios. As a consequence, it has become important to develop effective and affordable countermeasures to report of a drone flying over critical areas. In this context, our research chooses different short term parametrization in time and frequency domain of environmental audio data to develop a machine learning based UAV warning system which employs the support vector machines to understand and recognize the drone audio fingerprint. Preliminary experimental results have shown the effectiveness of the proposed approach.Discover the world's research15+ million members118+ million publications700k+ research projectsJoin for free Figures - uploaded by Andrea BernardiniAuthor contentAll content in this area was uploaded by Andrea BernardiniContent may be subject to copyright. Drone acoustic signature identification framework…  : Classes of the environmental audio frames… Download full-text PDFContent uploaded by Andrea BernardiniAuthor contentAll content in this area was uploaded by Andrea Bernardini on Feb 24, 2018 Content may be subject to copyright. 





Drone detection by acoustic signature identiﬁcationAndrea Bernardini, Federica Mangiatordi, Emiliano Pallotti, Licia Capodiferro; Fondazione Ugo Bordoni; Rome, ItalyAbstractIn the last years, the ductility and easiness of usageof unmanned aerial vehicles (UAV) and their affordable costhave increased the drones use by industry and private users.However, drones carry the potential of many illegal activitiesfrom smuggling illicit material, unauthorized reconnaissance andsurveillance of targets and individuals, to electronic and kineticattacks in the worse threatening scenarios. As a consequence, ithas become important to develop effective and affordable coun-termeasures to report of a drone ﬂying over critical areas. In thiscontext, our research chooses different short term parametrizationin time and frequency domain of environmental audio data todevelop a machine learning based UAV warning system whichemploys the support vector machines to understand and recognizethe drone audio ﬁngerprint. Preliminary experimental resultshave shown the effectiveness of the proposed approach.IntroductionNowadays commercial-grade and consumer-grade dronesare a market product since the technology to control and operateunmanned aircraft is cheap, widely available and fast developing.The drones applications range from hobby and amusement toaerial surveillance and lastly for illegal and criminal activities[1].The increasing drone efﬁciency, the installation of GPS con-trol systems and autopilot allow drones to ﬂy programmedroutes without a pilot for several miles[2]. This multiplies therisk of airspace exposition of sensitive buildings, facilities andpersonals[3]. So in various contexts the necessity of detecting andrestricting the illegal use of drones emerges. However, the speedand varying shape of drones make the discovery of ﬂying dronesa complex and difﬁcult task especially when the unmanned aerialvehicles (UAVs) have small size and a single detection method,using RF or optical sensors, is employed.On the other end the typical acoustic signature of most commer-cial drones suggest the opportunity to detect or boost the existingmonitoring system by using a UAV sound recognition system[4, 5, 6].Speciﬁcally, Mezei proposed a drone sound detection basedon the correlation technique of audio ﬁngerprinting [7].Besides, Souli presented an environmental sound spectrogramSVM classiﬁcation approach built on the reassignment of spectralpatches [8]. This research proposes a cheap and portable dronedetection system, that extracts and classiﬁes the temporal andspectral features of the recorded environmental sounds to ﬁgureout if there is or not a drone near. The proposed system is indepen-dent but integrable with other common detection approaches asmulti-sensor drone trackers that can use optical, thermal, infraredand RF array of sensors.This work generalizes the application context of Mezei approachdeveloping an acoustic signature identiﬁcation framework thatapplies the bag of frames [9] with machine learning techniques.In addition, it exploited audio analysis both in temporal andfrequency domain [10, 11] to obtain an higher accuracy.The paper is organized as it follows. The ﬁrst section describesthe drone acoustic detection framework. The second sectionpresents the selected audio descriptors.The third section discussesthe automatic identiﬁcation of drone sounds by using SVM. Theforth session presents the result. Finally, concluding remarks aregiven in section six.Proposed frameworkIn this paper, a drone detector engine is modelled as anintelligent system able to perceive the context in which it isdeployed by monitoring the environmental sound. When a dronesound is recognized an alert is triggered by the system.The proposed framework for the identiﬁcation of the droneacoustic signature is shown in Fig. (1). Its schema is constitutedby ﬁve main modules that perform the tasks detailed in thefollowing.•Audio acquisitionThe sounds produced by the surrounding environment arepicked up by an audio sensor and converted in a digitalformat by a sound card. To maintain a good time resolutionand a wide frequency bandwidth, it is assumed a samplingrate of 48 kHz and a linear encoding with 16 bits for sample.•PreprocessingEach digital sound segment, recorded in a buffer memory, isbroken into consecutive frames of predetermined duration(5 seconds); then the frames are normalized in the range[-1,1].•Short term analysisTo reduce the amount of data and identify discriminativemeaningful information, each input frame is furthersegmented into sub-frames of 20ms using a movingHamming window with overlap of 10ms. The sub-framesare processed by a bank of ﬁlters to compute the so calledshort term feature in both temporal and frequency domains.•Mid term analysis and frame modellingConsidering the sequence of the extracted local audiofeatures, their statistics are computed on a mid-termwindow of 200ms. The made assumption is that theaudio signal presents homogeneous physical characteristicsduring this temporal segment. Subsequently, the audioframe of 5s is represented by a signature vector, that isobtained by the concatenation of the set of feature statistics,associated with each segment of 200ms in the given frame.60 IS&T International Symposium on Electronic Imaging 2017Imaging and Multimedia Analytics in a Web and Mobile World 2017https://doi.org/10.2352/ISSN.2470-1173.2017.10.IMAWM-168© 2017, Society for Imaging Science and Technology




Figure 1: Drone acoustic signature identiﬁcation framework•Decision makingThe content of the environmental digital audio segment isranked by a set of SVM classiﬁers, operating on framebasis. Speciﬁcally, there is a set of binary classiﬁers thatprocess the audio signature vectors of the input frames. EachSVM classiﬁer is trained according to the paradigm oneagainst one to recognize the drone acoustic signature. If themajority of the frames is labelled with the tag ”drone” intothe given audio segment, it is assumed to recognize a ﬂyingdrone in the surrounding environment.Audio data descriptionA critical issue in the audio pattern recognition problem isthe choice of features for constructing an accurate identiﬁcationsystem. The audio features should be efﬁcient, robust andphysically interpretable so as to obtain a machine processable datarepresentation containing the key properties of the audio signal.In general, the environmental sounds can be generated by varietyof sources under multiple contexts and no assumptions can bemade about their spectral and temporal structure [12]. Besides,the corresponding audio signal is non-stationary in time, so thatthe signal can be assumed locally stationary only on short timerange of 10-30 ms. This implies that the time and spectralbehaviour of the audio signal can be considered practicallyhomogeneous in a time range of few milliseconds.Hence, to capture the heterogeneous audio information, twodifferent temporal scales are considered during the analysis phaseof environmental sound. Speciﬁcally, the audio segments areprocessed on a short time basis of 20ms to discriminate whichset of temporal and frequency features is effective in the droneidentiﬁcation problem. Subsequently, to give prominence tosalient audio features discarding the local detail, a mid term timeanalysis is performed on 200ms.Short term analysisDenoting with x(n)the discrete time representation ofnormalized audio frame, the audio data s(n)of each subframe aregiven by eq.(1).s(n)=x(n)·w(mn)w(n)=0.54 cos⇣2p(mn)L1⌘n2[0,L)(1)whew w(n)is an Hamming window of length Land mits timeshift. To compute the raw features and locally characterizethe corresponding audio waveform and spectrum shape, eachsub-frame s(n)is processed by a bank of speciﬁc ﬁlters on thebasis of the feature algorithms detailed below.In particular, the computed local features are: the Short-TimeEnergy (STE), the Zero Crossing Rate (ZCR) and the TemporalCentroid for time domain; the Spectral Centroid (SC), the SpectralRoll-Off (SRO), the Mel Frequency Cepstrum Coefﬁcients forfrequency domain.•Short Time EnergyIt is computed according to the expression (2) and provides ameasure of the energy variations of the environmental soundover time.ST E =1LL1Âi=0|s(i)|2(2)•Temporal centroidIt is deﬁned as the temporal balancing point of the amplitudedistribution of audio signal. It is expressed as:C=ÂLh=1h·s(i)ÂLh=1s(i)(3)•Zero crossing rateIt counts the average number of times where the audioIS&T International Symposium on Electronic Imaging 2017Imaging and Multimedia Analytics in a Web and Mobile World 2017 61



signal changes its sign within the short-time window. Thisfeatures is particular useful to identify voiced subframe.ZCR =12(L1)·L1Âi=0|sgn(s(i)) sgn(s(i1)) |(4)•Spectral CentroidIt represents the balancing point of audio spectrum p(f)specifying if lower or higher frequencies are contained inthe spectrum.SC =Âff·p(f)Âfp(f)p(f)=|L1Âi=0s(i)·ej2pf/L|2(5)•Spectral roll-offIt deﬁnes the frequency below which a certain amout bof the spectral energy is concentrated. In this work, it isassumed b=0.9SRO =argmaxmmÂf=1p(f)b·FÂf=1p(f)(6)•Mel frequency coefﬁcientsMel frequency cepstral coefﬁcients are the discrete cosinetransform of the mel-scaled log-power spectrum p(f). Themain steps to compute these Mcepstral coefﬁcients aredescribed below.- The Mbanks of Mel ﬁlters are used to map the powerspectrum p(f)onto the mel scale deﬁned by theeq.(7). The frequency responses of these ﬁlter banksare triangular and equally spaced along the mel-scale.fMel =2.595 log10⇣1+f700 ⌘(7)- The energies Emat the output of mth ﬁlter is computedand then compressed into a logarithmic scale. Let beCmthe relative log value.- Given the Mlog energies Cm, the correspondingDCT coefﬁcients are computed and constitute the Melcepstral coefﬁcients of audio signal s(n).ci=MÂk=1Ck·cos⇣p(2i+1)k2M⌘(8)To address the drone sound identiﬁcation problem, 13MFCCs are extracted because it is found that they leaddiscriminative information.Mid term analysis and features aggregationAfter analyzing the subframes into the environmental audioframe, the relative sequences of low level features are processedstatistically on a mid term time window, as mentioned in thesecond section. The goal is to obtain new salient mid term featureswith low sensitivity to the small variations of underlying audiosignal. Then, a set of mid term robust features are aggregated ina global vector that is able to completely describe the perceptualphysical property of the environmental audio frame.The maximization of the expressive power of the audio descriptorcan be obtained by selecting mid term window with a time length,that allows to decorrelate the various components of the vector,i.e. the selected set of mid term features.Let denote {fi}the generic sequence of low level feature intoan audio frame, and let be Nthe number of sub-frame containedinto the generic mid term window wj. Then the Nlocal features{fj+k,k=0,..N1}relative to wjare processed to computethe ﬁrst and second order statistics, written in eq(9).µj=1NN1Âk=0fj+ksj=1NN1Âk=0(fj+kµj)2(9)Given all mid term windows wjin the environmental audioframe, it is considered a selection of these statistics for thevarious raw features to generate a vector of mid term featuresby concatenation. This vector is the global audio descriptorrepresenting the audio signature vector of the frame. Besides, it isthe basic unit processed by the classiﬁer for the identiﬁcation ofdrone sound.Classiﬁer for drone sound identiﬁcationThe drone audio identiﬁcation problem is addressed at framelevel formulating a multiclass environmental audio recognitionproblem. Since the dependency of the mid term feature valuesfrom the properties of environmental sounds properties is notknown a priori, a multiclass SVM based classiﬁer is trained forestimating the multidimensional audio descriptors.This projectual choice is based on two main motivations. First,SVMs are powerful machine learning techniques that are appliedin various pattern recognition problems with excellent results[14, 15]. Secondly, a labelled training data set is the onlyinformation required to implement an SVM.In this work, it is adopted the one-against-one strategy [16] toimplement the SVM based classiﬁer with kclasses for dronesound identiﬁcation.This leads to H=k(k1)/2 binary SV M classiﬁers, each onetrained on data from two classes. The output class is selectedamong those chosen by most classiﬁers according to the max winsstrategy.If two classes obtained identical number of votes it is selectedrandomly one. Denoting with vthe multidimensional audiodescriptor for the input audio frame and with fj(v)the decisionfunction of the generic classiﬁer SV M j,j=1,2,...H, the outputlabel can be expressed by eq(10) with H=k(k1)/2.label(v)=arg max1jH{fj(v)}(10)Given a dataset of elements {vn,n=1,2,..M}belonging totwo different audio classes Aand B, the model of the relativebinary SV M jclassiﬁer is computed exploiting the principle of thestructural risk minimization. This implies minimizing a bound onthe generalization error, rather than minimizing the mean squareerror [13]. Hence, the learning problem for the SV M jcan beformulated as the determination of the optimal separating surfacethat maximize the margin between the two classes Aand B. This62 IS&T International Symposium on Electronic Imaging 2017Imaging and Multimedia Analytics in a Web and Mobile World 2017



margin is given by the perpendicular distance of the closest datapoint vnto the optimal separating surface. In the mapped spacez=f(v), the optimal separating surface becomes the optimaldecision hyperplane wT·z+b=0 and the margin between thetwo classes is represented by the minimum distance dmin of theclosest point vnfrom the bounding plane wT·z+b=0.To model the SV M jclassiﬁer, we search for the optimalvalues of the parameters wand bthat maximize the minimumdistance dmin. These values identify the optimal decisionhyperplane and deﬁne the SV M jdecision function fj(v)as:fj(v)=sgnMÂn=1antnK(v,vn)+bK(v,vn)=f(v)T·f(vn)w=MÂn=1antnf(vn)(11)where {tn,n=1,2,..M}are the target label for the two classes,assuming with tn2[1,1].In general the audio descriptors vnrepresent points linearlynon-separable in the audio descriptor space but which may beseparated in a non-linear way. This corresponds to the adoption ofSV M jwith non-linear kernel function K(v,vn). In particular thiswork consideres all SV M jhaving a Gaussian Radial Basis kernelfunction (RBF) [13] expressed as K(v,vn)=e|vvn|22s2.Experimental resultsTo evaluate the performance of the proposed system fordrone sound recognition, we used a dataset containing ﬁvedifferent typologies of environmental sounds corresponding to:drone ﬂying, nature daytime, street with trafﬁc, train passing,crowd. The dataset is created starting from background soundscollected from the web using a speciﬁc scraper for audioﬁles. The selection of a balanced number of elements for theﬁve main classes are the criteria for the construction of the dataset.Table 1: Classes of the environmental audio framesClasses TotalDrone 868Nature daytime 844Crowd 840Train passing 856Street with trafﬁc 864Following these criteria, the scraper surfs the web discardingall digital audio ﬁle with sampling rate less than 48 KHz.The search process ends when a balanced amount of audioﬁles is downloaded for each class speciﬁed in the queeringset. Subsequently, each found audio ﬁle is manually validatedto verify if it is correctly associated with the given label. Aspecialized software is used to divide each audio segment in frameof 5s and annotate them with the class label relative to the audiosegment. In table 1 it is reported the class of environmentalsounds and the relative number of audio frames in the datasetused for the train and test process. Namely, it corresponds to sixhours of environmental sounds. As suggested by the one againstone strategy, ten SVM binary classiﬁers are trained and tested,one for each possible couple of class. For each SVM classiﬁer,several experiments are performed to determine the optimaldiscriminative subset of aggregated features in combination withthe optimal SVM kernel among those linear, polynomial, RBFand sigmoid.The different choices are compared in terms of accuracy andprecision using the k-fold cross validation procedure (k=5) toprevent the overﬁtting problem.The experiments has revealed that the RBF kernel performedbetter than the others with audio descriptor vectors constituted bythe ﬁrst order statistics of raw features.After deﬁning the typology of SVM kernel, we have trainedthe different binary classiﬁer using speciﬁc subset of constructeddataset. For example in modelling the SVM that classiﬁes droneframes versus crowd frames, we considered 868 segments labeledas drone and 840 segments labeled as crowd. Then the 50% ofﬁles of these two classes is used for training while the remainingfor the test.The obtained performance for the adopted ten SVMs are shownin Table 2 in terms of accuracy and precision.Table 2: SVMs PerformanceSVM-Classiﬁer Accuracy PrecisionDrone/Crowd 0,964 0,984Drone/Nature daytime 0,992 0,983Drone/Train passing 0,978 0.983Drone/Street with trafﬁc 0,964 0,987Crowd/Nature daytime 0,959 0,919Crowd/Train passing 1 1Crowd/Street with trafﬁc 0,8911 0,782Nature daytime/Street with trafﬁc 0,991 1Nature daytime/Train passing 0,996 0,991Street with trafﬁc/Train passing 1 1To conclude we have tested the overall SVMs based clas-siﬁer, giving in input the same test frame to all ten SVM. Byusing the max wins strategy, the output label was identiﬁed. Theresulting precision of the drone recognition is 98,3 % .ConclusionsThis work investigates the efﬁcacy of machine learningtechniques to face the problem of drone detection in the contextof critical areas protection. To this end an empirical analysisof the environmental sounds, recorded by the audio sensors inthe critical areas, is performed. The extracted time-frequencyﬁngerprint is adopted by the warning system to recognize dronesounds.References[1] E. Vattapparamban, . Gven, A. . Yurekli, K. Akkaya and S. Uluaa,”Drones for smart cities: Issues in cybersecurity, privacy, and publicsafety,” 2016 International Wireless Communications and MobileComputing Conference (IWCMC), Paphos, 2016, pp. 216-221.[2] A. Harrington,”Who controls the drones? [Regulation UnmannedAircraft],” in Engineering & Technology, vol. 10, no. 2, pp. 80-83,March 2015.IS&T International Symposium on Electronic Imaging 2017Imaging and Multimedia Analytics in a Web and Mobile World 2017 63



[3] K. Hartmann and K. Giles, ”UAV exploitation: A new domain forcyber power,” 2016 8th International Conference on Cyber Conﬂict(CyCon), Tallinn, 2016, pp. 205-221.[4] S. Chu, S. Narayanan, C. c. J. Kuo and M. J. Mataric, ”Where am I?Scene Recognition for Mobile Robots using Audio Features,” 2006IEEE International Conference on Multimedia and Expo, Toronto,Ont., 2006, pp. 885-888.[5] A. Rabaoui, M. Davy, S. Rossignol and N. Ellouze, ”Using One-ClassSVMs and Wavelets for Audio Surveillance,” in IEEE Transactionson Information Forensics and Security, vol. 3, no. 4, pp. 763-775,Dec. 2008.[6] R. Serizel, V. Bisot, S. Essid and G. Richard, ”Machine listeningtechniques as a complement to video image analysis in forensics,”2016 IEEE International Conference on Image Processing (ICIP),Phoenix, AZ, USA, 2016, pp. 948-952.[7] J. Mezei, V. Fiaska and A. Molnr, ”Drone sound detection,” 201516th IEEE International Symposium on Computational Intelligenceand Informatics (CINTI), Budapest, 2015, pp. 333-338.[8] Sameh Souli, Zied Lachiri, and Alexander Kuznietsov,”Using ThreeReassigned Spectrogram Patches and Log-Gabor Filter for AudioSurveillance Application”. In Proceedings, Part I, of the 18thIberoamerican Congress on Progress in Pattern Recognition, ImageAnalysis, Computer Vision, and Applications - Volume 8258 (CIARP2013), Jos Ruiz-Shulcloper and Gabriella Sanniti Di Baja (Eds.),Vol. 8258. Springer-Verlag New York, Inc., New York, NY, USA,527-534.[9] J.-J. Aucouturier, B. Defreville, and F. Pachet, The bag-of-framesapproach to audio pattern recognition: A sufﬁcient model for urbansoundscapes but not for polyphonic music, J. Acoust. Soc. Amer., vol.122, no. 2, pp. 881891, Aug. 2007.[10] Xuan Guo, Yoshiyuki Toyoda, Huankang Li, Jie Huang, ShuxueDing, and Yong Liu. 2012. ”Environmental sound recognition usingtime-frequency intersection patterns”. Appl. Comp. Intell. Soft Com-put. 2012, Article 2 (January 2012), 6 pages[11] Mitrovi, Dalibor, Matthias Zeppelzauer, and Christian Breiteneder.”Features for content-based audio retrieval.” Advances in computers78 (2010): 71-150.[12] S. Chu, S. Narayanan and C. C. J. Kuo, ”Environmental SoundRecognition With TimeFrequency Audio Features,” in IEEE Trans-actions on Audio, Speech, and Language Processing, vol. 17, no. 6,pp. 1142-1158, Aug. 2009.[13] C. Bishop, ”Pattern Recognition and Machine Learning”, Informa-tion Science and Statistics ed.Springer, 2007[14] S. Theodoridis and K. Koutroumbas, Pattern Recognition, Aca-demic Press. ed., 2008.[15] P. Dhanalakshmi, S. Palanivel and V. Ramalingam, ”Classiﬁcationof audio signals using SVM and RBFNN,” Expert Systems withApplications: An International Journal, vol. 36, pp. 6069-6075, 2009.[16] Hsu,C.-W.,Lin,C.-C,”A comparison of methods for multi-class sup-port vector machines”.J.IEEE Transactions on Neural Networks13,pp.415-425, 2002Author BiographyAndrea Bernardini received his Dr. Ing. degree in ComputerEngineering at the University of Rome ”Roma Tre”. In 2010, he wasVisiting Researcher at the Institute for Computing, Information andCognitive Systems (ICICS) of the University of British Columbia (UBC ).In 2002 he joined The Fondazione Bordoni, where he works as researcherin Information processing and management Department. His researchinterests include User Experience, Data Mining and User Modeling.Licia Capodiferro received her Dr. Ing. degree in ElectronicEngineering from the University of Rome La Sapienza, Italy. In 1987 shejoined the Fondazione Ugo Bordoni where she currently works as head ofthe Department of Information Processing and Management. Her mainresearch interests are in the ﬁeld of multimedia processing, with a focuson algorithms that allow the use of images and videos on the differenttypes of terminals.Federica Mangiatordi received the M.Sc. Degree in ElectronicEngineering at University of Rome La Sapienza and the PhD inElectronic Materials, Optoelectronics and Microsystems from theUniversity of Roma TRE. She works at Fondazione Ugo Bordoni from2007. Her research interest concern multimedia retrieval, imagerestoration algorithms, novel metrics for full reference and no-referenceimage objective quality assessment.Emiliano Pallotti received the Laurea Degree in Telecommunica-tions Engineering at the University of Rome La Sapienza, Italy, andPhD in Electronic Materials, Optoelectronics and Microsystems fromthe University of Roma TRE. In 2007 he joined the Fondazione UgoBordoni where his research activities are in the ﬁeld of on computationalalgorithms and video process- ing techniques based on multiresolutionimage representation in wavelet domain.64 IS&T International Symposium on Electronic Imaging 2017Imaging and Multimedia Analytics in a Web and Mobile World 2017


Citations (2)References (17)... In [36], time and frequency domain acoustic features are extracted from micro-UAV audio recordings. These features are used to train a multi-class support vector machine (SVM) for micro-UAV identification.  ...Micro-UAV Detection and Classification from RF Fingerprints Using Machine Learning TechniquesPreprintFull-text availableJan 2019 Martins Ezuma Fatih Erden Chethan Kumar Anjinappa Ismail GuvencThis paper focuses on the detection and classification of micro-unmanned aerial vehicles (UAVs) using radio frequency (RF) fingerprints of the signals transmitted from the controller to the micro-UAV. In the detection phase, raw signals are split into frames and transformed into the wavelet domain. A Markov models-based naive Bayes approach is used to check for the presence of a UAV in each frame. In the classification phase, unlike the traditional approaches that rely solely on time-domain signals and corresponding features, the proposed technique uses the energy transient signal. This approach is more robust to noise and can cope with different modulation techniques. First, the normalized energy trajectory is generated from the energy-time-frequency distribution of the raw control signal. Next, the start and end points of the energy transient are detected by searching for the most abrupt changes in the mean of the energy trajectory. Then, a set of statistical features is extracted from the energy transient. Significant features are selected by performing neighborhood component analysis (NCA) to keep the computational cost of the algorithm low. Finally, selected features are fed to several machine learning algorithms for classification. The algorithms are evaluated experimentally using a database containing 100 RF signals from each of 14 different UAV controllers. The signals are recorded wirelessly using a high-frequency oscilloscope. The data set is randomly partitioned into training and test sets for validation with the ratio 4:1. Ten Monte Carlo simulations are run and results are averaged to assess the performance of the methods. All the micro-UAVs are detected correctly and an average accuracy of 96.3% is achieved using the k-nearest neighbor (kNN) classification. Proposed methods are also tested for different signal-to-noise ratio (SNR) levels and results are reported.ViewShow abstractDrone Detection Using Convolutional Neural Networks with Acoustic STFT FeaturesConference PaperNov 2018 Yoojeong SeoBeomhui JangSungbin ImViewShow moreDrones for smart cities: Issues in cybersecurity, privacy, and public safetyConference PaperSep 2016Edwin Vattapparamban Ismail GuvencAli I. Yurekli Selcuk UluagacViewMachine listening techniques as a complement to video image analysis in forensicsConference PaperSep 2016Romain SerizelVictor Bisot Slim Essid Gaël RichardViewUAV exploitation: A new domain for cyber powerConference PaperFull-text availableMay 2016 Kim Hartmann Keir GilesThe risks of military unmanned aerial vehicles (UAVs) being subjected to electronic attack are well recognised, especially following high-profile incidents such as the interception of unencrypted video feeds from UAVs in Iraq and Israel, or the diversion and downing of a UAV in Iran. Protection of military UAV assets rightly focuses on defence against sophisticated cyber penetration or electronic attack, including data link intercepts and navigational spoofing. Offensive activity to counter adversary drone operations presumes a requirement for high-end electronic attack systems. However, combat operations in eastern Ukraine in 2014–16 have introduced an entirely new dimension to UAV and counter-UAV operations. In addition to drones with military-grade standards of electronic defence and encryption, a large number of civilian or amateur UAVs are in operation in the conflict. This presents both opportunities and challenges to future operations combating hybrid threats. Actual operations in eastern Ukraine, in combination with studies of potential criminal or terrorist use of UAV technologies, provide indicators for a range of aspects of UAV use in future conflict. However, apart from the direct link to military usage, UAVs are rapidly approaching ubiquity with a wide range of applications reaching from entertainment purposes to border patrol, surveillance, and research, which imposes an indirect security and safety threat. Issues associated with the unguarded use of drones by the general public range from potentially highly dangerous situations such as failing to avoid controlled airspace, to privacy violations. Specific questions include attribution of UAV activities to the individuals actually directing the drone; technical countermeasures against hacking, interception or electronic attack; and options for controlling and directing adversary UAVs. Lack of attribution and security measures protecting civilian UAVs against electronic attack, hacking or hijacking, with the consequent likelihood of unauthorised use or interception, greatly increases the complication of each of these concerns.ViewShow abstractDrone sound detectionConference PaperFull-text availableNov 2015Jozsef MezeiViktor Fiaska Molnar AndrasViewWho controls the drones?ArticleMar 2015A. HarringtonCommercial drones are now available to the public at a steep price, but it possess a threat if it gets into a wrong hand. The Federal Aviation Authority (FAA) continues to set rules that make it impossible for companies like Amazon and Google to take drone commercialization into the retail delivery space to any significant extent. Amazon is talking about 30-minute drone deliveries, which would entail setting up a vast number of well-stocked warehouses distributed right across Europe and the US, rather than its current central distribution model. Richard Hardwick, a director of a Derbyshire-based design consultancy Fibreflight, which advises UAV manufacturers on engineering, aerodynamic and composite material issues related to the manufacture of their aircraft, says he is very happy with the rules the Civil Aviation Authority (CAA) currently has in place. Professor David Hastings Dunn from the University of Birmingham, a member of the Policy Commission, warns that even without a payload, drones represent a potential threat that is as yet unaccounted for in conventional risk assessments. Their size, cost and ease of use makes small drones ideal devices to be swarmed against vulnerable targets.ViewShow abstractPattern Recognition and Machine Learning ErrataArticleJan 2006Christopher M. BishopViewEnvironmental Sound Recognition Using Time-Frequency Intersection PatternsArticleFull-text availableMay 2012iCAST International Conference on Awareness Science and TechnologyXuan GuolYoshiyuki ToyodaHuankang LiYong LiulEnvironmental sound recognition is an important function of robots and intelligent computer systems. In this research, we tried to use a multi-stage perceptron type neural network system for environmental sound recognition. The input data is the one-dimensional combination of instantaneous spectrum at power peak and the power pattern in time domain. Since for almost environmental sounds, their spectrum changes are not remarkable compared with speech or voice, the combination of power and frequency pattern will preserve the major features of environmental sounds but with drastically reduced data. Two experiments were conducted using an original database and a database created by the RWCP. The recognition rate for about 45 data kinds of environmental sound was about 92%. The merit of this method is the use of a one-dimensional input which combines the power pattern and the instantaneous spectrum of sound data. Comparing with the method using only instantaneous spectrum, the new method are sufficient for larger sound database and the recognition rate was increased about 12%. The results are also comparable with the methods of HMM, while those methods require 2-dimensional spectrum time series data and more complicated computation.ViewShow abstractFeatures for Content-Based Audio RetrievalDataFull-text availableNov 2012ADV COMPUT Dalibor Mitrovic Matthias Zeppelzauer Christian BreitenederToday, a large number of audio features exists in audio retrieval for different purposes, such as automatic speech recognition, music information retrieval, audio segmentation, and environmental sound retrieval. The goal of this chapter is to review latest research in the context of audio feature extraction and to give an application-independent overview of the most important existing techniques. We survey state-of-the-art features from various domains and propose a novel taxonomy for the organization of audio features. Additionally, we identify the building blocks of audio features and propose a scheme that allows for the description of arbitrary features. We present an extensive literature survey and provide more than 200 references to relevant high-quality publications.ViewShow abstractFeatures for Content-Based Audio RetrievalDataFull-text availableNov 2012 Dalibor Mitrovic Matthias Zeppelzauer Christian BreitenederToday, a large number of audio features exists in audio retrieval for different purposes, such as automatic speech recognition, music information retrieval, audio segmentation, and environmental sound retrieval. The goal of this chapter is to review latest research in the context of audio feature extraction and to give an application-independent overview of the most important existing techniques. We survey state-of-the-art features from various domains and propose a novel taxonomy for the organization of audio features. Additionally, we identify the building blocks of audio features and propose a scheme that allows for the description of arbitrary features. We present an extensive literature survey and provide more than 200 references to relevant high-quality publications.ViewShow abstractEnvironmental Sound Recognition With Time–Frequency Audio FeaturesArticleFull-text availableSep 2009IEEE T AUDIO SPEECH Selina Chu Shrikanth S Narayanan C.-C. Jay KuoThe paper considers the task of recognizing environmental sounds for the understanding of a scene or context surrounding an audio sensor. A variety of features have been proposed for audio recognition, including the popular Mel-frequency cepstral coefficients (MFCCs) which describe the audio spectral shape. Environmental sounds, such as chirpings of insects and sounds of rain which are typically noise-like with a broad flat spectrum, may include strong temporal domain signatures. However, only few temporal-domain features have been developed to characterize such diverse audio signals previously. Here, we perform an empirical feature analysis for audio environment characterization and propose to use the matching pursuit (MP) algorithm to obtain effective time-frequency features. The MP-based method utilizes a dictionary of atoms for feature selection, resulting in a flexible, intuitive and physically interpretable set of features. The MP-based feature is adopted to supplement the MFCC features to yield higher recognition accuracy for environmental sounds. Extensive experiments are conducted to demonstrate the effectiveness of these joint features for unstructured environmental sound classification, including listening tests to study human recognition capabilities. Our recognition system has shown to produce comparable performance as human listeners.ViewShow abstractShow moreJoin ResearchGate to find the people and research you need to help your work.15+ million members118+ million publications700k+ research projectsJoin for freeRecommendationsDiscover more publications, questions and projects in DronesProjectMultimedia Federica Mangiatordi Emiliano Pallotti Andrea BernardiniView projectProjectSmart Microgrids Federica Mangiatordi Emiliano PallottiPaolo Del VecchioView projectProjectIntelligent Retrieval in Multimedia Archives Federica Mangiatordi Emiliano PallottiLicia Capodiferro[...]P. SitàView projectProjectSmart grid  requirements Emiliano PallottiView projectArticleNarrow-band autocorrelation function features for the automatic recognition of acoustic environmentsJuly 2013 · The Journal of the Acoustical Society of America Xavier Valero Francesc AlíasAcoustic environments are typically composed of multiple sound sources of different typologies, making them especially complex to model and parameterize. To develop an automatic acoustic environment recognition system, this work proposes a spectro-temporal signal parameterization technique inspired by human perception. The proposed parameters are derived from the analysis of the autocorrelation ... [Show full abstract] function of narrow-band signals (NB-ACF) obtained from an auditory gammatone filter bank. Five features related to acoustic phenomena are extracted from the NB-ACF to parameterize sound signals. Experiments are conducted on a 4 h sound database (composed of 15 different acoustic environments) employing four different machine learning techniques: K-nearest neighbor, Gaussian mixture models, neural networks and support vector machines. The averaged recognition rates increase by 4.5% when using the proposed NB-ACF features instead of the popular Mel frequency cepstral coefficients. The improvement is even greater (5.6%) when the features are tested in acoustic environments from unknown locations. These results are attributed to the better modeling of the acoustic environments thanks to the complementarity of the spectro-temporal features derived from NB-ACF analysis.Read moreArticleMultimedia analytics platform for profiling keywords embedded in photo cataloguesJanuary 2018 Federica Mangiatordi Andrea Bernardini Emiliano PallottiLicia CapodiferroRead moreConference PaperLecture Notes in Computer ScienceNovember 2013 Sameh Souli Zied Lachiri Alexander KuznietsovIn this paper, we propose a robust environmental sound spectrogram classification approach; its purpose is surveillance and security applications based on the reassignment method and log-Gabor filters. Besides, the reassignment method is applied to the spectrogram to improve the readability of the time-frequency representation, and to assure a better localization of the signal components. In this ... [Show full abstract] approach the reassigned spectrogram is passed through a bank of 12 log-Gabor filter concatenation applied to three spectrogram patches, and the outputs are averaged and underwent an optimal feature selection procedure based on a mutual information criterion. The proposed method is tested on a large database consists of 1000 environmental sounds belonging to ten classes. The averaged recognition accuracy is of order 90.87% which obtained using the multiclass support vector machines (SVM’s).Read moreConference PaperUsing single log-Gabor filter and reassignment method for audio classification applicationsMarch 2015 Sameh Souli Zied LachiriWe present a robust environmental sound classification approach, based on reassigned spectrogram and log-Gabor filters. In this method, reassigned spectrograms are passed through an appropriate log-Gabor filter and the outputs are underwent an optimal feature selection procedure based on mutual information criteria. The evaluation of this classification system is performed on a corpus of 6 ... [Show full abstract] environmental sounds classes. The best performance was obtained using multi-class support vector machines (SVM's), producing an average classification accuracy of the order 91.77%.Read moreDiscover moreDownload citationWhat type of file do you want? RIS BibTeX Plain TextWhat do you want to download? Citation only Citation and abstractDownloadInterested in research on Drones?Join ResearchGate to discover and stay up-to-date with the latest research from leading experts in Drones and many other scientific topics.Join for freeorDiscover by subject areaRecruit researchersJoin for freeLoginEmail Tip: Most researchers use their institutional email address as their ResearchGate loginPasswordForgot password? Keep me logged inLog inorContinue with LinkedInContinue with GoogleWelcome back! Please log in.Email · HintTip: Most researchers use their institutional email address as their ResearchGate loginPasswordForgot password? Keep me logged inLog inorContinue with LinkedInContinue with GoogleNo account? Sign upAboutNewsCompanyCareersSupportHelp centerFAQBusiness solutionsRecruitingAdvertising© ResearchGate 2019. All rights reserved.ImprintTermsPrivacy