url: https://www.researchgate.net/publication/318253221_Fusion_of_Multispectral_Imagery_and_Spectrometer_Data_in_UAV_Remote_Sensing








    






 













































(PDF) Fusion of Multispectral Imagery and Spectrometer Data in UAV Remote Sensing


We use cookies to make interactions with our website easy and meaningful, to better understand the use of our services, and to tailor advertising. For further information, including about cookie settings, please read our Cookie Policy . By continuing to use this site, you consent to the use of cookies.Got itWe value your privacyWe use cookies to offer you a better experience, personalize content, tailor advertising, provide social media features, and better understand the use of our services.To learn more or modify/prevent the use of cookies, see our Cookie Policy and Privacy Policy.Accept CookiestopSee all ›9 CitationsSee all ›32 ReferencesSee all ›5 FiguresDownload citationShare  Facebook Twitter LinkedIn RedditDownload full-text PDFFusion of Multispectral Imagery and Spectrometer Data in UAV Remote SensingArticle (PDF Available) in Remote Sensing 9(7):696 · July 2017 with 504 ReadsDOI: 10.3390/rs9070696  Cite this publication Chuiqing Zeng16.52Environmental and Climate Change Canada Douglas J. King34.57Carleton University Murray Richardson26.5Carleton University Bo ShanAbstractHigh spatial resolution hyperspectral data often used in precision farming applications are not available from current satellite sensors, and difficult or expensive to acquire from standard aircraft. Alternatively, in precision farming, unmanned aerial vehicles (UAVs) are emerging as lower cost and more flexible means to acquire very high resolution imagery. Miniaturized hyperspectral sensors have been developed for UAVs, but the sensors, associated hardware, and data processing software are still cost prohibitive for use by individual farmers or small remote sensing firms. This study simulated hyperspectral image data by fusing multispectral camera imagery and spectrometer data. We mounted a multispectral camera and spectrometer, both being low cost and low weight, on a standard UAV and developed procedures for their precise data alignment, followed by fusion of the spectrometer data with the image data to produce estimated spectra for all the multispectral camera image pixels. To align the data collected from the two sensors in both the time and space domains, a post-acquisition correlation-based global optimization method was used. Data fusion, to estimate hyperspectral reflectance, was implemented using several methods for comparison. Flight data from two crop sites, one being tomatoes, and the other corn and soybeans, were used to evaluate the alignment procedure and the data fusion results. The data alignment procedure resulted in a peak R2 between the spectrometer and camera data of 0.95 and 0.72, respectively, for the two test sites. The corresponding multispectral camera data for these space and time offsets were taken as the best match to a given spectrometer reading, and used in modelling to estimate hyperspectral imagery from the multispectral camera pixel data. Of the fusion approaches evaluated, principal component analysis (PCA) based models and Bayesian imputation reached a similar accuracy, and outperformed simple spline interpolation. Mean absolute error (MAE) between predicted and observed spectra was 17% relative to the mean of the observed spectra, and root mean squared error (RMSE) was 0.028. This approach to deriving estimated hyperspectral image data can be applied in a simple fashion at very low cost for crop assessment and monitoring within individual fields.Discover the world's research15+ million members118+ million publications700k+ research projectsJoin for free Figures - available via license: CC BY 4.0Content may be subject to copyright. Illustration of unmanned aerial vehicle (UAV) flight system and the concept of data fusion. " FOV " stands for field of view.…  The study site with the planned flight path (a), and the actual flight path (b) draped over a Google Earth image, taken on a later date (3 September 2016). (Site 1 at bottom left: tomato field; Site 2 in upper right: corn and soybean fields).…  . Evaluation of hyperspectral data and multispectral image fusion result over the two agriculture sites.…  +1An example of the three types of data fusion compared with an observed spectrum over a crop sample at Site 1. The " PCA " is the " PCA_TSR " approach, " trimmed scores regression " defined in Section 3.2; the " Bayesian " is the " Bayesian_Gibbs " (Bayesian method with Gibbs sampling [27]) as defined in Section 3.2. " MS bands " are the spline interpolated reflectances derived directly from the multispectral (MS) camera data.… Available via license: CC BY 4.0Content may be subject to copyright.Download full-text PDF 





 Remote Sens. 2017, 9, 696; doi:10.3390/rs9070696  www.mdpi.com/journal/remotesensing Article Fusion of Multispectral Imagery and Spectrometer Data in UAV Remote Sensing   Chuiqing Zeng 1,*, Douglas J. King 1, Murray Richardson 1 and Bo Shan 2 1  Department of Geography and Environmental Studies, Carleton University, 1125 Colonel By Dr.,   Ottawa, ON K1S 5B6, Canada; doug.king@carleton.ca (D.J.K.); Murray.Richardson@carleton.ca (M.R.) 2  A&L Canada Laboratories, 2136 Jetstream Rd., London, ON N5V 3P5, Canada; bshan@alcanada.com * Correspondence: chqzeng@gmail.com Academic Editors: Farid Melgani and Prasad S. Thenkabail Received: 24 April 2017; Accepted: 2 July 2017; Published: 6 July 2017 Abstract: High spatial resolution hyperspectral data often used in precision farming applications are not available from current satellite sensors, and difficult or expensive to acquire from standard aircraft. Alternatively, in precision farming, unmanned aerial vehicles (UAVs) are emerging as lower cost and more flexible means to acquire very high resolution imagery. Miniaturized hyperspectral sensors have been developed for UAVs, but the sensors, associated hardware, and data processing software are still cost prohibitive for use by individual farmers or small remote sensing firms. This study simulated hyperspectral image data by fusing multispectral camera imagery and spectrometer data. We mounted a multispectral camera and spectrometer, both being low cost and low weight, on a standard UAV and developed procedures for their precise data alignment, followed by fusion of the spectrometer data with the image data to produce estimated spectra for all the multispectral camera image pixels. To align the data collected from the two sensors in both the time and space domains, a post-acquisition correlation-based global optimization method was used. Data fusion, to estimate hyperspectral reflectance, was implemented using several methods for comparison. Flight data from two crop sites, one being tomatoes, and the other corn and soybeans, were used to evaluate the alignment procedure and the data fusion results. The data alignment procedure resulted in a peak R2 between the spectrometer and camera data of 0.95 and 0.72, respectively, for the two test sites. The corresponding multispectral camera data for these space and time offsets were taken as the best match to a given spectrometer reading, and used in modelling to estimate hyperspectral imagery from the multispectral camera pixel data. Of the fusion approaches evaluated, principal component analysis (PCA) based models and Bayesian imputation reached a similar accuracy, and outperformed simple spline interpolation. Mean absolute error (MAE) between predicted and observed spectra was 17% relative to the mean of the observed spectra, and root mean squared error (RMSE) was 0.028. This approach to deriving estimated hyperspectral image data can be applied in a simple fashion at very low cost for crop assessment and monitoring within individual fields.   Keywords: UAV; data alignment; data fusion; precision farming; spectrometer; multispectral image  1. Introduction   Hyperspectral sensors with many narrow spectral bands have been shown to be able to characterize vegetation type, health, and function [1–4]. Compared with multispectral imagery, hyperspectral data were reported to perform better in modelling vegetation chlorophyll content [5]. They can also be used to calculate narrow band indices for modelling crown temperature, carotenoids, fluorescence, and plant 




Remote Sens. 2017, 9, 696    2 of 20 disease [6,7], as well as crop growth period [8], soil status [9], net photosynthesis, and crop water stress [10], amongst other vegetation parameters. In precision farming applications, hyperspectral data with high spatial resolution are required [4,11], but such data are currently not available from satellite sensors. Hyperspectral sensors designed for standard aircraft are generally expensive, and such aircraft require significant infrastructure, maintenance, and personnel resources. Unmanned aerial vehicles (UAVs) are a rapidly evolving and flexible platform for remote sensing. They offer a promising alternative to standard aircraft in terms of timing flexibility and capability to fly at very low altitudes, thereby acquiring imagery of very high spatial resolution. Small, low cost UAVs are particularly advantageous for applications in small individual farm management. The improvement of such UAV platforms and the miniaturization of sensors have stimulated much remote sensing research and the development of new systems [12]. For example, Jaakkola, et al. [13] developed a multiple sensor platform, with the spectrometer and GPS units weighing 3.9 kg. More recently, hyperspectral sensors of about 2 kg or less have been developed for small UAVs [14,15].   When the total system requirements are considered, however, hyperspectral imaging systems are often too expensive and complicated for small applications-based companies or individual farmers without aviation and remote sensing systems expertise. Besides the sensor itself, the payload requirements also generally include accurate GPS/IMU instrumentation, and an on-board computer for effective data collection. These, combined with other sensors such as a standard RGB camera, may necessitate a larger and more expensive UAV platform, driving up system costs. In addition, hyperspectral sensors acquire data in pushbroom or whiskbroom mode, one image line at a time. The image geometry is therefore more affected by UAV rotations than for a frame camera, which acquires imagery over a two dimensional space in each exposure. The post-acquisition data processing and cost to correct such line-by-line geometric distortions is therefore significant with hyperspectral sensors.   To develop a means for low cost and simplified acquisition of hyperspectral data, this study took an alternative approach. The goal was to fuse the high spectral information content of a low cost miniature spectrometer with the high spatial information content of a low cost multispectral camera system. Using a spectrometer alone in a UAV can provide excellent spectral information [16], but it can only collect samples of a limited number of ground locations, and the footprint in which radiance is collected does not contain explicit spatial information as in a raster image. By exploiting the relationship between such sample-based spectrometer measured spectra, and image data from a frame-based multispectral sensor, hyperspectral data can be estimated at all locations (pixels) in the image where spectrometer data do not exist, thereby deriving an estimated hyperspectral image. The estimated hyperspectral image is in a 2D frame format, which eliminates the need for the geometric processing required for standard hyperspectral line sensors. Moreover, such a system has the potential to cost a fraction of current UAV-based hyperspectral systems and reuse existing cameras and equipment.   Challenges of data collection via multiple low cost sensors include sensor calibration, alignment and data processing. Calibration of UAV hyperspectral sensor data has been conducted using calibration targets in the field [17,18], and lab-based spectrometry with lamp reference sources [19], to evaluate the accuracy of observed hyperspectral data and the dominant noise sources, such as dark current, sensor temperature, atmosphere, and weather [20]. Sensor alignment has been a consideration for multiple sensors of the same type [21], and for different sensors [22]. In low cost UAV multiple sensor systems, it is not easy, or possible, to optically align sensors in a mount that will maintain the precise alignment in the field over multiple flight hours. Thus, matching data from multiple sensors requires post-flight consideration of their spatial misalignment, as well as their data–rate timing differences. Once data alignment from the two sensors has been achieved, fusion can be accomplished by using coincident data from both sensors (i.e., data acquired at the same locations) as training data to build a model. Subsequently, the model can be used to estimate hyperspectral reflectance at other locations where only the multispectral camera has acquired data.   



Remote Sens. 2017, 9, 696    3 of 20 The objectives of this research were to develop methods for (1) alignment of data collected from a spectrometer and a multispectral camera system, (2) fusion of these two data types to provide estimated hyperspectral data at each pixel in the multispectral camera imagery.   2. Methodology 2.1. The Conceptual Framework of Data Acquisition and Fusion The remote sensing system for the proposed data fusion process includes a multispectral imaging sensor and a spectrometer; for our purposes, they are mounted on a small UAV as shown in Figure 1, but they could be mounted on any in situ or airborne platform. Mounting the two sensors is conducted to visually achieve alignment of their optical axes (e.g., with the spectrometer and camera housings parallel). The multispectral sensor acquires 2D multiple band images from separate cameras, each with a specific bandpass filter, while the spectrometer collects one full spectrum sample per measurement within a near circular footprint determined by the angle of view (optics) and the platform altitude and orientation. The details of our experimental UAV system, settings and camera configuration are further described in Section 3. Given an appropriate frame rate or GPS-controlled camera triggering is used, the multispectral camera images should cover the required area; for most applications a mosaic must be produced from multiple images. The spectrometer samples will cover a set of footprints, each being much larger than the multispectral image pixels, and these footprints may not cover the whole study area, due to the spectrometer data rate and UAV velocity. The goal is to associate individual reflectance measurements from the 1D spectrometer, with pixels in the 2D multispectral images that fall within the field of view of the spectrometer. The approach taken in this study was to develop post-acquisition processing methods to optimize the alignment of the data from the two sensors, by evaluating their timing and spatial correlations. Details are provided in Section 2.2. MultispectralCameraUAVSpectrometer900nm800nm720nm680nm550nm490nm wavelengthreflectanceMultispectral imageSpectrometer footprintFOV Figure 1. Illustration of unmanned aerial vehicle (UAV) flight system and the concept of data fusion. “FOV” stands for field of view. Once data alignment has been optimized, fusion of the two data types is conducted by cross calibrating coincident sample points of full spectrum spectrometer data and multispectral image data, 



Remote Sens. 2017, 9, 696    4 of 20 and using that relationship to estimate full spectrum reflectance at all pixels in the multispectral camera images. Many spectra acquired by the spectrometer are selected to build a training dataset of representative ground features (e.g., soil, corn, wheat). A small proportion of the measured spectra can be set aside as independent reference data to assess the accuracy of the estimated hyperspectral imagery. More details about the data fusion methods are explained in Section 2.3. 2.2. Alignment of Spectrometer and Multispectral Camera Data The data from the two sensors need to be aligned before fusion is implemented, due to misalignment caused by sensor timing differences and inclined optical axes. This study assumes that the highest data correlation should be found when the multispectral camera and spectrometer data are optimally aligned. Since data misalignment occurs in both the time domain and spatial domain, this study employs a global alignment procedure that considers both time and space offsets simultaneously.   Spectrometer data are acquired in raw format as arbitrary intensity units. A white barium sulfate disc representing close to 100% Lambertian reflectance is used before and after flights to measure reference intensity. Reflectance of ground targets is calculated as   ′()=(,)(,) (1) where  ′ is nominal surface reflectance,  is wavelength, I is the radiance intensity recorded by the spectrometer for ground target surfaces (S) and the white reference (R), respectively. Dark noise (measured with the lens cap on) is subtracted from all intensity readings I  at each wavelength before reflectance is calculated. A second skyward pointing spectrometer attached with cosine corrector (180° field of view or “FOV”) that can be on the UAV or on the ground (to minimize UAV weight) measures downwelling irradiance. Nominal reflectance ′ is further adjusted for illumination conditions as measured by the skyward pointing spectrometer: ()=()×()() =()×(,)(,) (2) where E is the downwelling irradiance [23] measured by the skyward spectrometer. E(S) and E(R) are measured at the same time as the air-unit intensity (I) measurements of the target surfaces (S) and white reference (R), respectively. The sum of irradiance over all wavelengths as a coefficient is used to represent the total sun irradiance at a given moment. For the multispectral sensor, each spectral band has a transmittance function over a wider range of wavelengths than the spectrometer bands. For instance, the red band of the camera used in this study is centered at 680 nm, with a Gaussian transmittance distribution between 660 and 700 nm [24]. To make the reflectance of the multispectral data comparable to the spectrometer data, a convolution of the spectrometer reflectance over the wavelength range of each multispectral band is required: ′=(,)∙()(,)∙() (3) where  ′  is the nominal convoluted reflectance corresponding to multispectral band b, and Tb(λ) is the transmittance function over band b within the wavelength range λmi n to λmax.  In addition, the spectrometer footprint approximates a circle on the ground that covers multiple pixels of the multispectral image, as illustrated in Figure 2. To produce spatially comparable data, the mean of the multispectral imagery pixel intensity values (V) within the spectrometer footprint is calculated: 



Remote Sens. 2017, 9, 696    5 of 20 (,)=1(,),,∈(−,−)< (4) where V is the multispectral image pixel value (intensity), (x, y) and (i, j) are image coordinates using the multispectral image center as the origin, dist is the 2D distance based on X and Y directions, and n is the number of pixels within the given footprint area of radius r. In nadir view, assuming UAV rotations are negligible, r is constant for a given flight where the sensors are mounted at fixed positions on the UAV.   Following the above steps, the acquired signals from the multispectral camera and spectrometer are comparable spectrally and spatially. The correlation (determination coefficient, R2) between the two data sets is then calculated to determine the best alignment; i.e., the alignment for which the R2 between the datasets is maximized (Equation (5)).   max(∆,∆):=(∆),(∆,∆) (5) where the f is the linear correlation with its coefficient of determination R2, ρb and V were as previously defined. Note that while ρb is reflectance and V intensity, their relationship is linear, so V does not have to be converted to reflectance to be correlated against ρb. Δt is the misalignment due to time offset of the sensors, and Δθ is the view angle misalignment between the sensors. Both contribute to the offset of the spectrometer footprint in the horizontal and vertical directions (Δx, Δy) from the multispectral image center. This misalignment in both time and spatial domains (Δt, Δθ), expressed as (Δt, Δx, Δy), is optimized simultaneously.   MultispectralCameraSpectrometerLensMultispectral imageXYt∆xSpectrometer footprint∆yImage centerMultispectral image sequenceHyperspectral data sequenceUAV Flight path∆y∆x…t…Spectra at image centersOther spectra… Figure 2. Need for alignment of the multispectral camera and spectrometer data. Δx and Δy are the offsets between the spectrometer data closest to the center of a multispectral image and the pixels at the center of the image. The offsets are caused by timing differences between the two sensors and by misalignment of the sensors’ optical axes. The goal was to determine and correct for Δt, Δx, and Δy before performing fusion of the two data types.  



Remote Sens. 2017, 9, 696    6 of 20 Reasonable search ranges for both the time and space domains are initially set for the optimization. The time domain search range is defined as the time shift from the initial multispectral camera–spectrometer data match according to their nominal time tags; 10 s before and after this initial match was used in this study. As illustrated in Figure 2, the method uses the time of multispectral images as the base, while shifting the timeline of spectrometer data in steps equal to the spectrometer data collection interval (0.2 s). In the space domain, the search range is defined around the multispectral camera image center. A search range at an interval of 5 pixels (about 8 cm) in both the X and Y directions out to ±100 pixels from the multispectral image center was defined. For each interval in the search ranges of both the time and space domains,  R2 was calculated; the maximum R2  within the searching range according to Equation (5) corresponds to the optimized time offset from its initial matched time (Δt) and the optimized spatial offset from the multispectral camera image center (Δx, Δy). 2.3. Fusion of Spectrometer and Multispectral Camera Data Suppose    represents the intensity recorded by the multispectral camera in m spectral bands, and  represents the reflectance recorded by the spectrometer in n bands; observation of a given location with both the multispectral camera and the spectrometer can be expressed as  = [, ], =,,,…,,=,,,…, (6) After a flight, each location on the ground is covered by a multispectral camera pixel, thus  is known for all l ground locations. However, there is only a limited number of k locations represented by the spectrometer measurements. The measurement matrix for a flight Obs, is therefore comprised of pairs of measurement locations  , to ,, as well as multispectral camera measurements   to , for which there are no spectrometer measurements, as shown in Equation (7):  = …… = ,,,…,,?…,? (7) The first k measurements from both sensors can be used as training data to predict the unknown spectral reflectance in the remaining (l  − k) ground (pixel) locations, where only  exists. Methods to estimate hyperspectral reflectance at unknown locations in recent studies include spline interpolation [25,26], Bayesian imputation [8,27], missing data imputation methods [28,29] based on principal component analysis (PCA), and others. Spline interpolation simply exploits the high correlation between spectral bands to interpolate the hyperspectral band reflectance from the limited number of multispectral bands. Bayesian estimation [8,27] assumes that, given the a priori covariance of the hyperspectral bands, a full spectra can be inferred from the multispectral imagery using Bayesian imputation. Bayesian imputation estimates a missing hyperspectral value as the mean value of the predictive based on a posterior conditional distribution (hyperspectral conditional on multispectral). The PCA-based data imputation method selected for this study, compresses the data into a few explanatory latent variables, and imputes unknown hyperspectral reflectance based on the latent structure of the data set [28–30]. Specifically, PCA-based imputation methods first calculate a PCA model from both multispectral and hyperspectral data (unknown hyperspectral data are filled with zeros in this stage); this PCA will then be employed to impute and replace unknown hyperspectral bands with known multispectral data as input. Iteration is then carried out until 



Remote Sens. 2017, 9, 696    7 of 20 the difference between the newly imputed hyperspectral data and the previous ones are smaller than a specified tolerance, thus indicating convergence.   This study implemented these three groups of fusion methods. In the PCA methods, we tested slightly different methods as defined in [29], including trimmed scores regression (TSR), known data regression (KDR), known data regression with principal component regression (KDR–PCR), KDR with partial least squares regression (KDR–PLS), projection to the model plane (PMP), filling in the missing data with the predictions obtained from previous PCA models iterated recursively until convergence (IA), modified nonlinear iterative partial least squares regression algorithm (NIPALS), and data augmentation (DA). In the Bayesian methods, three different approaches to perform stochastic sampling: Gibbs, expectation maximization (EM), and iterative conditional modes (ICM), were employed to estimate hyperspectral data based on multivariate normal distribution and maximum likelihood estimation [27]. Apart from these three categories of hyperspectral prediction approaches, there have been many studies on the data fusion of hyperspectral imagery and multispectral imagery (e.g., [31,32]). Hyperspectral–multispectral (HS–MS) image fusion, however, is a different problem from the hyperspectral prediction studied here. In HS–MS data fusion,   and   are both known for all locations in the image but with different spatial resolution. By contrast, in this study,    is known at all pixels, but   exists only at a set of sample locations within the image (Figures 1 and 2).   3. Study Site, UAV System, and Flight Design 3.1. Study Site In the application and evaluation of the data fusion approach, UAV flights were conducted over an experimental farm on 21 July 2016, near London Ontario, Canada. A hexa-copter (DJI Spreading Wings S800) was used with take-off weight (including sensors) of 6 kg. For the lightweight sensors used in this study (Sections 3.2 and 3.3), an even smaller and lower cost UAV could be used. The major ground cover types were crops, bare soil, and gravel roads, with smaller extents of other short vegetation, trees and buildings. The planned and actual UAV flight paths are shown in Figure 3a,b, where two flights were conducted over a tomato field (Site 1), and a larger field of corn and soybean (Site 2), respectively. The flight speed was 5 m/s, and the flight altitude was 30 m and 50 m above ground for Site 1 for Site 2, respectively. The sky was clear during the flights with negligible sun illumination variation.  (a)                                            (b) Figure 3. The study site with the planned flight path (a), and the actual flight path (b) draped over a Google Earth image, taken on a later date (3 September 2016). (Site 1 at bottom left: tomato field; Site 2 in upper right: corn and soybean fields). 



Remote Sens. 2017, 9, 696    8 of 20 3.2. UAV Spectrometer and Multispectral Camera Systems The UAV spectrometer was an Ocean Optics STS–VIS model, which is 40 mm × 42 mm × 24 mm and weighs 60 g. It has a 1024 bands, each of 0.46 nm full bandwidth, over the spectral range from about 350 nm to 800 nm. The FOV was 1.5° using a fiber optic cable (length: 25 cm, core size: 400 μm) and collimating lens. The integration time is manually set between 100 ms and 1000 ms (1 s), depending on illumination conditions; for this flight under sunny summer conditions, it was set to 200 ms. A full radiance spectrum was recorded every 0.2 s during the flight. The spectrometer was controlled by a Raspberry PI 3 Model B microcomputer (22.9 cm × 17.5 cm × 4.8 cm), which was remotely operated through a 2.4 GHz wireless network when the UAV was on the ground, to input the appropriate spectrometer settings before the flight. A 3000 mAh Lithium ion battery was used to power the spectrometer system, which lasts about 3.0 h. As the Raspberry PI computer could not log real world time in its default settings, an external DS3231 Real Time Clock was connected to it to keep time to a precision of one second. Each spectrum recorded included a local time (millisecond accuracy) relative to the time the PI computer was powered on. The whole spectrometer system weighs about 180 g. The multispectral camera (TetraCAM miniMCA) is comprised of an array of six 1280 × 1024 pixel 10-bit cameras mounted in a case with dimensions of 131.4 mm × 78.3 mm × 87.6 mm, and a weight of 700 g. It was configured with spectral bands at 490 nm, 550 nm, 680 nm, 720 nm, 800 nm and 900 nm. The bandwidth for all spectral bands was 10 nm (full-width at half maximum). The 900 nm band was not used in this study, since the spectrometer sensitivity does not extend beyond 800 nm. The lens focal length was 9.6 mm, providing a FOV of 38.26° and 30.97°, horizontally and vertically, respectively, which corresponded to 20.8 m × 16.6 m image extent when flying at 30 m above ground, and 34.7 m × 27.7 m at 50 m altitude. The nominal ground pixel size was approximately 1.6 cm × 1.6 cm at 30 m altitude and 2.7 cm × 2.7 cm at 50 m altitude. The aperture setting was f/3.2. The spectrometer footprint was 0.78 m diameter at 30 m altitude and 1.3 m at 50 m altitude, corresponding to a diameter of 48 multispectral pixels, and an area of about 1810 pixels. Camera images were acquired every 2 s. Once acquired, all six bands were simultaneously transferred to each camera’s compact flash memory card, and then spatially aligned through its default software (PixelWrench 2), based on a pre-defined alignment file. The camera system tags a nominal time to each image, which is inaccurate unless the camera is linked to an external GPS. There is also a relative millisecond level internal clock record (as for the spectrometer) for each image with time set to zero at camera power on.   On the ground, Datalink (900 Mhz) and GroundStation 4.0 software served as the flight control system, which provided real time monitoring of the UAV status such as flight speed, altitude, and battery. The hexa-copter was operated in automatic pilot mode with pre-defined waypoints. A second Ocean Optics STS–VIS spectrometer with cosine corrector was pointed skyward to record downwelling sun and sky irradiance, as stated earlier. This, and the UAV spectrometer, were configured pre-flight through a wireless network and web page where integration time and other acquisition parameters were manually set.   To calibrate the data radiometrically and geometrically, ground reference objects and targets were used. The white reference disc (ASD Spectralon, 9.2 cm diameter, U.S. Pharmacopeial Convention (USP) for USP 1119) was used to calibrate the spectrometers before and after each flight, and to calculate reflectance (Equation (1)). Around 10 other targets, consisting of white background with black strips across the diagonals, were distributed evenly throughout the study site. Their locations were measured using a GPS to assist in geo-referencing, and mosaicking of the multispectral images.   4. Results: Data Alignment and Fusion to Produce Estimated Hyperspectral Imagery   A sequence of 128 multispectral images (multispectral image ID: 2573–2700) obtained in Site 1, and a similar sequence of 228 images (ID: 2873–3100) obtained at Site 2 together with their corresponding hyperspectral data as “samples”, were employed to analyze the performance of the data alignment and 



Remote Sens. 2017, 9, 696    9 of 20 fusion. The multispectral camera and spectrometer data were processed using the method described in Section 2. According to Equation (5), correlations were calculated for samples from the multispectral camera and spectrometer data. The alignment was conducted in both time and space domains (Δt, Δx, Δy), which produced a four-dimensional R2 matrix. R2 was calculated separately for each spectral band, and the maximum average R2 within the search range was taken as the optimized time alignment. Both the multispectral camera and spectrometer write a millisecond level timestamp into their acquired data files, and at the same time, acquire data files with a real world time tag (in seconds), provided by the operating system. The time of the two sensors was initially matched using their real world time tags, with an accuracy of about 1 s. Then, the more accurate local millisecond timestamps were used in the optimization. In the spatial domain, the mean value of the multispectral image bands according to Equation (4), was calculated for each offset within the search range. Specifically, the image–spectrometer data pairs were used to calculate R2 between the two data types, by iteratively shifting the spectrometer timeline by 0.2 s within a range ±10 s and an interval of 5 pixels, in both the X and Y directions, out to ±100 pixels of the multispectral image center, as illustrated in Figure 2. Multiple groups of methods as described in Section 2.3 were employed to fuse the hyperspectral data and multispectral imagery. 4.1. The Data Alignment Procedure 4.1.1. Profile of Time Domain Alignment The profiles of R2 of multispectral–spectrometer data pairs at the ideal alignment (Δt = −0.2, Δx = 45, Δy = 5) at Site 1, and (Δt = −1.2, Δx = 85, Δy = −20) at Site 2, along the time domain, are given in Figure 4.   (a)  (b) Figure 4. Linear correlation between sample pairs of multispectral camera intensity and spectrometer reflectance at Site 1 (a) and Site 2 (b). Each subfigure includes five curves representing the five multispectral bands, plus a thicker line representing the average R2. Each curve is the correlation for the given band at 0.2 s interval time offsets over the ±10 s range. The maximum average R2 and its corresponding time offset are given in the upper left of each graph. According to Figure 4, the best correlation for both sites was obtained for small time shifts of 1.2 s or less from their initial timeline match based on the nominal time tags; the time shift was not constant between the two sites because the sensors were reset or restarted before each flight. Furthermore, in cases where sensors have inaccurate clocks (more common for low-cost sensors), or they stay offline for a long time, their clocks may diverge from real world time, thereby rendering time alignment critical.   Either the individual bands, or their average at both sites, provides a clear unimodal R2 distribution over the time offset range. Site 2 has a lower R2 compared with Site 1, but still exhibits an obvious peak Shift from intial matched timeline-10-8-6-4-2024681000.10.20.30.40.50.60.70.80.91Avg R-square: 0.946Time offset: -0.2490nm550nm680nm720nm800nmaverageShift from intial matched timeline-10-8-6-4-2024681000.10.20.30.40.50.60.70.8Avg R-square: 0.713Time offset: -1.2490nm550nm680nm720nm800nmaverage



Remote Sens. 2017, 9, 696    10 of 20 value. An explanation for the lower Site 2 correlations is that it had a larger population and more cover types (soybean, corn, soil, etc.), compared with Site 1. An analysis using a subset of samples in Site 1 (start, end, and the entirety of the flight) also reveals a similar trend, where lower sample numbers produced higher R2. The correlation scatterplot for each spectral band with samples from the entire Site 1 flight at −0.2 s time offset, is provided in Figure 5. The distribution of samples in each band is quite similar, with R2 values ranging from 0.90 to 0.99.    (a)  (b)  (c)  (d)  (e)  Figure 5. Per band multispectral camera-spectrometer regression for the optimal time alignment at Site 1 for spectral bands at 490 nm, 550 nm, 680 nm, 720 nm, and 800 nm (a–e), respectively. “DN” is digital number, the intensity units of the multispectral camera signal. 4.1.2. Profile of Spatial Domain Alignment The profile of R2 distribution along the spatial domain at the ideal time offset (−0.2 s) in Site 1, is shown in Figure 6a, and the profiles over the X and Y directions are given in Figure 6b. A peak R2 value that was closest to the multispectral image center was selected; the resulting offset was Δx = +45, Δy = +5 pixels with R2 = 0.946 as shown in Figure 6a.   For Site 1, since the spectrometer footprint in the multispectral images for Site 1 was a circle with diameter of 48 pixels, a Y offset of 5 pixels is minor, especially considering the search interval was 5 pixels. This almost negligible offset probably resulted from fixing the spectrometer against the camera housing; rotational misalignment in the Y direction more than this small amount was not possible. In the X (flight) direction, however, the spectrometer was mounted by visually aligning the fiber optic cable and head with the vertically oriented edges of the camera body. The resulting 45 pixel offset in the X direction is not unexpected, and almost the same length as the footprint diameter. This spatial offset corresponds to 1.4° angular difference in the X direction between the multispectral camera and the spectrometer. Furthermore, Figure 6b shows an obvious R2 peak in the Y direction, but a plateau in R2 in the X direction. A possible explanation is that the flight lines (X direction) were parallel to rows of tomato 1040246800.20.40.60.8R2: 0.9651040246800.20.40.6R2: 0.969104012340.10.20.30.40.50.6R2: 0.91010412345600.20.40.60.81R2: 0.897



Remote Sens. 2017, 9, 696    11 of 20 plants in the site, causing R2  to be unchanged for certain offsets as both sensors detected radiance from the same row(s), but with a slight offset.    (a)   (b) Figure 6. Site 1 data alignment. (a) The distribution of R2 between the multispectral camera image brightness and spectrometer reflectance for spatial domain offsets (Δx, Δy) up to ±100 pixels at time offset  = −0.2 s. (b) the separate X (left) and Y (right) profiles through the optimal location. After data alignment in Site 1, an optimized parameter set, (Δt = −0.2, Δθ = 1.4°) or (Δt = −0.2, Δx = 45,  Δy = 5) was obtained, with R2 of 0.946 between the multispectral camera image brightness and convoluted spectrometer reflectance. A similar trend of smaller Y offset was also found for the Site 2 data (Δx = 85, Δy = −20). Results from both sites show the need for the time and spatial domain alignment procedure. The resulting well-aligned data from the two sensors were then combined in the fusion procedure, as described in the next section. 4.1.3. Global Optimization vs. Two-Step Optimization   The global optimization of both time and space domain simultaneously involves intensive computation. To evaluate time and space offsets separately, the multi-hyperspectral data alignment was also optimized in a two-step process with the time domain processed first followed by the spatial domain. The alternative two-step optimization procedure starts by presuming that the spectrometer and camera are perfectly aligned first as shown in Equation (8).   max(∆,∆ = 0)→max(∆, ∆ = 0,∆ = 0 ) (8) After the alignment in the time domain where an optimized time offset ̅  is retrieved, the second step is alignment in the spatial domain, where (∆, ∆) is optimized, as described in Equation (9), to obtain the best spatial shift (̅,). Together with the optimized time offset retrieved in the first step, the alignment in both time and spatial domains is achieved via a combination of (̅,̅,). max(∆ = ̅,∆)→max(∆ = ̅,∆,∆) (9) This alternative two-step optimization was implemented and compared using the 128 spectrometer/image data samples of Site 1. The two-step optimization approach first reached  ̅ = −0.2 s in the time domain and then (̅,) = (45, 5), which is the same alignment solution as the global optimization. The computation time using both approaches is reported as in Table 1.   The results were generated using a laptop with Intel CoreTM i7-5600U CPU @2.60 GHz, 12 GB RAM, and SSD drive, running a Windows 7 Professional 64-bit operation system. As in Table 1, the pre-processing stage used a large amount of time, because the convoluted multispectral band average values according to Equation (Error! Reference source not found.) were calculated at each possible (Δx, Δy) in Y offset (in pixels) when x offset = 45-100 -50 0 50 100Average R20.40.50.60.70.80.91X offset (in pixels) when y offset = 5Y=5, R2=0.946 X=45, R2=0.946



Remote Sens. 2017, 9, 696    12 of 20 the spatial search range. The global optimization consumed 16.9 min while the two step-approach used 16.5 s; i.e., the two-step approach was about 60 times faster than the global optimization. The two-step optimization reported R2 = 0.81 for the first step (time-domain optimization) and then R2 = 0.95 for the second step, which equals that for the global optimization. Finally, a hyperspectral image (556 by 506 pixels after resampling) was generated using the data fusion approach in 8.5 min.   Table 1. Computation time consumption in optimization strategies. Optimization Methods Pre-Processing Time Domain Space Domain Data Fusion * Global  24.3 min  16.9 min 8.5 min Two-step    1.9 s  14.6 s * Data fusion to generate hyperspectral (HS) image used the PCA_TSR method reported later in Table 2. 4.2. The Performance of Multispectral-Spectrometer Data Fusion 4.2.1. Accuracy of Data Fusion To predict full spectra at locations with only multispectral data, the PCA modelling method was implemented according to Equations (6) and (7), where m = 5 multispectral camera bands and n = 840 spectrometer bands within the wavelength range of 400 nm to 800 nm. The l = 128 samples of spectrometer and multispectral camera data taken from the optimal image location as determined in the previous section, were used as reference data in Equation (7). For validation purposes, 21 random samples of the spectrometer reflectance measurements were set aside, to be compared with predicted spectra. Thus k = 107 training data samples were used in the PCA imputation. Following experimentation with similar PCA methods [27–29], trimmed scores regression, based on the PCA method, was adopted in this study, because of its balance between prediction quality and robustness with data structure complexity and computation time [29]. The first three principal components were used in this procedure. According to suggested default values given in [28], the maximum number of iterations was set at 10, and the convergence tolerance was set at 0.0001. The 21 validation (observed) spectra, the corresponding predicted spectra, and the difference between them are shown in Figure 7a–c, respectively.   In Figure 7, the 21 validation spectra include vegetation (crops), soil, and some combined soil–vegetation data. The predicted curves in b have similar shapes and reflectance ranges, when compared with the observation curves in a. The absolute values of the “predicted–observed” residuals shown in c are all less than 0.1. The mean error, as a percentage of the residuals to the corresponding observations for the 21 validation samples is 9.48%, while the mean absolute error is 19.9%. According to Figure 7c, peak residuals usually occur around 680–700 nm, where reflectance of vegetation and soil has large contrast.   (a)   (b) Wavelength (nm)400 450 500 550 600 650 700 750 80000.050.10.150.20.250.30.350.40.45Wavelength (nm)400 450 500 550 600 650 700 750 80000.050.10.150.20.250.30.350.40.45



Remote Sens. 2017, 9, 696    13 of 20  (c)  Figure 7. The observed (a) and predicted (b) reflectance spectra and the corresponding residuals (c). Each spectrum is given a random color. 4.2.2. Stability of Data Fusion Parameters In addition, to test the impact of data fusion method parameters on accuracy, using Site 1 data, the data fusion step was run many times with each parameter changed as a sequence, and the corresponding RMSE of estimated hyperspectral spectra was computed. Specifically, the two PCA-based data fusion parameters were tested: iteration times (5–120, default 10), convergence tolerance (10−5–10−3, default 10−4), and the training sample number was also changed from 10 , to 12 0. Figu re 8 sho ws the i mpact  of v ariati ons in converge tolerance on RMSE to be minor, while for RMSE was stable for numbers of iterations above 20. The number of training samples significantly affected RMSE; RMSE dropped after about 50 training samples among the 128 samples tested with the Site 1 data.  Figure 8. The impact of data fusion parameters on hyperspectral data fusion.   4.2.3. Comparison of Data Fusion Methods To quantitatively compare the different data fusion methods described in Section 2.3, general accuracy evaluation measures, together with ones specifically designed for hyperspectral imagery, were used. In the results presented below, “ME” is the percent Mean Error, (predicted spectra–observed spectra)/observed spectra; “MAE” is the percent Mean Absolute Error; “RMSE” is the Root Mean Square of Error; and “STD_AE” is the Standard Deviation of the percent Absolute Error. For measures used in hyperspectral image quality evaluation [31,32], “SNR” is the signal to noise ratio; “UIQI” is the universal Wavelength (nm)400 450 500 550 600 650 700 750 800-0.1-0.08-0.06-0.04-0.0200.020.040.060.080.1 123456789101112131415161718192021Number of training samples0 20 40 60 80 100 1200.020.040.060.080.1Training sampleToleranceIteration times0 2 4 6 8 10 12 14 16 18 200.020.040.060.080.1Iteration timesTolerance (x10-5)



Remote Sens. 2017, 9, 696    14 of 20 image quality index; “SAM” is spectral angle mapper; “ERGAS” is the relative dimensionless global error in synthesis, which calculates the amount of spectral distortion in the image; and “DD” is the degree of distortion. With the exception of SNR and UIQI, smaller values are better for all measures. According to Table 2, PCA-based and Bayesian-based data fusion approaches performed similarly, especially the PCA group of methods. The Bayesian imputation with Gibbs sampling reported with the best overall performance, and used a slightly longer time than most other approaches. The spline approach was the simplest, and produced the worst predicted spectra accuracy. An example of a single crop location, estimated using the three fusion methods, Figure 9, illustrates the difference between these approaches. When comparing the data fusion result of the two study sites reported in Table 3, the PCA-based and Bayesian methods performed most consistently. For example, both of them produced MAE of around 17%, which means the absolute difference from predicted hyperspectral curves to the observed curves are around 17%. The ME is ±3% relative to the observed average in both sites in Table 3.  Figure 9. An example of the three types of data fusion compared with an observed spectrum over a crop sample at Site 1. The “PCA” is the “PCA_TSR” approach, “trimmed scores regression” defined in Section 3.2; the “Bayesian” is the “Bayesian_Gibbs” (Bayesian method with Gibbs sampling [27]) as defined in Section 3.2. “MS bands” are the spline interpolated reflectances derived directly from the multispectral (MS) camera data.    Wavelength(nm)400 500 600 700 80000.10.20.30.4ObservedPCABayesianSplineMS-band



Remote Sens. 2017, 9, 696    15 of 20   Table 2. Detailed comparison of fusion approaches implemented on the Site 1 data (tomato field). Category  Method  Time (s)  ME  MAE  RMSE (10−3) STD_AE SNR UIQI SAM ERGAS DD (10−3)  TSR 0.46 3.63% 16.83% 28.947 0.137  14.97 0.960 12.37  22.10  20.751  KDR 0.50 5.09% 17.54% 28.954 0.141 14.97 0.961 12.47 22.22  20.896 PCA  PCR 0.49 3.63% 16.83% 28.947 0.137  14.97 0.960 12.37  22.10  20.751 KDR–PLS 0.67  3.57% 16.78%  28.953  0.137  14.97 0.960 12.37  22.11  20.746  PMP 0.50 3.63% 16.83% 28.947 0.137  14.97 0.960 12.37  22.10  20.751  IA 0.19 3.63% 16.83% 28.947 0.137  14.97 0.960 12.37  22.10  20.751  NIPALS 0.68 4.46% 16.69%  29.357 0.139 14.85 0.959 12.42  22.51  21.039  DA 124.20 3.19% 17.40% 28.906 0.146 14.99 0.961 12.43 22.23  20.785 Bayesian  Gibbs 38.67 2.80% 17.41%  28.620 0.142  15.07 0.962 12.30 22.02 20.649EM 1.03 3.25% 17.35% 28.895 0.145 14.99 0.961 12.42 22.23 20.773 ICM 0.20 3.70% 17.28% 28.910 0.144 14.99 0.961 12.43 22.23  20.769 Spline *  0.004 −4.28% 116.63% 39.437 28.990 13.29 0.923 14.18 26.71 30.678* Metrics for the spline approach were calculated in the wavelength range that is valid for the multispectral image (490–800 nm) rather than the full 400–800 nm range. The accuracy metrics are defined in the text: “ME” is the percent Mean Error, (predicted spectra–observed spectra)/observed spectra; “MAE” is the percent Mean Absolute Error; “RMSE” is the Root Mean Square of Error; and “STD_AE” is the Standard Deviation of the percent Absolute Error, “SNR” is the signal to noise ratio; “UIQI” is the universal image quality index; “SAM” is spectral angle mapper; “ERGAS” is the relative dimensionless global error in synthesis, which calculates the amount of spectral distortion in the image; and “DD” is the degree of distortion [31,32]. Bold text indicates best performance in a column.   Table 3. Evaluation of hyperspectral data and multispectral image fusion result over the two agriculture sites. Method #Image #Training #Test Time (Min) AvgR2 Δt(s) Δx(Pixel) Δy(Pixel)Site 1 (tomato)  128 107 21 40.23 0.946  −0.2 45 5Site 2 (corn/soybean)  228  185  43  41.25  0.715    −1.2 85  −20   Time (s) ME MAE RMSE (10−3) STD_AE SNR UIQI SAM ERGAS DD (10−3)Site 1 PCA_TSR 0.46  3.63% 16.83%  28.947  0.137 14.97 0.9597  12.37  22.10  20.751 Bys_Gibbs 38.67  2.80% 17.41%  28.620  0.142 15.07 0.9617  12.30  22.02  20.649 Spline 0.004 −4.28% 116.63%  39.437  28.990 13.29 0.9230  14.18  26.71  30.678 Site 2 PCA_TSR 0.54 −1.37% 16.99% 27.244 0.284 15.34 0.9651 13.65 21.01 15.903Bys_Gibbs 91.07  −3.10% 16.80%  27.488  0.309 15.26 0.9652  13.83  20.99  15.586 Spline   0.05  −4.32% 111.52%  38.474  43.272 13.37 0.9326  15.14  26.37  26.987 Note: “Method” are data fusion methods listed in Table 2. “#Image/Training/Test” are the number of samples for all, training, and test purposes, respectively. “AvgR2” is the average R2 among all the multispectral bands. The optimal alignment, (Δt, Δx, Δy) given as seconds in the time domain and pixels in the space domain. “Time” is the computation time in seconds (s) or minutes (min). “PCA_TSR” approach is “trimmed scores regression” in the PCA group defined in Section 3.2; “Bys_Gibbs” is the Bayesian method with Gibbs sampling [27] as defined in Section 3.2.



Remote Sens. 2017, 9, x FOR PEER REVIEW    16 of 20  4.3. Fused Hyperspectral Imagery  Once this approach to predict the hyperspectral reflectance was developed and evaluated, a subset of the multispectral camera images was mosaicked, and then used as input for hyperspectral image prediction. Agisoft Photoscan was employed to mosaic the multispectral images. The UAV on-board GPS/IMU provided the initial positions of each image center to assist the mosaicking process. Images for each spectral band, which had been calibrated using a Teflon surface, were then combined into a single TIFF file for input to the mosaicking procedure. The red band (680 nm) was set as the master band during mosaicking, as the multispectral camera sensitivity was highest in this region. The images were geocoded using the WGS84 + EGM96 Geoid coordinate system. The mosaic production process was automated; tie points were selected by the software. The default mosaicking approach of the software divides data into several frequency domains, which were then blended independently, with the blending effect decreasing with increasing distance from the seamline. The average control point position error was 1.99 m after bundle adjustment. The resulting false color mosaic, the generated hyperspectral image as an image cube, and a vegetation health index map are given in Figure 10, covering the area shown by the white polygon in Figure 3.   (a) (b)  (c) Figure 10. Data fusion and hyperspectral imagery generation. (a) A preview of the multispectral subarea mosaic in false color (RGB: 800 nm, 680 nm, 550 nm). (b) The estimated hyperspectral (HS) image cube. (c) An example application of the estimated hyperspectral imagery: mapping a narrowband vegetation index, the Photo-chemical Reflectance Index (PRI). 



Remote Sens. 2017, 9, x FOR PEER REVIEW    17 of 20  A hyperspectral image was then estimated from the multispectral mosaic using the fusion imputation methods described in Equations (6) and (7). To demonstrate the utility and advantage of such a hyperspectral image, a narrow band index, the Photo-chemical Reflectance Index (PRI) [33] was calculated as: (R570 − R531)/(R570 + R531), where R is reflectance extracted from the estimated hyperspectral imagery. The resulting PRI map is shown in Figure 9c. Red indicates low photosynthetic light use, while green indicates dense vegetation and high light use efficiency. Such an index cannot be calculated from multispectral camera data unless specific very narrowband interference filters are mounted on individual cameras. Such an approach would require changing filters if other indices incorporating different spectral bands were to be derived. In contrast, the estimated hyperspectral images allow derivation of a variety of such index maps from a single multispectral camera and spectrometer data set. 5. Discussion The spectrometer footprint was assumed to be consistent in size and shape among samples, which is not always true. The UAV platform orientation fluctuated by wind and its own rotation. When the platform was tilted, the spectrometer footprint was an ellipsoid rather than a circle, as in the nadir case. Furthermore, the integration time brought in a considerable footprint shift at full UAV speed. Taking this study as an example, the 0.2 s integration time was equal to 1 m shift at full speed of 5 m/s. Hence, the shape of the footprint under the full speed is not a stationary circle, but a moving circle dragged through 1 m distance. Furthermore, the UAV needs to frequently accelerate and decelerate from one waypoint to another, accumulating the effect of tilt, velocity, and acceleration on the irregular shape of footprints. Despite including these sources of uncertainty, the proposed data alignment method handled the uncertainty quite well, mainly because the platform inclination consistently affects both hyperspectral and multispectral sensors, and the spatial auto-correlation with neighboring areas was carried through the observed data, and mitigated the effect of dragged footprint shapes.   The sources of uncertainty in the data fusion stage were numerous. A major source of uncertainty in the estimated spectra was the number of training samples, where many samples of convolved reflectance corresponded to a very similar multispectral band intensity combination, or vice versa. Figure 5 demonstrated such a trend in a linear regression model. Such an ambiguity of multispectral-hyperspectral sample pairing undermined the effectiveness of multi-hyperspectral data fusion method. Spectral mixing in the spectrometer footprint also complicates the imputation. Spectra prediction was poorer for pixels absent from the training set.   The broader impact of this study is significant. Drone and sensor technology are developing rapidly, with many manufacturers, standards, and products. There are compatibility issues among various sensors and platforms, due to differences in external computers, ports, triggering methods, and data storage methods. It is therefore challenging to operate sensors on a lightweight UAV using a single central control system, and impractical for general users (e.g., farmers) without advanced hardware and software skills. Accordingly, it is valuable that data from various sensors can be aligned post-flight by exploiting the correlations between two or more data types, as proposed in this study. Such a post-flight alignment approach allows farmers to use their existing equipment together with new sensors and platform, without advanced skills.   The proposed system has the advantage of very low cost and simple sensor positional/orientation control and measurement requirements compared to current hyperspectral cameras. It is a software solution that is easy to implement compared to development of a full UAV-based hyperspectral system. Consequently, it is well suited to precision farming applications on individual or small farms by farmers or other users with little remote sensing system expertise or funding. Results of this study can benefit precision farming companies, such as A&L Canada (http://www.alcanada.com/), which collaborated on this research, to offer services to the agriculture industry and farm owners. The system developed and evaluated in this study is expected to enhance capabilities to generate the high resolution information on crop condition and other environmental parameters needed in precision farming, water, and land use management.   



Remote Sens. 2017, 9, x FOR PEER REVIEW    18 of 20  6. Conclusions This study demonstrated that a multispectral camera coupled with a spectrometer on a UAV platform can provide a means to estimate high spatial resolution hyperspectral imagery at very low cost for precision farming applications. The study involved two processes: alignment of data acquired by the two sensors, and fusion of the aligned data from the two sensors to estimate hyperspectral reflectance at all image pixel locations. In the sensor alignment process, as an alternative to more expensive and complex hyperspectral cameras, and as an alternative to complex positional and orientation measurement and processing, the multispectral camera and spectrometer were simply visually aligned in the UAV mount. A software approach was then used to align the datasets by determining the location in the image that corresponded to the spectrometer footprint collected closest to the image center. This data alignment procedure included analysis of correlations between the two data types for samples representing different acquisition timing between the two sensors and data offsets, due to sensor misalignment. The best location match for the spectrometer data was taken as the image pixels that maximized the correlation between the two data types. Empirical testing was conducted using UAV data acquired at 30 m altitude over a tomato farm and 50 m over a soybean/corn field with a mix of soil, vegetation and other land cover types. In a global optimization, a maximum R2 value of 0.95 was found at a −0.2 s time offset and an offset of 5 and 45 pixels in the X and Y directions, respectively, for one test site and −1.2 s, 85 and −20 pixels (X, Y) for the other site. A simplified two-step alignment approached was also tested which reaches the same alignment solution, but 60 times faster. To fuse the multispectral camera and spectrometer data and produce a hyperspectral image, reflectance measured by the spectrometer was convolved to match the wider multispectral camera bands, and the pixel intensity values in the multispectral images were spatially averaged to match with spectrometer footprint. Three groups of spectra estimation methods were implemented; PCA-based and Bayesian-based estimation reported similar levels of residuals with mean absolute differences around 17% relative to the mean of observed spectra, and RMSE around 0.028 for the predicted spectra. A simple spline interpolation using the multispectral camera data proved to be ineffective in hyperspectral estimation. The tolerance and number of iterations had limited impact on the PCA-based data fusion method, which suggests the fusion method is stable. However, training sample numbers did have an effect; half of the 128 samples were required in training to maintain a stable low RMSE. The results reported for the two different flights over different crop types show consistent accuracy of predicted spectra. Using these results, a mosaic of multispectral images for a subarea was produced and used to predict the corresponding hyperspectral reflectance data at each pixel. The hyperspectral data were then used, as a demonstration, to map of narrowband PRI index, which is not possible using multispectral data unless the required narrowband filters (e.g., ±2 nm) are installed on individual cameras. Acknowledgments: This project was funded by an NSERC Engage grant and NSERC Discovery funding. The authors are very grateful for the contributions of A&L Canada Laboratories in all phases of this research. We are grateful to the anonymous reviewers who contributed to the improvement of this manuscript. Author Contributions: Chuiqing Zeng was responsible for the research design, field work plans, UAV flight campaign, data processing, experiments and manuscript drafting. Douglas J. King and Murray Richardson provided guidance and suggestions in the research design and provided important comments to shape this study in further stages. Douglas J. King also largely rewrote and polished the manuscript. Bo Shan carried out most field measurements and organized the equipment and personnel.   Conflicts of Interest: The authors declare no conflict of interest. References 1. Zhang, M.; Qin, Z.; Liu, X.; Ustin, S.L. Detection of stress in tomatoes induced by late blight disease in California, USA, using hyperspectral remote sensing. Int. J. Appl. Earth Obs. Geoinf. 2003, 4, 295–310. 



Remote Sens. 2017, 9, x FOR PEER REVIEW    19 of 20  2. Goel, P.K.; Prasher, S.O.; Landry, J.A.; Patel, R.M.; Viau, A.A.; Miller, J.R. Estimation of crop biophysical parameters through airborne and field hyperspectral remote sensing. Trans. Am. Soc. Agric. Eng. 2003, 46, 1235–1246. 3. Lee, W.S.; Alchanatis, V.; Yang, C.; Hirafuji, M.; Moshou, D.; Li, C. Sensing technologies for precision specialty crop production. Comput. Electron. Agric. 2010, 74, 2–33. 4. Houborg, R.; Fisher, J.B.; Skidmore, A.K. Advances in remote sensing of vegetation function and traits. Int. J. Appl. Earth Obs. Geoinf. 2015, 43, 1–6. 5. Navarro-Cerrillo, R.M.; Trujillo, J.; de la Orden, M.S.; Hernández-Clemente, R. Hyperspectral and multispectral satellite sensors for mapping chlorophyll content in a mediterranean Pinus sylvestris L. Plantation. Int. J. Appl. Earth Obs. Geoinf. 2014, 26, 88–96. 6. Calderón, R.; Navas-Cortés, J.A.; Lucena, C.; Zarco-Tejada, P.J. High-resolution airborne hyperspectral and thermal imagery for early detection of verticillium wilt of olive using fluorescence, temperature and narrow-band spectral indices. Remote Sens. Environ. 2013, 139, 231–245. 7. Zarco-Tejada, P.J.; Catalina, A.; González, M.R.; Martín, P. Relationships between net photosynthesis and steady-state chlorophyll fluorescence retrieved from airborne hyperspectral imagery. Remote Sens. Environ. 2013, 136, 247–258. 8. Gevaert, C.M.; Suomalainen, J.; Tang, J.; Kooistra, L. Generation of spectral—Temporal response surfaces by combining multispectral satellite and hyperspectral uav imagery for precision agriculture applications. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2015, 8, 3140–3146. 9. Roelofsen, H.D.; van Bodegom, P.M.; Kooistra, L.; van Amerongen, J.J.; Witte, J.-P.M. An evaluation of remote sensing derived soil pH and average spring groundwater table for ecological assessments. Int. J. Appl. Earth Obs. Geoinf. 2015, 43, 149–159. 10. Zarco-Tejada, P.J.; González-Dugo, M.V.; Fereres, E. Seasonal stability of chlorophyll fluorescence quantified from airborne hyperspectral imagery as an indicator of net photosynthesis in the context of precision agriculture. Remote Sens. Environ. 2016, 179, 89–103. 11. Candiago, S.; Remondino, F.; De Giglio, M.; Dubbini, M.; Gattelli, M. Evaluating multispectral images and vegetation indices for precision farming applications from uav images. Remote Sens. 2015, 7, 4026–4047. 12. Pajares, G. Overview and current status of remote sensing applications based on unmanned aerial vehicles (uavs). Photogramm. Eng. Remote Sens. 2015, 81, 281–329. 13. Jaakkola, A.; Hyyppä, J.; Kukko, A.; Yu, X.; Kaartinen, H.; Lehtomäki, M.; Lin, Y. A low-cost multi-sensoral mobile mapping system and its feasibility for tree measurements. ISPRS J. Photogramm. Remote Sens. 2010, 65, 514–522. 14. Suomalainen, J.; Anders, N.; Iqbal, S.; Roerink, G.; Franke, J.; Wenting, P.; Hünniger, D.; Bartholomeus, H.; Becker, R.; Kooistra, L. A lightweight hyperspectral mapping system and photogrammetric processing chain for unmanned aerial vehicles. Remote Sens. 2014, 6, 11013–11030. 15. Uto, K.; Seki, H.; Saito, G.; Kosugi, Y.; Komatsu, T. Development of a low-cost, lightweight hyperspectral imaging system based on a polygon mirror and compact spectrometers. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2016, 9, 861–875. 16. Burkart, A.; Cogliati, S.; Schickling, A.; Rascher, U. A novel uav-based ultra-light weight spectrometer for field spectroscopy. IEEE Sens. J. 2014, 14, 62–67. 17. Aasen, H.; Burkart, A.; Bolten, A.; Bareth, G. Generating 3d hyperspectral information with lightweight uav snapshot cameras for vegetation monitoring: From camera calibration to quality assurance. ISPRS J. Photogramm. Remote Sens. 2015, 108, 245–259. 18. Liu, Y.; Wang, T.; Ma, L.; Wang, N. Spectral calibration of hyperspectral data observed from a hyperspectrometer loaded on an unmanned aerial vehicle platform. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2014, 7, 2630–2638. 19. Hruska, R.; Mitchell, J.; Anderson, M.; Glenn, N.F. Radiometric and geometric analysis of hyperspectral imagery acquired from an unmanned aerial vehicle. Remote Sens. 2012, 4, 2736–2752. 20. Zeng, C.; Richardson, M.; King, D.J. The impacts of environmental variables on water reflectance measured using a lightweight unmanned aerial vehicle (UAV)-based spectrometer system. ISPRS J. Photogramm. Remote Sens. 2017, 130, 217–230. 21. Rau, J.Y.; Jhan, J.P.; Huang, C.Y. Ortho-Rectification of Narrow Band Multi-Spectral Imagery Assisted by Dslr Rgb Imagery Acquired by a Fixed-Wing Uas. Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci. 2015, 40, 67–74. 



Remote Sens. 2017, 9, x FOR PEER REVIEW    20 of 20  22. Yahyanejad, S.; Rinner, B. A fast and mobile system for registration of low-altitude visual and thermal aerial images using multiple small-scale uavs. ISPRS J. Photogramm. Remote Sens. 2015, 104, 189–202. 23. Mobley, C.D. Light and Water: Radiative Transfer in Natural Waters; Academic Press: San Diego, CA, USA, 1994. 24. Andover, C. Standard Bandpass Optical Filter. Available online: https://www.andovercorp.com/products/bandpass-filters/standard/600-699nm/ (accessed on 1 June  2016). 25. Mello, M.P.; Vieira, C.A.O.; Rudorff, B.F.T.; Aplin, P.; Santos, R.D.C.; Aguiar, D.A. Stars: A new method for multitemporal remote sensing. IEEE Trans. Geosci. Remote Sens. 2013, 51, 1897–1913. 26. Villa, G.; Moreno, J.; Calera, A.; Amorós-López, J.; Camps-Valls, G.; Domenech, E.; Garrido, J.; González-Matesanz, J.; Gómez-Chova, L.; Martínez, J.A.; et al. Spectro-temporal reflectance surfaces: A new conceptual framework for the integration of remote-sensing data from multiple different sensors. Int. J. Remote Sens. 2013, 34, 3699–3715. 27. Murphy, K. Machine Learning: A Probabilistic Perspective; The MIT Press: Cambridge, MA, USA, 2012; p. 1096. 28. Folch-Fortuny, A.; Arteaga, F.; Ferrer, A. Missing data imputation toolbox for matlab. Chemom. Intell. Lab. Syst. 2016, 154, 93–100. 29. Folch-Fortuny, A.; Arteaga, F.; Ferrer, A. Pca model building with missing data: New proposals and a comparative study. Chemom. Intell. Lab. Syst. 2015, 146, 77–88. 30. Grung, B.; Manne, R. Missing values in principal component analysis. Chemom. Intell. Lab. Syst. 1998, 42, 125–139. 31. Wei, Q.; Bioucas-Dias, J.; Dobigeon, N.; Tourneret, J.Y. Hyperspectral and multispectral image fusion based on a sparse representation. IEEE Trans. Geosci. Remote Sens. 2015, 53, 3658–3668. 32. Yokoya, N.; Grohnfeldt, C.; Chanussot, J. Hyperspectral and multispectral data fusion: A comparative review. IEEE Geosci. Remote Sens. Mag. 2017, in press. 33. Gamon, J.A.; Peñuelas, J.; Field, C.B. A narrow-waveband spectral index that tracks diurnal changes in photosynthetic efficiency. Remote Sens. Environ. 1992, 41, 35–44. © 2017 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).    


Citations (9)References (32)... Burkart et al. [6] demonstrated the use of a small spot-measuring spectroradiometer mounted on a unmanned aircraft system (UAS) offering new potential across numerous applications including upscaling, calibration and validation of airborne and satellite sensors (e.g., airborne sensor HyPlant and/or the FLORIS sensor on-board of a future ESA FLEX mission [7][8][9]), and near-range retrieval of photosynthetic chlorophyll fluorescence emissions [10][11][12][13]. To fully exploit this technology on a UAS platform [6,[14][15][16], accurate geolocation of the spectroradiometer measurements is required. This study aims to propagate the input uncertainties originated from position and orientation sensors to analyse the footprint geolocation accuracy of spectroradiometer observations as a function of the sensor field of view (FOV), above ground level (AGL) height, grades of the inertial measurement unit (IMU), spectroradiometer integration time, and UAS flight speed.  ...... Computation of some robust broad-band vegetation indices, such as for instance NDVI, can tolerate more spectral contamination, whereas a weak vegetation signal such as chlorophyll fluorescence demands low contamination by non-vegetated surfaces as its retrieval requires narrow spectral bands and oxygen absorption features. Despite several attempts to use a spectroradiometer on a UAS to study vegetation spectral properties ( [6,[14][15][16]45]), the level of acceptable spectral contamination resulting from footprint uncertainty to size ratio is still not very well understood.  ...Error Budget for Geolocation of Spectroradiometer Point Observations from an Unmanned Aircraft SystemArticleFull-text availableOct 2018SENSORS-BASEL Deepak GautamChristopher Watson Arko Lucieer Zbyněk MalenovskýWe investigate footprint geolocation uncertainties of a spectroradiometer mounted on an unmanned aircraft system (UAS). Two microelectromechanical systems-based inertial measurement units (IMUs) and global navigation satellite system (GNSS) receivers were used to determine the footprint location and extent of the spectroradiometer. Errors originating from the on-board GNSS/IMU sensors were propagated through an aerial data georeferencing model, taking into account a range of values for the spectroradiometer field of view (FOV), integration time, UAS flight speed, above ground level (AGL) flying height, and IMU grade. The spectroradiometer under nominal operating conditions (8 ∘ FOV, 10 m AGL height, 0.6 s integration time, and 3 m/s flying speed) resulted in footprint extent of 140 cm across-track and 320 cm along-track, and a geolocation uncertainty of 11 cm. Flying height and orientation measurement accuracy had the largest influence on the geolocation uncertainty, whereas the FOV, integration time, and flying speed had the biggest impact on the size of the footprint. Furthermore, with an increase in flying height, the rate of increase in geolocation uncertainty was found highest for a low-grade IMU. To increase the footprint geolocation accuracy, we recommend reducing flying height while increasing the FOV which compensates the footprint area loss and increases the signal strength. The disadvantage of a lower flying height and a larger FOV is a higher sensitivity of the footprint size to changing distance from the target. To assist in matching the footprint size to uncertainty ratio with an appropriate spatial scale, we list the expected ratio for a range of IMU grades, FOVs and AGL heights.ViewShow abstract... Transient cloud cover can also substantially affect the amount of ambient light present over short durations. Another shortfall is the practical limitation of having tarps or other reference targets in all images, especially when high resolution data is desired or a large area is covered ( Zeng et al., 2017). Devising a method that can keep track of ambient light changes while measuring the reflectance from a spectral target is desired.  ...... Promising results were shown for further experiments (Burkart et al., 2014;Tsouvaltsidis et al., 2015;Von Bueren et al., 2015). In a study by Zeng et al. (2017), spectral data from a portable spectrometer was fused with multispectral camera images to provide expanded spectral information for at each pixel.  ...Classifying Soil Moisture Content Using Reflectance-Based Remote SensingThesisFull-text availableSep 2018 Ali HamidisepehrThe ability to quantify soil moisture spatial variability and its temporal dynamics
over entire fields through direct soil observations using remote sensing will improve early
detection of water stress before crop physiological or economic damage has occurred, and it will contribute to the identification of zones within a field in which soil water is depleted faster than in other zones of a field.
The overarching objective of this research is to develop tools and methods for
remotely estimating soil moisture variability in agricultural crop production. Index-based
and machine learning methods were deployed for processing hyperspectral data collected from moisture-controlled samples.
In the first of five studies described in this dissertation, the feasibility of using “lowcost”
index-based multispectral reflectance sensing for remotely delineating soil moisture
content from direct soil and crop residue measurements using down-sampled spectral data were determined. The relative reflectance from soil and wheat stalk residue were measured using visible and near-infrared spectrometers. The optimal pair of wavelengths was chosen using a script to create an index for estimating soil and wheat stalk residue moisture levels. Wavelengths were selected to maximize the slope of the linear index function (i.e., sensitivity to moisture) and either maximize the coefficient of determination (R2) or minimize the root mean squared error (RMSE) of the index. Results showed that
wavelengths centered near 1300 nm and 1500 nm, within the range of 400 to 1700 nm,
produced the best index for individual samples; however, this index worked poorly on
estimating stalk residue moisture.
In the second of five studies, 20 machine learning algorithms were applied to full
spectral datasets for moisture prediction and comparing them to the index-based method
from the previous objective. Cubic support vector machine (SVM) and ensemble bagged
trees methods produced the highest composite prediction accuracies of 96% and 93% for 
silt-loam soil samples, and 86% and 93% for wheat stalk residue samples, respectively.
Prediction accuracy using the index-based method was 86% for silt-loam soil and 30% for
wheat stalk residue.
In the third study, a spectral measurement platform capable of being deployed on a
UAS was developed for future use in quantifying and delineating moisture zones within
agricultural landscapes. A series of portable spectrometers covering ultraviolet (UV),
visible (VIS), and near-infrared (NIR) wavelengths were instrumented using a Raspberry
Pi embedded computer that was programmed to interface with the UAS autopilot for
autonomous reflectance data acquisition. A similar ground-based system was developed to keep track of ambient light during reflectance target measurement. The systems were tested under varying ambient light conditions during the 2017 Great American Eclipse.
In the fourth study, the data acquisition system from the third study was deployed
for recognizing different targets in the grayscale range using machine learning methods
and under ambient light conditions. In this study, a dynamic method was applied to update integration time on spectrometers to optimize sensitivity of the instruments. It was found that by adjusting the integration time on each spectrometer such that a maximum intensity across all wavelengths was reached, the targets could be recognized simply based on the reflectance measurements with no need of a separate ambient light measurement.
Finally, in the fifth study, the same data acquisition system and variable integration
time method were used for estimating soil moisture under ambient light condition. Among 22 machine learning algorithms, linear and quadratic discriminant analysis achieved the maximum prediction accuracy.
A UAS-deployable hyperspectral data acquisition system containing three portable
spectrometers and an embedded computer was developed to classify moisture content from spectral data. Partial least squares regression and machine learning algorithms were shown to be effective to generate predictive models for classifying soil moisture.ViewShow abstract... Moreover, computer vision and open source philosophy has improved the manageability of UAVs and data processing. Therefore, the increased technological developments in Unmanned Aircraft Systems (UAS) combined with hyperspectral imagery and machine learning approaches have opened the possibility of improving image resolution and quality[15]and research outcomes in biosecurity[16]. Over the last few decades, there has been little historical research on the detection of mounds through remote sensing.  ...... Moreover, computer vision and open source philosophy has improved the manageability of UAVs and data processing. Therefore, the increased literature and technological developments in Unmanned Aircraft Systems (UAS) design and path planning, combined with hyperspectral imagery and machine learning approaches have opened the possibility of improving image quality [15] and increasing research outcomes in biosecurity [16], air quality, precision agriculture and environmental sensing [17][18][19][20].  ...Towards the Automatic Detection of Pre-Existing Termite Mounds through UAS and Hyperspectral ImageryArticleFull-text availableSep 2017SENSORS-BASEL Juan SandinoAdam Wooler Luis Felipe GonzalezThe increased technological developments in Unmanned Aerial Vehicles (UAVs) combined with artificial intelligence and Machine Learning (ML) approaches have opened the possibility of remote sensing of extensive areas of arid lands. In this paper, a novel approach towards the detection of termite mounds with the use of a UAV, hyperspectral imagery, ML and digital image processing is intended. A new pipeline process is proposed to detect termite mounds automatically and to reduce, consequently, detection times. For the classification stage, several ML classification algorithms’ outcomes were studied, selecting support vector machines as the best approach for their role in image classification of pre-existing termite mounds. Various test conditions were applied to the proposed algorithm, obtaining an overall accuracy of 68%. Images with satisfactory mound detection proved that the method is “resolution-dependent”. These mounds were detected regardless of their rotation and position in the aerial image. However, image distortion reduced the number of detected mounds due to the inclusion of a shape analysis method in the object detection phase, and image resolution is still determinant to obtain accurate results. Hyperspectral imagery demonstrated better capabilities to classify a huge set of materials than implementing traditional segmentation methods on RGB images only.ViewShow abstract... Moreover, unmanned aerial vehicle (UAV) technology is also employed in the agricultural information acquisition. A typical application of UAV technology is to acquire high-resolution visible, spectral or infrared images of plants at low attitude to achieve large scope farmland monitoring and hazard prediction ( Lan et al., 2017;Zeng and King, 2017;Gracia-Romero et al., 2018). However, there is little exploration of the co-operation and integration between WSN and UAV technology.  ...Computers and Electronics in Agriculture Automatic delivery and recovery system of Wireless Sensor Networks (WSN) nodes based on UAV for agricultural applications ☆ArticleFull-text availableApr 2019COMPUT ELECTRON AGR Fan Ouyang Hui Cheng Yubin LanShengde ChenIn recent times, Wireless Sensor Network (WSN) technology is widely applied in various agricultural applications. However, when a WSN becomes large and complex, it is difficult to maintain and adjust the whole WSN or multiple WSNs manually. Therefore, we propose an UAV-based scheme for automatic delivery and recovery of WSN nodes in the field. This scheme includes a combination of specially designed UAV, node platforms together with a GNSS-RTK (Global Navigation Satellite System, Real-Time Kinematic) system. A prototype system was built to test the UAV-based scheme. The results and analyses of indoor and outdoor experiments demonstrated the feasibility and efficacy of the scheme.ViewShow abstract... The data from the UAV and ground sensors need to be aligned based on time/coordinate prior to calculation of reflectance. Reflectance of ground targets is calculated as stated in [25].  ...A UAV-Based Sensor System for Measuring Land Surface Albedo: Tested over a Boreal Peatland EcosystemArticleFull-text availableMar 2019 Francis CanisiusShusen Wang Holly Croft Rong WangA multiple sensor payload for a multi-rotor based UAV platform was developed and tested for measuring land surface albedo and spectral measurements at user-defined spatial, temporal, and spectral resolutions. The system includes a Matrice 600 UAV with an RGB camera and a set of four downward pointing radiation sensors including a pyranometer, quantum sensor, and VIS and NIR spectrometers, measuring surface reflected radiation. A companion ground unit consisting of a second set of identical sensors simultaneously measure downwelling radiation. The reflected and downwelling radiation measured by the four sensors are used for calculating albedo for the total shortwave broadband, visible band and any narrowband at a 1.5 nm spectral resolution within the range of 350-1100 nm. The UAV-derived albedo was compared with those derived from Landsat 8 and Sentinel-2 satellite observations. Results show the agreement between total shortwave albedo from UAV pyranometer and Landsat 8 (R 2 = 0.73) and Sentinel-2 (R 2 = 0.68). Further, total shortwave albedo was estimated from spectral measurements and compared with the satellite-derived albedo. This UAV-based sensor system promises to provide high-resolution multi-sensors data acquisition. It also provides maximal flexibility for data collection at low cost with minimal atmosphere influence, minimal site disturbance, flexibility in measurement planning, and ease of access to study sites (e.g., wetlands) in contrast with traditional data collection methods.ViewShow abstract... The sensor is able to detect weeds when they become heavy and widespread (Ghiyamat and Shafri 2010) and this has serious economic implications. Alternatively, the use of hyperspectral data with many narrow contiguous spectral bands that are sensitive to phenological changes is a promising option ( Zeng et al. 2017). Hyperspectral remote sensing offers greater spectral resolving power than multispectral data, making it well-suited for extracting weeds' biochemical and structural attributes at fine scales ( Underwood et al. 2003).  ...Remote sensing of forest health and vitality: a South African perspectiveArticleFull-text availableNov 2018SOUTH FORESTS Sifiso Xulu Kabir Y Peerbhay Michael GebreslasieCommercial forestry plantations are an important and valuable segment of the South African economy and forest managers are required to maximise and sustain forest productivity. However, various factors such as the outbreak of damaging agents are constantly hampering forest health and thus decrease productivity. It is therefore important to detect the presence and spread of these agents within plantation forests, a task efficiently achieved using remote sensing technology. A wide assortment of sensors with varying resolutions are available and have been extensively used for this purpose. This paper reviews the current status of remote sensing of forest health in South Africa by providing insight on the latest developments on the use of the technology in forest plantations. A systematic search was executed on Google Scholar, ScienceDirect® and EBSCOhost® databases that identified 627 articles of which 29 made reference to remote sensing of forest health in South Africa. Four key results were found: (1) the latest technology is capable of detecting and monitoring forest health with great accuracy, especially with the adoption of machine learning methods; (2) studies employing remote sensing to characterise forest health have burgeoned since 2006 with even more applying hyperspectral data; (3) most studies were spatially concentrated in the KwaZulu-Natal Midlands region around Pietermaritzburg with only a few over the Western Cape; and (4) the remote detection of pest outbreaks and pathogens have received much attention followed by alien invasive plants and a few studies directed to fragmentation. Present and future partnerships may open up opportunities for exploit- ing remote sensing further; this should address growing expectations from government and industry for more detailed and accurate information concerning the health and condition of South Africa's plantation forests.ViewShow abstract... Furthermore, UAV point spectrometers have been used to investigate the impact of environmental variables on water reflectance [26] and for intercomparison with data from a moderate resolution imaging spectroradiometer (MODIS) in Greenland [27]. Besides, UAV point measurements have been fused with multispectral 2D imaging sensors [28] and applied from fixed-wing UAVs [29], e.g., to monitor phytoplankton bloom [30]. Additionally, first attempts have been made to build low-cost whiskbroom systems for UAVs, which are able to quickly scan points on the surface [31].  ...Quantitative Remote Sensing at Ultra-High Resolution with UAV Spectroscopy: A Review of Sensor Technology, Measurement Procedures, and Data Correction WorkflowsArticleFull-text availableJul 2018 Helge Aasen Eija Honkavaara Arko Lucieer Pablo J. Zarco-TejadaIn the last 10 years, development in robotics, computer vision, and sensor technology has provided new spectral remote sensing tools to capture unprecedented ultra-high spatial and high spectral resolution with unmanned aerial vehicles (UAVs). This development has led to a revolution in geospatial data collection in which not only few specialist data providers collect and deliver remotely sensed data, but a whole diverse community is potentially able to gather geospatial data that fit their needs. However, the diversification of sensing systems and user applications challenges the common application of good practice procedures that ensure the quality of the data. This challenge can only be met by establishing and communicating common procedures that have had demonstrated success in scientific experiments and operational demonstrations. In this review, we evaluate the state-of-the-art methods in UAV spectral remote sensing and discuss sensor technology, measurement procedures, geometric processing, and radiometric calibration based on the literature and more than a decade of experimentation. We follow the ‘journey’ of the reflected energy from the particle in the environment to its representation as a pixel in a 2D or 2.5D map, or 3D spectral point cloud. Additionally, we reflect on the current revolution in remote sensing, and identify trends, potential opportunities, and limitations.ViewShow abstractA Low-cost Method for Collecting Hyperspectral Measurements from a Small Unmanned Aircraft SystemConference PaperFull-text availableMay 2018 Ali Hamidisepehr Michael SamaSmall unmanned aircraft systems (UAS) are a relatively new tool for collecting remote sensing data at dense spatial and temporal resolutions. This study aimed to develop a spectral measurement platform for deployment on a UAS for quantifying and delineating moisture zones within an agricultural landscape. A series of portable spectrometers covering ultraviolet (UV), visible (VIS), and near-infrared (NIR) wavelengths were instrumented using a Raspberry Pi embedded computer that was programmed to interface with the UAS autopilot for autonomous data acquisition. A second set of identical spectrometers were fitted with calibrated irradiance lenses to capture ambient light during data acquisition. Data were collected during the 2017 Great American Eclipse while observing a reflectance target to determine the ability to compensate for ambient light conditions. A calibration routine was developed that scaled raw reflectance data by sensor integration time and ambient light energy. The resulting calibrated reflectance exhibited a consistent spectral profile and average intensity across a wide range of ambient light conditions. Results indicated the potential for mitigating the effect of ambient light when passively measuring reflectance on a portable spectral measurement system. Future work will use multiple reflectance targets to test the ability to classify targets based on spectral signatures under a wide range of ambient light conditions.ViewShow abstractMulti-depth suspended sediment estimation using high-resolution remote-sensing UAV in Maumee River, OhioArticleFull-text availableApr 2018INT J REMOTE SENS Matthew D. Larson Simic Milas Anita Robert K Vincent James EvansSatellite remote-sensing has been widely used to map suspended sediment concentration (SSC) in waterbodies. Current development of the unmanned aerial vehicle (UAV) technology allows mapping of SSC at finer spatial resolution providing high flexibility in terms of cost and acquisition time. However, the technology is immature and transfer of empirical algorithms from existing remote-sensing technologies to UAV still has to be explored. This study uses the MicaSense Sequoia sensor with four bands (green, red, red edge, and near-infrared [NIR]) mounted on-board a fixed-wing UAV to map SSC within the Maumee River in Ohio, USA, at multiple depth intervals (15, 61, 91, and 182 cm). The simple linear and stepwise regression models show the advantage of multiple bands and band ratios over single bands in mapping SSC. The findings show a limited performance of the Sequoia sensor when compared to field spectroradiometer measurements. In all cases but one, the adjusted coefficient of determination ( ) values is lower for the UAV data. The regression equations become similar at and below a depth of 0–61 cm, and become constant at and below a depth of 0–91 cm. While the spectroradiometer-related equations are sensitive to a wider spectral range (from green at the surface to NIR wavelength at 182 cm depth), the UAV-related equations are insensitive to green spectrum and they include a narrower spectral range (from red to NIR) over all depth increments. Field spectroradiometer measurements exhibit a strong relationship with cumulative SSC at 182 cm depth (0–182 cm) ( ) whereas UAV reflectance data show the best relationship with SSC at 91 cm (0–91 cm) ( ) suggesting that ~91 cm may be an optimal depth for UAV under given conditions. The results show that UAVs can be a practical but somewhat limited tool to monitor SSC in small- to medium-sized rivers.ViewShow abstractShow moreThe impacts of environmental variables on water reflectance measured using a lightweight unmanned aerial vehicle (UAV)-based spectrometer systemArticleFull-text availableAug 2017ISPRS J PHOTOGRAMM Chuiqing Zeng Murray Richardson Douglas J. KingRemote sensing methods to study spatial and temporal changes in water quality using satellite or aerial imagery are limited by the inherently low reflectance signal of water across the visible and near infrared spectrum, as well as environmental variables such as surface scattering effects (sun glint), substrate and aquatic vegetation reflectance, and atmospheric effects. This study exploits the low altitude, high-resolution remote sensing capabilities of unmanned aerial vehicle (UAV) platforms to examine the major environmental variables that affect water reflectance acquisition, without the confounding influence of atmospheric effects typical of higher-altitude platforms. After observation and analysis, we found: (1) multiple water spectra measured at the same location had a standard deviation of 10.4%; (2) water spectra changes associated with increasing altitude from 20 m to 100 m were negligible; (3) the difference between mean reflectance at three off-shore locations in an urban water body reached 29.9%; (4) water bottom visibility increased water reflectance by 20.1% in near shore areas compared to deep water spectra in a clear water lake; (5) emergent plants caused the water spectra to shift towards a shape that is characteristic of vegetation, whereas submerged vegetation showed limited effect on water spectra in the studied lake; (6) cloud and sun glint had major effects and caused water spectra to change abruptly; while glint and shadow effects on spectra may balance each other under certain conditions, the water reflectance can also be unpredictable at times due to wave effects and their effects on lines-of-site to calm water; (7) water spectra collected under a variety of different conditions (e.g. multiple locations, waves) resulted in weaker regression models compared to spectra collected under ideal conditions (e.g. single location, no wave), although the resulting model coefficients were relatively stable. The methods and results from this study contribute to better understanding of water reflectance acquisition using remote sensing, and can be applied in UAV-based water quality assessment or to aid in validation of higher altitude imagery.ViewShow abstractHyperspectral and Multispectral Data Fusion: A comparative review of the recent literatureArticleJun 2017 Naoto Yokoya Claas Grohnfeldt Jocelyn ChanussotIn recent years, enormous efforts have been made to design image-processing algorithms to enhance the spatial resolution of hyperspectral (HS) imagery. One of the most commonly addressed problems is the fusion of HS data with higher spatial resolution multispectral (MS) data. Various techniques have been proposed to solve this data-fusion problem based on different theories, including component substitution (CS), multiresolution analysis (MRA), spectral unmixing, and Bayesian probability. This article presents a comparative review of those HS-MS fusion techniques with extensive experiments. Ten state-of-the-art HS-MS fusion methods are compared by assessing their fusion performance both quantitatively and visually. Eight data sets featuring different geographical and sensor characteristics are used in the experiments to evaluate the generalizability and versatility of the fusion algorithms. To maximize the fairness and transparency of this comparison, publicly available source codes are used, and parameters are individually tuned for maximum performance.ViewShow abstractSeasonal stability of chlorophyll fluorescence quantified from airborne hyperspectral imagery as an indicator of net photosynthesis in the context of precision agricultureArticleJun 2016REMOTE SENS ENVIRON Pablo J. Zarco-Tejada Victoria Gonzalez-dugo E. FereresThe seasonal stability of solar-induced chlorophyll fluorescence (SIF) vs field-measured leaf CO2 assimilation (A) was assessed over a period of 2 years by means of airborne flights performed at midday and diurnally over a citrus (evergreen) crop canopy. The orchard was cultivated under a control treatment (ET) that received 100% of its water requirements and two regulated deficit irrigation (RDI) treatments with water supply reduced to 37% and 50% of the control level during the summer. Field measurements consisted of assimilation rate, stomatal conductance, stem water potential, leaf fluorescence and leaf reflectance. The airborne campaigns took place in 2012 and 2013, and were flown on the solar plane in order to acquire hyperspectral imagery at 40 cm resolution, 260 spectral bands and 1.85 nm/pixels in the 400-885 nm spectral region. A thermal camera was installed in tandem in all flights, acquiring imagery in the 7.5-13 μm spectral region at 640 × 480 pixel resolution, yielding a 50 cm pixel size. The robustness of the SIF quantification through the Fraunhofer Line Depth (FLD) principle based on three spectral bands (FLD3), as well as the performance of physiological and structural hyperspectral indices, was evaluated in order to understand their ability to track photosynthesis at different phenological and stress stages throughout the season. Solar induced fluorescence quantified as FLD3 was the most robust indicator of photosynthesis in all the airborne campaigns performed in the course of the two-year experiment, which comprised seven midday flights and two diurnals. The relationships between fluorescence (FLD3) and assimilation rates yielded correlation coefficients (R) between 0.64 and 0.82 across all dates, these being statistically significant with p-values between p < 0.05 and p < 0.0001. Fluorescence retrievals performed better than structural and physiological indices, with the structural MTVI1 index being the only other statistically significant indicator throughout the season, although it yielded lower levels of significance than FLD3. A normalization strategy proposed for SIF FLD3 for all dates using control (ET) trees on each flight date as a reference permitted the generation of a single relationship between FLD3 normalized (FLDn) and assimilation rates for the entire year, both at tree (r2 = 0.5; p < 0.0001) and treatment level (r2 = 0.72; p < 0.0001), a strategy that confirmed the ability of seasonal SIF retrievals to track photosynthesis from broader resolution hyperspectral imagery (i.e. spectral resolution 1-5 nm full-width at half maximum (FWHM)) for applications in the context of precision agriculture and crop-monitoring studies.ViewShow abstractMissing data imputation toolbox for MATLABArticleFull-text availableMar 2016CHEMOMETR INTELL LAB Abel Folch-Fortuny Francisco Arteaga Alberto FerrerHere we introduce a graphical user-friendly interface to deal with missing values called Missing Data Imputation (MDI) Toolbox. This MATLAB toolbox allows imputing missing values, following missing completely at random patterns, exploiting the relationships among variables. In this way, principal component analysis (PCA) models are fitted iteratively to impute the missing data until convergence. Different methods, using PCA internally, are included in the toolbox: trimmed scores regression (TSR), known data regression (KDR), KDR with principal component regression (KDR-PCR), KDR with partial least squares regression (KDR-PLS), projection to the model plane (PMP), iterative algorithm (IA), modified nonlinear iterative partial least squares regression algorithm (NIPALS) and data augmentation (DA). MDI Toolbox presents a general procedure to impute missing data, thus can be used to infer PCA models with missing data, to estimate the covariance structure of incomplete data matrices, or to impute the missing values as a preprocessing step of other methodologies.ViewShow abstractEvaluating Multispectral Images and Vegetation Indices for Precision Farming Applications from UAV ImagesArticleFull-text availableApr 2015 Sebastian Candiago Fabio Remondino Michaela De GiglioGattelli MarioUnmanned Aerial Vehicles (UAV)-based remote sensing offers great possibilities to acquire in a fast and easy way field data for precision agriculture applications. This field of study is rapidly increasing due to the benefits and advantages for farm resources management, particularly for studying crop health. This paper reports some experiences related to the analysis of cultivations (vineyards and tomatoes) with Tetracam multispectral data. The Tetracam camera was mounted on a multi-rotor hexacopter. The multispectral data were processed with a photogrammetric pipeline to create triband orthoimages of the surveyed sites. Those orthoimages were employed to extract some Vegetation Indices (VI) such as the Normalized Difference Vegetation Index (NDVI), the Green Normalized Difference Vegetation Index (GNDVI), and the Soil Adjusted Vegetation Index (SAVI), examining the vegetation vigor for each crop. The paper demonstrates the great potential of high-resolution UAV data and photogrammetric techniques applied in the agriculture framework to collect multispectral images and evaluate different VI, suggesting that these instruments represent a fast, reliable, and cost-effective resource in crop assessment for precision farming applications.ViewShow abstractDevelopment of a Low-Cost, Lightweight Hyperspectral Imaging System Based on a Polygon Mirror and Compact SpectrometersArticleFull-text availableSep 2015IEEE J-STARS Kuniaki UtoHaruyuki SekiGenya Saito Teruhisa KomatsuLow-altitude hyperspectral observation systems using aerial observation with unmanned aerial vehicles (UAVs) have advantages over satellite systems with respect to frequency, accuracy, and spatial resolution. Although low-cost lightweight UAVs have become available in recent years, the current price ranges of lightweight pushbroom and snapshot hyperspectral sensors remain high. For sustainable operation of UAV-mounted hyperspectral sensing, the challenge in production has been shifted from the size and weight to the cost of the lightweight hyperspectral sensors. In this paper, we develop a low-cost, lightweight whiskbroom hyperspectral imaging system. The gross weight of the sensor is 1200 g. The spectral range of the 256-band spectrometer extends from 340 to 750 nm with a 14-nm spectral resolution. The viewing angle across the flight direction is controlled by the rotation of an eight-sided polygon mirror. When the exposure time, flight altitude, flight speed, and focal length of the optical lens are 3.2 ms, 10 m, 10 m/s, and 8 mm, respectively, then the estimated values of the swath and the area coverage per second are 13.4 m and [Formula: see text], respectively. The spatial resolution is [Formula: see text]. In preliminary close-range measurements with a 3.2-ms exposure time per area and a 1224-ms rotation period of the polygon mirror, the reflected light from 12 areas of a printed checkered color pattern in the moving direction were measured. We found that the calculated reflectance based on the measurements is spatially consistent and spectrally accurate.ViewShow abstractOverview and Current Status of Remote Sensing Applications Based on Unmanned Aerial Vehicles (UAVs)ArticleApr 2015PHOTOGRAMM ENG REM S Gonzalo PajaresRemotely Piloted Aircraft (RPA) is presently in continuous development at a rapid pace. Unmanned Aerial Vehicles (UAVs) or more extensively Unmanned Aerial Systems (UAS) are platforms considered under the RPAs paradigm. Simultaneously, the development of sensors and instruments to be installed onboard such platforms is growing exponentially. These two factors together have led to the increasing use of these platforms and sensors for remote sensing applications with new potential. Thus, the overall goal of this paper is to provide a panoramic overview about the current status of remote sensing applications based on unmanned aerial platforms equipped with a set of specific sensors and instruments. First, some examples of typical platforms used in remote sensing are provided. Second, a description of sensors and technologies is explored which are onboard instruments specifically intended to capture data for remote sensing applications. Third, multi-UAVs in collaboration, coordination, and cooperation in remote sensing are considered. Finally, a collection of applications in several areas are proposed, where the combination of unmanned platforms and sensors, together with methods, algorithms, and procedures provide the overview in very different remote sensing applications. This paper presents an overview of different areas, each independent from the others, so that the reader does not need to read the full paper when a specific application is of interest.ViewShow abstractGenerating 3D hyperspectral information with lightweight UAV snapshot cameras for vegetation monitoring: From camera calibration to quality assuranceArticleOct 2015ISPRS J PHOTOGRAMM Helge Aasen Andreas Burkart Andreas BoltenGeorg BarethThis paper describes a novel method to derive 3D hyperspectral information from lightweight snapshot cameras for unmanned aerial vehicles for vegetation monitoring. Snapshot cameras record an image cube with one spectral and two spatial dimensions with every exposure. First, we describe and apply methods to radiometrically characterize and calibrate these cameras. Then, we introduce our processing chain to derive 3D hyperspectral information from the calibrated image cubes based on structure from motion. The approach includes a novel way for quality assurance of the data which is used to assess the quality of the hyperspectral data for every single pixel in the final data product. The result is a hyperspectral digital surface model as a representation of the surface in 3D space linked with the hyperspectral information emitted and reflected by the objects covered by the surface. In this study we use the hyperspectral camera Cubert UHD 185-Firefly, which collects 125 bands from 450 to 950 nm. The obtained data product has a spatial resolution of approximately 1 cm for the spatial and 21 cm for the hyperspectral information. The radiometric calibration yields good results with less than 1% offset in reflectance compared to an ASD FieldSpec 3 for most of the spectral range. The quality assurance information shows that the radiometric precision is better than 0.13% for the derived data product. We apply the approach to data from a flight campaign in a barley experiment with different varieties during the growth stage heading (BBCH 52 – 59) to demonstrate the feasibility for vegetation monitoring in the context of precision agriculture. The plant parameters retrieved from the data product correspond to in-field measurements of a single date field campaign for plant height (R 2 = 0.7), chlorophyll (BGI2, R 2 = 0.52), LAI (RDVI, R 2 = 0.32) and biomass (RDVI, R 2 = 0.29). Our approach can also be applied for other image-frame cameras as long as the individual bands of the image cube are spatially co-registered beforehand.ViewShow abstractORTHO-RECTIFICATION OF NARROW BAND MULTI-SPECTRAL IMAGERY ASSISTED BY DSLR RGB IMAGERY ACQUIRED BY A FIXED-WING UASArticleFull-text availableAug 2015 Jiann-Yeou Rau Jyun-Ping Jhan Cho-ying HuangMiniature Multiple Camera Array (MiniMCA-12) is a frame-based multilens/multispectral sensor composed of 12 lenses with narrow band filters. Due to its small size and light weight, it is suitable to mount on an Unmanned Aerial System (UAS) for acquiring high spectral, spatial and temporal resolution imagery used in various remote sensing applications. However, due to its wavelength range is only 10 nm that results in low image resolution and signal-to-noise ratio which are not suitable for image matching and digital surface model (DSM) generation. In the meantime, the spectral correlation among all 12 bands of MiniMCA images are low, it is difficult to perform tie-point matching and aerial triangulation at the same time. In this study, we thus propose the use of a DSLR camera to assist automatic aerial triangulation of MiniMCA-12 imagery and to produce higher spatial resolution DSM for MiniMCA12 ortho-image generation. Depending on the maximum payload weight of the used UAS, these two kinds of sensors could be collected at the same time or individually. In this study, we adopt a fixed-wing UAS to carry a Canon EOS 5D Mark2 DSLR camera and a MiniMCA-12 multi-spectral camera. For the purpose to perform automatic aerial triangulation between a DSLR camera and the MiniMCA-12, we choose one master band from MiniMCA-12 whose spectral range has overlap with the DSLR camera. However, all lenses of MiniMCA-12 have different perspective centers and viewing angles, the original 12 channels have significant band misregistration effect. Thus, the first issue encountered is to reduce the band misregistration effect. Due to all 12 MiniMCA lenses being frame-based, their spatial offsets are smaller than 15 cm and all images are almost 98% overlapped, we thus propose a modified projective transformation (MPT) method together with two systematic error correction procedures to register all 12 bands of imagery on the same image space. It means that those 12 bands of images acquired at the same exposure time will have same interior orientation parameters (IOPs) and exterior orientation parameters (EOPs) after band-to-band registration (BBR). Thus, in the aerial triangulation stage, the master band of MiniMCA-12 was treated as a reference channel to link with DSLR RGB images. It means, all reference images from the master band of MiniMCA-12 and all RGB images were triangulated at the same time with same coordinate system of ground control points (GCP). Due to the spatial resolution of RGB images is higher than the MiniMCA-12, the GCP can be marked on the RGB images only even they cannot be recognized on the MiniMCA images. Furthermore, a one meter gridded digital surface model (DSM) is created by the RGB images and applied to the MiniMCA imagery for ortho-rectification. Quantitative error analyses show that the proposed BBR scheme can achieve 0.33 pixels of average misregistration residuals length and the co-registration errors among 12 MiniMCA ortho-images and between MiniMCA and Canon RGB ortho-images are all less than 0.6 pixels. The experimental results demonstrate that the proposed method is robust, reliable and accurate for future remote sensing applications.ViewShow abstractAdvances in remote sensing of vegetation function and traitsArticleJul 2015INT J APPL EARTH OBS Rasmus HouborgJoshua B. Fisher Andrew K. SkidmoreRemote sensing of vegetation function and traits has advanced significantly over the past half-century in the capacity to retrieve useful plant biochemical, physiological and structural quantities across a range of spatial and temporal scales. However, the translation of remote sensing signals into meaningful descriptors of vegetation function and traits is still associated with large uncertainties due to complex interactions between leaf, canopy, and atmospheric mediums, and significant challenges in the treatment of confounding factors in spectrum-trait relations. This editorial provides (1) a background on major advances in the remote sensing of vegetation, (2) a detailed timeline and description of relevant historical and planned satellite missions, and (3) an outline of remaining challenges, upcoming opportunities and key research objectives to be tackled. The introduction sets the stage for thirteen Special Issue papers here that focus on novel approaches for exploiting current and future advancements in remote sensor technologies. The described enhancements in spectral, spatial and temporal resolution and radiometric performance provide exciting opportunities to significantly advance the ability to accurately monitor and model the state and function of vegetation canopies at multiple scales on a timely basis.ViewShow abstractShow moreJoin ResearchGate to find the people and research you need to help your work.15+ million members118+ million publications700k+ research projectsJoin for freeRecommendationsDiscover more publications, questions and projects in SpectrometersProjectCurrent Projects: Hyperspectral tundra vegetation productivity; UAV multi/hyperspectral imaging for farming & water quality; Farmland dynamics & biodiversity; Grassland/forage classification; Forested wetland vegetation/hydrology characterization. Douglas J. KingVarious. Contact for more information.  View projectProjectImpacts of spatial heterogeneity in farmlands on ecosystem services and biodiversity Scott W. Mitchell Lenore Fahrig Douglas J. King[...] Joseph R BennettView projectProjectMonitoring forest cover change in Karnataka, India using Landsat Data Ravinder Virk Douglas J. KingView projectConference PaperGPU Acceleration of UAV Image Splicing Using Oriented Fast and Rotated Brief Combined with PCAJuly 2018Chia-Cheng YehYang-Lang ChangPai-Hui HsuCheng-Huan HsienRead moreChapter3D Data Fusion Using Unmanned Aerial Vehicle (UAV) Photogrammetry and Terrestrial Laser Scanner (TLS...January 2018Mohamad Aizat Asyraff Mohamad Azmi Mohd Azwan Abbas Khairulazhar Zainuddin[...] Anuar AspuriRead moreArticleFull-text availableCONCEPT FOR CLASSIFYING FACADE ELEMENTS BASED ON MATERIAL, GEOMETRY AND THERMAL RADIATION USING MULT...August 2017Rebecca Ilehag Andreas SchenkS. HinzThis paper presents a concept for classification of facade elements, based on the material and the geometry of the elements in addition to the thermal radiation of the facade with the usage of a multimodal Unmanned Aerial Vehicle (UAV) system. Once the concept is finalized and functional, the workflow can be used for energy demand estimations for buildings by exploiting existing methods for ... [Show full abstract] estimation of heat transfer coefficient and the transmitted heat loss. The multimodal system consists of a thermal, a hyperspectral and an optical sensor, which can be operational with a UAV. While dealing with sensors that operate in different spectra and have different technical specifications, such as the radiometric and the geometric resolution, the challenges that are faced are presented. Addressed are the different approaches of data fusion, such as image registration, generation of 3D models by performing image matching and the means for classification based on either the geometry of the object or the pixel values. As a first step towards realizing the concept, the result from a geometric calibration with a designed multimodal calibration pattern is presented.View full-textArticleFull-text availableOBSTACLE DETECTION SYSTEM INVOLVING FUSION OF MULTIPLE SENSOR TECHNOLOGIESAugust 2017 Carmine Giannì Marco Balsi Salvatore Esposito P. FallavollitaObstacle detection is a fundamental task for Unmanned Aerial Vehicles (UAV) as a part of a Sense and Avoid system. In this study, we present a method of multi-sensor obstacle detection that demonstrated good results on different kind of obstacles. This method can be implemented on low-cost platforms involving a DSP or small FPGA. In this paper, we also present a study on the typical targets that ... [Show full abstract] can be tough to detect because of their characteristics of reflectivity, form factor, heterogeneity and show how data fusion can often overcome the limitations of each technology.View full-textChapterAnalyzing UAV-Based Remote Sensing and WSN Support for Data FusionJanuary 2018 · Advances in Intelligent Systems and Computing Ramón Alcarria Borja Bordel Sánchez Miguel Ángel Manso Callejo[...] Marina PérezRecent developments in remote sensing are significantly contributing to exploration and data acquisition through improved efficiency and risk reduction. Unmanned Aerial Vehicles (UAV) are involved in a wide range of remote sensing applications, as they are rapid, efficient and flexible acquisition systems. They represent a valid alternative or a complementary solution to satellite or airborne ... [Show full abstract] sensors. Wireless Sensor Networks (WSN), on the other hand, have proliferated significantly in recent years thanks to their timely, cheap and extremely rich data acquisition capacity with respect to other acquisition systems. This paper analyzes current state of the art in UAV-based remote sensing and WSN support for the generation of integrated data. An architecture is proposed for the combination of these technologies and the data acquisition, communication, fusion, and presentation processes are analyzed along with the proposal of future challenges.Read moreDiscover moreDownload citationWhat type of file do you want? RIS BibTeX Plain TextWhat do you want to download? Citation only Citation and abstractDownloadInterested in research on Spectrometers?Join ResearchGate to discover and stay up-to-date with the latest research from leading experts in Spectrometers and many other scientific topics.Join for freeorDiscover by subject areaRecruit researchersJoin for freeLoginEmail Tip: Most researchers use their institutional email address as their ResearchGate loginPasswordForgot password? Keep me logged inLog inorContinue with LinkedInContinue with GoogleWelcome back! Please log in.Email · HintTip: Most researchers use their institutional email address as their ResearchGate loginPasswordForgot password? Keep me logged inLog inorContinue with LinkedInContinue with GoogleNo account? Sign upAboutNewsCompanyCareersSupportHelp centerFAQBusiness solutionsRecruitingAdvertising© ResearchGate 2019. All rights reserved.ImprintTermsPrivacy