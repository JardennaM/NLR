url: https://www.researchgate.net/publication/331929918_A_Distributed_Architecture_for_Human-Drone_Teaming_Timing_Challenges_and_Interaction_Opportunities








    






 













































(PDF) A Distributed Architecture for Human-Drone Teaming: Timing Challenges and Interaction Opportunities


We use cookies to make interactions with our website easy and meaningful, to better understand the use of our services, and to tailor advertising. For further information, including about cookie settings, please read our Cookie Policy . By continuing to use this site, you consent to the use of cookies.Got itWe value your privacyWe use cookies to offer you a better experience, personalize content, tailor advertising, provide social media features, and better understand the use of our services.To learn more or modify/prevent the use of cookies, see our Cookie Policy and Privacy Policy.Accept CookiestopSee all ›16 ReferencesSee all ›2 FiguresDownload citationShare  Facebook Twitter LinkedIn RedditDownload full-text PDFA Distributed Architecture for Human-Drone Teaming: Timing Challenges and Interaction OpportunitiesArticle (PDF Available) in Sensors 19(6):1379 · March 2019 with 26 ReadsDOI: 10.3390/s19061379  Cite this publication Karin Hummel Manuela Pollak2.79Johannes Kepler University Linz Johannes KrahoferAbstractDrones are expected to operate autonomously, yet they will also interact with humans to solve tasks together. To support civilian human-drone teams, we propose a distributed architecture where sophisticated operations such as image recognition, coordination with humans, and flight-control decisions are made, not on-board the drone, but remotely. The benefits of such an architecture are the increased computational power available for image recognition and the possibility to integrate interfaces for humans. On the downside, communication is necessary, resulting in the delayed reception of commands. In this article, we discuss the design considerations of the distributed approach, a sample implementation on a smartphone, and an application to the concrete use case of bookshelf inventory. Further, we report experimentally-derived first insights into messaging and command response delays with a custom drone connected through Wi-Fi.Discover the world's research15+ million members118+ million publications700k+ research projectsJoin for free Figures - available via license: CC BY 4.0Content may be subject to copyright. Visualization of book detection results marked as red rectangles, observed at three different distances.…  Overview of the testbed drone hardware and software.… Download full-text PDFAvailable via license: CC BY 4.0Content may be subject to copyright. 





sensorsArticleA Distributed Architecture for Human-DroneTeaming: Timing Challenges andInteraction OpportunitiesKarin Anna Hummel * , Manuela Pollak and Johannes KrahoferDepartment of Telecooperation, Johannes Kepler University, Altenberger Strasse 69, 4040 Linz, Austria;manuela.pollak@jku.at (M.P.); Johannes.Krahofer@gmx.at (J.K.)*Correspondence: karin_anna.hummel@jku.at; Tel.: +43-732-2468-4699Received: 15 October 2018; Accepted: 28 February 2019; Published: 20 March 2019Abstract:Drones are expected to operate autonomously, yet they will also interact with humansto solve tasks together. To support civilian human-drone teams, we propose a distributedarchitecture where sophisticated operations such as image recognition, coordination with humans,and ﬂight-control decisions are made, not on-board the drone, but remotely. The beneﬁts of such anarchitecture are the increased computational power available for image recognition and the possibilityto integrate interfaces for humans. On the downside, communication is necessary, resulting in thedelayed reception of commands. In this article, we discuss the design considerations of the distributedapproach, a sample implementation on a smartphone, and an application to the concrete use caseof bookshelf inventory. Further, we report experimentally-derived ﬁrst insights into messaging andcommand response delays with a custom drone connected through Wi-Fi.Keywords:drones; UAVs (unmanned aerial vehicles); networked systems; network performance;human–robot interaction1. IntroductionIn recent years, employing mini-drones, also known as UAVs (unmanned aerial vehicles) andmicro-aerial vehicles, in civilian applications has become popular. The capabilities of drones to screenan area quickly with on-board cameras and to establish wireless networks in the air [1,2] providenovel opportunities in search and rescue missions [3], 3D cartography, entertainment, (small) productdelivery, smart farming, surveillance, and many other application domains. Yet, assuring the technicalpreconditions of safe and practical autonomous ﬂight is challenging due to limited system reliability,short ﬂight-time, and restricting legal regulations. Further, in realistic settings, autonomous droneswill likely cooperate with humans, which is a new ﬁeld of human–computer interaction.Employing drones in human-drone teams involves the coordination of autonomous dronebehavior and user interactions. A major challenge is to ﬁnd a ﬁtting architecture that supports bothaspects. One unconventional option to follow is a distributed system design, where only basic controlis implemented on the drone’s microcontroller and computationally-intense tasks of drone behaviorare executed on a more powerful remote computer, or even in a cloud computing infrastructure.The on-board controller is basically used to receive commands and to execute them similar to manualremote control. Such a distributed architecture is advantageous as it overcomes limitations in applying,e.g., machine learning, image processing, and big data analysis on drones. Further, human interactionscan be integrated well into the remote control loop. On the downside, communication delays occur inthis distributed architecture.To split drone applications between the physical drone and a remote computer, the drone hasa model-based digital representation on the remote machine. The remote drone model is used toSensors 2019,19, 1379; doi:10.3390/s19061379 www.mdpi.com/journal/sensors




Sensors 2019,19, 1379 2 of 13process the control logic based on sensory input such as camera images received from the physicaldrone. The outcomes of the control logic are commands sent back to the drone, which adjusts itsbehavior accordingly. Since control is a real-time task, there is a need for characterizing delaysand their effect on task decisions, in particular when communicating through standard wirelesscommunication networks.Our main focus is on investigating whether a distributed architecture and distributed controlloop to decide on the next actions are feasible and which delays have to be considered. Hereby, ourassumptions are that the drone provides camera images as a primary sensor input and that a humanis integrated in the system to form a team with the drone. To give ﬁrst insights into the problem, wepresent the implementation of a distributed architecture for human-drone teaming applied to a sampletask, as well as quantitative and qualitative experimental results. In detail, we make the followingcontributions in the remainder of the paper:•In Section 2, we present a survey of the state-of-the-art in control and timing-related droneresearch and on human-drone teaming with a focus on human–drone interaction.•In general, drone systems are rather novel robotic systems and require a wide exploration ofpotential application ﬁelds. This is even more the case for human-drone teaming. We contributeby designing and implementing the functions necessary to provide a drone system for bookshelfinventory, which may be used by the research community as a sample test application. In Section 3,we describe this use case.•We introduce a distributed architecture for human-drone teams and model timing in thisarchitecture. Further, we demonstrate the feasibility of such a distributed architecture witha prototype app implementation running on a smartphone, as detailed in Section 4.•We investigate the delay and aspects of the timing behavior in a real testbed with a Parrot Mambodrone equipped with an FPV (ﬁrst person view) camera, operating through Wi-Fi in Section 5.Further, we provide qualitative insights for the speciﬁc use case of bookshelf inventory.2. Related WorkPreparing drones for complex, autonomous tasks that involve humans has been investigated fromvarious perspectives in the literature. Besides providing tools for deployment and remote testing, suchas the deployment tool provided for drone swarms in [4] and learning-based testing of spatio-temporalproperties in [5], understanding the real-time behavior of a drone system is crucial for operation.Hereby, the control loop and the timing characteristics of autopilots are most demanding. Anotherchallenge is to deﬁne appropriate interfaces for human–drone interaction.2.1. Control and TimingControlling the ﬂight behavior of modern micro-drones is supported by autonomously-operatingpilots such as the Ardupilot, a widely-used autopilot (http://ardupilot.org/). Typically, the navigationpath is determined remotely, e.g., by setting GPS waypoints in outdoor scenarios, but speciﬁc motioncontrol is performed locally on the drone using a low-level control loop that determines pitch, roll,and yaw movement. The control loop is usually time-driven, i.e., periodically invoked with a constantrate. An event-based, reactive approach is suggested in [6,7]. By adapting the control rate dynamicallybased on environmental needs such as wind gusts and pressure gradients, a more efﬁcient schemecan be derived that saves energy, leading to an extension of the ﬂight-time. Besides, more accuratedrone motion is achieved. In particular for indoor drones, vision is used for navigation, obstacleidentiﬁcation, and obstacle avoidance. In [8], it is shown that vision-based navigation works well evenfor very small drones termed pocket-drones that weigh only 40 g. In contrast to these local controlapproaches, our architecture allows distributing parts of the control loop between the drone and aremote computer.



Sensors 2019,19, 1379 3 of 13Modeling the control loop allows understanding the drone’s behavior, to proof characteristics,and to perform exhaustive tests. A particular aspect to model is the timing behavior. As in any othercyber-physical system, the time between sensing, processing, and actuation has to remain below aconstraining bound. Concerning drones, information at least from inertial sensors and distance sensorshas to be continuously evaluated to control the ﬂight behavior of the aircraft by activating motorsand rotors. To analyze the end-to-end time in the control loop, a pipe model is suggested in [9]. Themodel allows for analyzing the reaction time, as well as the freshness of events, thus assuring boundedoperation at an early stage of development. The approach is validated by a port of the Cleanﬂightﬂight controller ﬁrmware to Quest (Intel Aero board). In [10], the Ardupilot is modeled with the aim toinvestigate timing behavior. With the model, it can be shown that the autopilot does not always meetthe timing requirements, which is a severe problem for fail-safe operation. These erroneous executionsare then also shown in real tests. Our work conﬁrms the importance of accurate and bounded timing,as well as unexpected delays in the autopilot of the Mambo drone.An approach similar to ours is taken by the work presented in [11]. A representative of a drone(or a swarm of drones) is leveraged to control the actual drone in the ﬁeld and to get feedback fromthis ﬁeld drone. This drone is termed the shadow drone. Besides real-time operation, the shadowdrone can also be used during implementation and testing. Latency and disruption are mentionedas major challenges in [11]. In the paper, prototype implementations for the Parrot Rolling Spiderand AR Drone 2.0 are described, yet an evaluation is missing. In our work, we consider a model asa representation of the physical drone used at the remote site, instead of operating a shadow drone.Furthermore, we present the results of an experimental study of communication delays that occurbetween the physical drone and the control unit running on an external computer.To leverage the processing power of high-performance computers, drone and cloud integration isthe natural next step. Related work has shown that integrating with a cloud can indeed provide beneﬁtsto autonomous vehicles. In [12], remote resources such as terrestrial antennas and cloud infrastructuresare added to a local unmanned vehicle team (here, underwater vehicles). Computationally-extensivetasks are executed in the cloud. Simulations demonstrate that including cloud resources allows forperformance improvements. Yet, this architecture considers a loosely-coupled distributed systemwhere real-time behavior is not an issue. Differently, the cloud is used to support vision-basednavigation in [13]. This real-time task makes use of cloud-based recognition of objects in order to avoidcollisions. The latencies of image frame delivery are in the range of 10 s of ms to about 1 s. Theselatencies are conﬁrmed by our study, which further evaluates the command response time.2.2. Human–Drone InteractionIn addition to the autopilot control loop, our work targets the integration of humans and dronesthrough suitable interfaces. This includes questions about how drones perceive humans and humaninteractions, as well as how humans perceive drones. Drones may use cameras to interact with humans,e.g., to avoid collisions with humans by applying obstacle avoidance methods and to react to humangestures. Alternatively, drones may use messaging to receive user input through an arbitrary devicesuch as a remote control device, smartphone, smart watch, etc. In turn, drones may signal theirstatus to the human. In [14], drone gestures are used to signal acknowledgment. The basic gesturesare waggle, nod, and changing orientation. The drones used for experimentation are the Air-RobotAR-100B, Parrot AR 2, and Parrot Bebop 2. Based on user studies, the work found that distances ofabout 2 m are best to perform drone gestures and that orientation towards the human attracts humanattention very well. A simple way of interaction is to just attract the user to follow a drone, which evenworks for visually-impaired humans following the sound emitted by a guiding drone [15]. Our aim inthis work extends the basic interaction options by adding the collaboration aspect of human-droneteams. This means the drone and the human should work together to fulﬁll a given task. The humanhas to be aware of the solving degree of the task and requires a means to help the drone to proceed.



Sensors 2019,19, 1379 4 of 133. Sample Task: Inventory of a BookshelfTo exemplify a joint human-drone team effort, we select the task of bookshelf inventory.The drone’s task is to team-up with a human in order to survey the available books in an institution,such as a public library. Integrating drones in such a task may allow speeding up inventories andinvolving humans preferably only in non-standard, difﬁcult, and unclear situations. Furthermore, asdrones can move in three dimensions, they are well suited to perform inventories even at high altitudes.In detail, bookshelf inventory requires (i) counting the number of books available and (ii) detectingthe book title, and potentially other information about the book. One option to simplify the task isto label the books with barcodes, QR-codes, or RFID (radio frequency identiﬁcation) tags. Anotheroption is to use image recognition to identify books and text recognition to derive the title, author, etc.The latter is advantageous as it does not require any particular tags on the books. Thus, we choosevision as the main input channel for performing the task and assume that the drone is equipped withan on-board camera. A sample library bookshelf of our university and our experimentation droneParrot Mambo (https://www.parrot.com/eu/drones/parrot-mambo-fpv/) are depicted in Figure 1.(a) (b)Figure 1.Bookshelf inventory components: (a) university library bookshelves and (b) Parrot Mamboequipped with a FPV (ﬁrst person view) camera.The drone operates indoors and mostly autonomously, i.e., it has to position itself close to thebookshelf at a starting position to perform the scanning task thereafter. As a starting point, the upperleft corner of the shelf is searched for by ﬁnding the most left book in the top layer. Scanning isimplemented as a traditional search behavior, i.e., one layer of the whole shelf is scanned after theother from the starting position following a zigzag movement pattern. The ﬂight direction is alternatedbetween ﬂying from left to right and right to left. A major challenge is to move accurately alongthe optimal trajectory. The trajectory is calculated to allow the best placement for book recognition.Whenever a new image is provided by the drone’s camera, image recognition is invoked.3.1. Image RecognitionImage recognition is aimed at identifying books, i.e., to ﬁnd rectangles by evaluating the edgecurvature of a contour and the number of edges (i.e., the angle has to be about 90◦and four edges needto be detected). Furthermore, the ratio between the book’s widthWand heightH(both in pixels) andthe book surfaceA(in pixels) are evaluated. Finally, to differentiate between single books and a fullshelf, a minimum amount of nbooks with a similar height of Hpixels is required.To recognize books in real libraries, books with different shapes have to be identiﬁed. Thus, apre-phase has to be considered to classify and learn the different types of books available in one library.In our implementation of the bookshelf inventory, we make a few simplifying assumptions. First, weassume that the books are of similar size and that the height lies in the interval of[H−100, H+100]pixels. Further, in our conﬁguration, the following shape conditions have to be fulﬁlled:H≥3×Wpixels,A≥1500 pixels, andn≥5 (this conﬁguration has been experimentally derived and will besubject to change in other settings).



Sensors 2019,19, 1379 5 of 133.2. Autonomous NavigationThe drone should perform autonomous, vision-based navigation in order to solve the inventorytask. The drone’s state machine is designed to solve the task by splitting the drone operation into thefollowing states:•Take-off: This is the initial state of the drone when it starts its ﬂight.•Find start position: After taking off, the drone needs to identify the top-left book by combiningimage recognition to detect books and re-positioning to get the best position for image recognition.In detail, the drone changes the distance to the bookshelf to identify the starting position andthen navigates to this position until the top-left book is in the center of the view. Then, the dronechanges to the scanning state.•Scan: Depending on the layer, the drone scans books from left to right or right to left. The altitudeis lowered when one layer is totally scanned.•Finish: When no more shelf layers are left, the drone lands.The drone uses various movement behaviors to assure that a stable position can be maintained asdemanded by the states. Naturally, the outcome of the inventory task heavily depends on the methodsused for image detection. We will detail the implementation of navigation and image recognition inSection 4.3.4. Architecture Description and ImplementationAny drone application requires control of ﬂight behavior in order to support the speciﬁctask of the drone. Traditionally, autonomous ﬂight behavior is implemented on the drone inorder to avoid delays and to access the auto-pilot directly. Yet, this architecture limits thecomputation capabilities; thus, we follow a novel direction in drone application design by proposing adistributed architecture.4.1. Distributed ArchitectureWith a distributed design, we aim at overcoming the limitations of small-footprint embeddedprocessors on drones. This comes with the advantage of speeding up algorithms that require substantialresources such as machine learning and image recognition. Our ﬁrst design decision is to distribute thedrone logic between the drone and a remote computer with more advanced computation capabilities,keeping only low-level piloting at the drone. A second design decision originates from our aim tosupport human-drone teams; thus, we require a mobile computer or interface for including the humanin the team. Finally, the drone’s task is to support the remote logic with sensor-based input data, whichhave to be generated by the drone.Figure 2visualizes a concrete case of such a distributed architecture, capable of supporting theuse case bookshelf inventory described in Section 3. The drone captures and sends video data usingthe on-board FPV (ﬁrst person view) camera, performs ﬂight maneuvers, and sends status informationabout the current directional speed. A smartphone app receives the drone’s information, evaluates theimage and status data of the drone, and sends back ﬂight commands. In addition, the smartphoneapp provides voice recognition and a touch interface to allow the human team-mate to issue ﬂightcommands and displays the progress of the scanning task. The communication technology is Wi-Fidue to the large bandwidth required for video transmission; standard TCP/UDP messages are used tocommunicate with the drone. The drone provides the Wi-Fi network and acts as an access point andDHCP server. The smartphone connects to the Wi-Fi network as a client device. Image recognition isperformed on the smartphone (note that the concept of the distributed architecture is not limited tosmartphones, camera input, or to the sample use case).


Load moreCitations (0)References (16)This research hasn't been cited in any other publications.Fundamental concepts of reactive control for autonomous dronesArticleSep 2018COMMUN ACMLuca MottolaKamin WhitehouseAutonomous drones represent a new breed of mobile computing system. Compared to smartphones and connected cars that only opportunistically sense or communicate, drones allow motion control to become part of the application logic. The efficiency of their movements is largely dictated by the low-level control enabling their autonomous operation based on high-level inputs. Existing implementations of such low-level control operate in a time-triggered fashion. In contrast, we conceive a notion of reactive control that allows drones to execute the low-level control logic only upon recognizing the need to, based on the influence of the environment onto the drone operation. As a result, reactive control can dynamically adapt the control rate. This brings fundamental benefits, including more accurate motion control, extended lifetime, and better quality of service in end-user applications. Based on 260+ hours of real-world experiments using three aerial drones, three different control logic, and three hardware platforms, we demonstrate, for example, up to 41% improvements in motion accuracy and up to 22% improvements in flight time.ViewShow abstractLearning-based testing for autonomous systems using spatial and temporal requirementsConference PaperFull-text availableSep 2018 Hojat Khosrowjerdi Karl MeinkeCooperating cyber-physical systems-of-systems (CO-CPS) such as vehicle platoons, robot teams or drone swarms usually have strict safety requirements on both spatial and temporal behavior. Learning-based testing is a combination of machine learning and model checking that has been successfully used for black-box requirements testing of cyber-physical systems-of-systems. We present an overview of research in progress to apply learning-based testing to evaluate spatio-temporal requirements on autonomous systems-of-systems through modeling and simulation.ViewShow abstractA Framework for Programming a Swarm of UAVsConference PaperJun 2018 Dimitris DedousisVana KalogerakiIn recent years, sensing systems in urban environments are being replaced by Unmanned Aerial Vehicles (UAVs). UAVs, also known as drones, have shown great potential in executing different kinds of sensing missions, such as search and rescue, object tracking, inspection, etc. The UAVs' sensing capabilities and their agile mobility can replace existing complex solutions for such missions. However, coordinating a swarm of drones for mission accomplishment is not a trivial task. Existing works in the literature focus solely on managing the swarm and do not provide options for automating entire missions. In this paper, we present PaROS (PROgramming Swarm), a novel framework for programming a swarm of UAVs. PaROS provides a set of programming primitives for orchestrating a swarm of drones along with automating certain types of missions. These primitives, referred as abstract swarms, control every drone in the swarm, hiding the complexity of low level details from a programmer such as assigning flight plans, task partitioning, failure recovery and area division. Our experimental evaluation proves that our approach is stable, time-efficient and practical.ViewShow abstractKnowing You, Seeing Me: Investigating User Preferences in Drone-Human AcknowledgementConference PaperFull-text availableApr 2018 Walther JensenSimon Hansen Hendrik KnocheIn the past, human proxemics research has poorly predicted human robot interaction distances. This paper presents three studies on drone gestures to acknowledge human presence and clarify suitable acknowledging distances. We evaluated four drone gestures based on non-verbal human greetings. The gestures included orienting towards the counterpart and salutation gestures. We tested these individually and in combination to create a feeling of acknowledgement in people. Our users preferred being acknowledged from two meters away but gestures were also effective from four meters. Rotating the drone towards the user elicited a higher degree of acknowledgement than without. We conclude with a set design guidelines for drone gestures.ViewShow abstractEnd-to-end Analysis and Design of a Drone Flight ControllerArticleFull-text availableFeb 2018IEEE T COMPUT AID D Zhuoqun Cheng Richard West Craig EinsteinTiming guarantees are crucial to cyber-physical applications that must bound the end-to-end delay between sensing, processing and actuation. For example, in a flight controller for a multirotor drone, the data from a gyro or inertial sensor must be gathered and processed to determine the attitude of the aircraft. Sensor data fusion is followed by control decisions that adjust the flight of a drone by altering motor speeds. If the processing pipeline between sensor input and actuation is not bounded, the drone will lose control and possibly fail to maintain flight. Motivated by the implementation of a multithreaded drone flight controller on the Quest RTOS, we develop a composable pipe model based on the system's task, scheduling and communication abstractions. This pipe model is used to analyze two semantics of end-to-end time: reaction time and freshness time. We also argue that end-to-end timing properties should be factored in at the early stage of application design. Thus, we provide a mathematical framework to derive feasible task periods that satisfy both a given set of end-to-end timing constraints and the schedulability requirement. We demonstrate the applicability of our design approach by using it to port the Cleanflight flight controller firmware to Quest on the Intel Aero board. Experiments show that Cleanflight ported to Quest is able to achieve end-to-end latencies within the predicted time bounds derived by analysis.ViewShow abstractControl of a Remote Swarm of Drones/Robots Through a Local (Possibly Model) Swarm: Qualitative and Quantitative IssuesConference PaperNov 2017 Serge Chaumette Frederic GuinandDrones (aerial, terrestrial, marine, underwater, etc.) are more and more widely used in both civilian and military scenario. Still, they remain complex systems for which training, operation preparation and execution of effective operations require adapted tools and support. In this paper we propose such a tool, that we call Thunderbird, based on a shadow drone that is used to control and to get feedback form a drone on an effective field of operation. In this position paper we detail a number of issues that we have identified in the design of such a tool and we describe additional problems that arise when considering not only a single drone but a swarm of possibly heterogeneous drones. We also suggest some possible ways to cope with the identified issues. We eventually present a first prototype/proof of concept that we have developed.ViewShow abstractTowards Autonomous Navigation of Multiple Pocket-Drones in Real-World EnvironmentsConference PaperFull-text availableSep 2017 Kimberly Mcguire Mario Coppola Christophe De Wagter Guido de CroonPocket-drones are inherently safe for flight near humans, and their small size allows maneuvering through narrow indoor environments. However, achieving autonomous flight of pocket-drones is challenging because of strict on-board hardware limitations. Further challenges arise when multiple pocket-drones operate as a team and need to coordinate their movements. This paper presents a setup that can achieve autonomous flight in an indoor environment with avoidance of both static obstacles and other pocket-drones. The pocket-drones use only on-board sensing and processing implemented on a STM32F4 microprocessor (168M Hz). Experiments were conducted with two 40g pocket-drones flying autonomously in a real-world office while avoiding walls, obstacles, and each-other.ViewShow abstractDroneNavigator: Using Drones for Navigating Visually Impaired PersonsConference PaperFull-text availableOct 2015Mauro Avila Markus Funk Niels HenzeEven after decades of research about navigation support for visually impaired people, moving independently still remains a major challenge. Previous work in HCI explored a large number of navigation aids, including auditory and tactile guidance systems. In this poster we propose a novel approach to guide visually impaired people. We use small lightweight drones that can be perceived through the distinct sound and the airflow they naturally produce. We describe the interaction concept we envision, first insights from proof-of-concept tests with a visually impaired participant, and provide an overview of potential application scenarios.ViewShow abstractModel-based Real-time Testing of Drone AutopilotsConference PaperJun 2016Andrea PatelliLuca MottolaKey to the operation of robot drones is the autopilot software that realizes the low-level control. The correctness of autopilot implementations is currently mainly verified based on simulations. These may overlook the timing aspects of control loop executions, which are however fundamental to dependable operation. We report on our experience in applying model-based real-time testing to Ardupilot, a widely adopted autopilot. We describe our approach at deriving a model of Ardupilot's core functionality and at reducing the model to enable practical testing. Our work reveals that Ardupilot may fail in meeting the time constraints associated to critical functionality, such as enabling fail-safe operation. Through controlled experiments, we demonstrate the real-world occurrence of such erroneous executions.ViewShow abstractReactive Control of Autonomous DronesConference PaperJun 2016Endri BreguNicola CasamassimaDaniel CantoniKamin WhitehouseAerial drones, ground robots, and aquatic rovers enable mobile applications that no other technology can realize with comparable flexibility and costs. In existing platforms, the low-level control enabling a drone's autonomous movement is currently realized in a time-triggered fashion, which simplifies implementations. In contrast, we conceive a notion of reactive control that supersedes the time-triggered approach by leveraging the characteristics of existing control logic and of the hardware it runs on. Using reactive control, control decisions are taken only upon recognizing the need to, based on observed changes in the navigation sensors. As a result, the rate of execution dynamically adapts to the circumstances. Compared to time-triggered control, this allows us to: i) attain more timely control decisions, ii) improve hardware utilization, iii) lessen the need to over-provision control rates. Based on 260+ hours of real-world experiments using three aerial drones, three different control logic, and three hardware platforms, we demonstrate, for example, up to 41% improvements in control accuracy and up to 22% improvements in flight time.ViewShow abstractShow moreJoin ResearchGate to find the people and research you need to help your work.15+ million members118+ million publications700k+ research projectsJoin for freeRecommended publicationsDiscover more publications, questions and projects in TimingConference PaperPaving the Way for Civilian Fleets of Unmanned Aerial Vehicles: A Networking PerspectiveDecember 2015 Karin Anna HummelPast experiment-driven research has shown that unmanned aerial vehicle (UAV) networks are challenged by system characteristics of the UAVs. The frame and flight dynamics of micro UAVs may cause wireless signal obstruction, data rate adaptation has not been optimized for the aerial use, and brute-force transmission when coming in range of another UAV is not efficient. Further, in multi-hop UAV ... [Show full abstract] networks routing algorithms are affected by mobility-related disconnections. Promising transmission and data forwarding solutions exploit delay-tolerant data dissemination as well as location and movement context of the UAVs. Moreover, UAVs are flying robots that can deliberately move to improve communication quality. The paper provides a summary of major findings in past experiment-driven research extended by a discussion of major system and scaling challenges in UAV fleet communication.Read moreArticleMobile Systems Research with DronesApril 2017Luca MottolaKamin WhitehouseRobot vehicle platforms, often called "drones," offer exciting new opportunities for mobile computing. While many systems respond to device mobility (such as smartphones), drones allow computer systems to actively control device location, allowing them to interact with the physical world in new ways and with newfound scale, efficiency, or precision. Read moreConference PaperAnalysis of Harmful Interference to and from Aerial IEEE 802.11 Systems.May 2015Bertold Van den Bergh Tom Vermeulen S. PollinCivil Unmanned Aerial Vehicles (UAVs) enable a manifold of exciting new services. UAVs are about to be part of our everyday lives. The range of applications is very broad, ranging from swarms of UAVs that can be used for 3D modeling and surveillance of large areas, to search and rescues and fire fighter missions. An important cornerstone for the development of civil UAVs is communication ... [Show full abstract] technology. To ensure lightweight and cheap UAV technology, it is necessary to use existing commodity communication technology and chip-sets for UAV communications. Yet, the impact of the novel aerial and high altitude propagation context on those technologies needs to be understood. In this paper, we analyze the performance of IEEE 802.11 communication, both experimentally and by means of simulation. It is shown that the aerial context results in very different communication performance for an 802.11 network compared to the typical terrestrial context. Decreased shadowing gives a larger number of networks that can be seen, resulting in higher interference and packet loss due to collisions.Read moreConference Paper3D Scanner Based on an Autonomous Wi-Fi Unmanned Mini QuadcopterMarch 2017 · Advances in Intelligent Systems and ComputingDaniela FloresDiego Marcillo João da Silva PereiraThis research is about how to create 3D model objects using an autonomous unmanned aerial vehicle (UAV). Our system is composed by a small drone that has Wi-Fi, a mini camera, and it can be controlled remotely by a smartphone. A smart network of Wi-Fi access points with directional and remote controlled antennas are utilized to determine the location of the mini drone, using the Angle of Arrival ... [Show full abstract] (AoA) method. By estimating its position, our mini UAV is able to follow automatically a planned route around an object. During its trajectory, our UAV takes automatically some photos that can be used to generate a 3D model using appropriate software.Read moreDiscover moreLast Updated: 08 Apr 2019Download citationWhat type of file do you want? RIS BibTeX Plain TextWhat do you want to download? Citation only Citation and abstractDownloadInterested in research on Timing?Join ResearchGate to discover and stay up-to-date with the latest research from leading experts in Timing and many other scientific topics.Join for freeorDiscover by subject areaRecruit researchersJoin for freeLoginEmail Tip: Most researchers use their institutional email address as their ResearchGate loginPasswordForgot password? Keep me logged inLog inorContinue with LinkedInContinue with GoogleWelcome back! Please log in.Email · HintTip: Most researchers use their institutional email address as their ResearchGate loginPasswordForgot password? Keep me logged inLog inorContinue with LinkedInContinue with GoogleNo account? Sign upAboutNewsCompanyCareersSupportHelp centerFAQBusiness solutionsRecruitingAdvertising© ResearchGate 2019. All rights reserved.ImprintTermsPrivacy