url: https://www.researchgate.net/publication/287207354_Development_of_a_Control_and_Vision_Interface_for_an_ARDrone








    






 











































(PDF) Development of a Control and Vision Interface for an AR.Drone


We use cookies to make interactions with our website easy and meaningful, to better understand the use of our services, and to tailor advertising. For further information, including about cookie settings, please read our Cookie Policy . By continuing to use this site, you consent to the use of cookies.Got itWe value your privacyWe use cookies to offer you a better experience, personalize content, tailor advertising, provide social media features, and better understand the use of our services.To learn more or modify/prevent the use of cookies, see our Cookie Policy and Privacy Policy.Accept CookiestopSee all ›1 CitationsSee all ›18 ReferencesSee all ›9 FiguresDownload citationShare  Facebook Twitter LinkedIn RedditDownload full-text PDFDevelopment of a Control and Vision Interface for an AR.DroneConference Paper (PDF Available) in MATEC Web of Conferences 56 · April 2016 with 826 ReadsDOI: 10.1051/matecconf/20165607002  Conference: 8th International Conference on Computer and Automation EngineeringCite this publication Prasad Cheema3.85The University of Sydney Simon LuoAbstractThe AR.Drone is a remote controlled quadcopter which is low cost, and readily available for consumers. Therefore it represents a simple test-bed on which control and vision research may be conducted. However, interfacing with the AR.Drone can be a challenge for new researchers as the AR.Drone's application programming interface (API) is built on low-level, bit-wise, C instructions. Therefore, this paper will demonstrate the use of an additional layer of abstraction on the AR.Drone’s API via the Robot Operating System (ROS). Using ROS, the construction of a high-level graphical user interface (GUI) will be demonstrated, with the explicit aim of assisting new researchers in developing simple control and vision algorithms to interface with the AR.Drone. The GUI, formally known as the Control and Vision Interface (CVI) is currently used to research and develop computer vision, simultaneous localisation and mapping (SLAM), and path planning algorithms by a number of postgraduate and undergraduate students at the school of Aeronautical, Mechanical, and Mechatronics Engineering (AMME) in The University of Sydney.Discover the world's research15+ million members118+ million publications700k+ research projectsJoin for free Figures - uploaded by Prasad CheemaAuthor contentAll content in this area was uploaded by Prasad CheemaContent may be subject to copyright. An image of the AR.Drone demonstrating the EPP covering and motor positioning. 13 approximately 10 minutes. The internal processor of the Drone is a 468 MHz ARM9-processor and comes included with 128 Mb of RAM memory. The processor runs a minimalist version of Linux on board. Moreover an integrated 802.11g wireless card provides network connectivity with an external computing device via WiFi. 14 In regards to sensor technology and actuators, the AR.Drone 2.0 is equipped with a 3-axis accelerometer (BMA 150), and a 2-axis roll and pitch gyroscope (IDG-500) with an additional single axis gyroscope (Epson Toyocom XV-3500CB), which are of MEMS type. An ultra sound altimeter is also included for height calculations. 14 Moreover the Drone has two cameras (forward and down facing). The forward facing camera covers a field of view 92 • , has a resolution 640×360. According to literature it generally contains significant radial distortion that needs to be corrected through via calibration techniques. The second camera points down, covers a field of view of 64 • and records at 60 fps. At any point in time only one video stream can be used to record video. 9 For movement the Drone relies upon differential torque and thrust. By spinning each rotor with a different amount of angular velocity, the three basic motions of flight mechanics, yaw, pitch and roll can be achieved. 14 The Software Development Kit (SDK) that is used within AR.Drone is a low level C-based interface. Ultimately, it allows third party developers to create new software for the Drone through the use of libraries which aid in vision processing and controllability of the Drone. However, it must be noted that the SDK does not support the writing of ones own embedded software. That is to say, it offers no direct access to manipulate low level drone hardware. Figure 2 shows a layered architecture overview of the AR.Drone SDK. The yellow coloured portions of the diagram are those parts which the user has direct control over (for example the programmer will have the ability to read in data streams, as well as send data streams). Note that the AR.Drone Control Engine exists only specifically for the iOS devices. This allows the AR.Drone's Application Programming Interface (API) to be portable for such devices. The SDK communicates through to an external PC via UDP and TCP for different purposes. These communication ports are outlined in Table 1.…  . Communication port streams used by the AR.Drone.…  …  +5An image of the AR.Drone demonstrating the EPP covering and motor positioning .… Content uploaded by Prasad CheemaAuthor contentAll content in this area was uploaded by Prasad Cheema on Dec 18, 2015 Content may be subject to copyright.Download full-text PDF 





Development of a Control and Vision Interface for anAR.DronePrasad Cheema ∗and Simon Luo∗The University of Sydney, Sydney, New South Wales, 2006, AustraliaThe AR.Drone is a remote controlled quadcopter which is low cost, and readily avail-able for consumers. Therefore it represents a simple test-bed on which control and visionresearch may be conducted. However, interfacing with the AR.Drone can be a challengefor new researchers as the AR.Drone’s application programming interface (API) is builton low-level, bit-wise, C instructions. Therefore, this paper will demonstrate the use of anadditional layer of abstraction on the AR.Drone’s API via the Robot Operating System(ROS). Using ROS, the construction of a high-level graphical user interface (GUI) willbe demonstrated, with the explicit aim of assisting new researchers in developing simplecontrol and vision algorithms to interface with the AR.Drone. The GUI, formally knownas the Control and Vision Interface (CVI) is currently used to research and develop com-puter vision, simultaneous localisation and mapping (SLAM) and path planning algorithmsby a number of postgraduate and undergraduate students at the school of Aeronautical,Mechanical, and Mechatronics Engineering (AMME) in The University of Sydney.I. IntroductionQuadcopters have recently garnered a lot of interest in the aerospace community. This is predominantlydue to the simple nature of their force calculations (only lift and thrust required), and the inexpensivenature of their construction which allows for the wide spread availability for this platform.1,2This hasultimately culminated in the production of the AR.Drone - a cheap quadcopter freely available for purchaseby the public. As a result, the AR.Drone has received much interest to act as a research platform for roboticresearch in the ﬁelds of control and vision processing.Traditional drone research has involved using computer vision for autonomous ﬂight in an unmapped envi-ronment, via simultaneous localization and mapping (SLAM).3,4,5Many researchers have traditionally reliedon expensive aerial vehicles with computationally intensive sensors to accomplish SLAM.6However, recentwork has attempted to visually map the environment using low cost drones such as the Parot AR.Drone byusing eﬃcient algorithms.7Similar techniques have been used for quadcopter technology by other institutes,such as those described by researchers in California Institute of Technology.8Another pertinent examplemay be seen by the Computer Vision Group at Tehcnische Universitat Munchen (TUM). This is most notedin their use of the AR.Drone as a platform for control algorithms and vision navigation research, as wellas a tool for educational and outreach purposes. Their research mainly pertains to the use of monocularRGB-D cameras to implement SLAM for the purpose of scale aware autonomous navigation and dense visualodometry.9In their research the vision group at TUM have employed a PID controller successfully on theParrot Drone. The signals for the PID are sent to the Drone at 100 Hz. Given a target position and thepredicted quadcopter state a separate PID controller is applied to all four ‘relevant’ states of the Drone(x, y, z, ψ) to obtain optimal control gains.9Control techniques similar to those demonstrated by TUMhave been shown in Altu & Taylor,10 as well as Bristeau et al.11 Furthermore, as researchers at CornellUniversity demonstrate, a particular class of drones known as micro aerial vehicles (MAV) have been ableto be autonomously moved in an indoor environment using algorithms based on feature detection. That is,canny edge detection has been used in conjunction with a probabilistic Hough transform to ﬁnd the lines ofa staircase. The MAV also uses the vanishing point (the furthest point until the image becomes a point) tonavigate around a particular staircase.12∗Undergraduate Student, School of Aerospace, Mechanical and Mechatronic Engineering, University of Sydney, NSW,2006,AIAA Student Member.1of9American Institute of Aeronautics and Astronautics




Thus there is a variety of research performed on MAV-type drones, but with the emergence of theAR.Drone a cheap and readily available research alternative to the traditional, more expensive drone modelshas been developed. It is the purpose of this paper to try and develop a control and vision interface (CVI)in order to assist new researchers in control and navigation ﬁeld, for interfacing with the AR.Drone.II. Overview of the AR.DroneThe AR.Drone 2.0 is a remote controlled consumer quadcopter developed by Parrot. The body is madeof a carbon ﬁbre tube structure and high resistance plastic. A protection hull is made of ExpandedPolypropylene (EPP) foam which is durable, light in weight, and recyclable. The propellers are powered byfour brushless motors (35 000 rpm, 15W power), and on-board energy is provided by a Lithium polymerbattery with a capacity of 1000 mAh. This allows a ight time ofFigure 1. An image of the AR.Drone demonstratingthe EPP covering and motor positioning.13approximately 10 minutes. The internal pro-cessor of the Drone is a 468 MHz ARM9-processorand comes included with 128 Mb of RAM memory.The processor runs a minimalist version of Linuxon board. Moreover an integrated 802.11g wirelesscard provides network connectivity with an externalcomputing device via WiFi.14In regards to sensor technology and actuators,the AR.Drone 2.0 is equipped with a 3-axis ac-celerometer (BMA 150), and a 2-axis roll and pitchgyroscope (IDG-500) with an additional single axisgyroscope (Epson Toyocom XV-3500CB), which areof MEMS type. An ultra sound altimeter is alsoincluded for height calculations.14 Moreover theDrone has two cameras (forward and down facing).The forward facing camera covers a ﬁeld of view 92◦,has a resolution 640×360. According to literature it generally contains signiﬁcant radial distortion that needsto be corrected through via calibration techniques. The second camera points down, covers a ﬁeld of viewof 64◦and records at 60 fps. At any point in time only one video stream can be used to record video.9For movement the Drone relies upon diﬀerential torque and thrust. By spinning each rotor with a diﬀerentamount of angular velocity, the three basic motions of ﬂight mechanics, yaw, pitch and roll can be achieved.14The Software Development Kit (SDK) that is used within AR.Drone is a low level C- based interface.Ultimately, it allows third party developers to create new software for the Drone through the use of librarieswhich aid in vision processing and controllability of the Drone. However, it must be noted that the SDKdoes not support the writing of ones own embedded software. That is to say, it oﬀers no direct access tomanipulate low level drone hardware. Figure 2shows a layered architecture overview of the AR.Drone SDK.The yellow coloured portions of the diagram are those parts which the user has direct control over (forexample the programmer will have the ability to read in data streams, as well as send data streams). Notethat the AR.Drone Control Engine exists only speciﬁcally for the iOS devices. This allows the AR.Drone’sApplication Programming Interface (API) to be portable for such devices.The SDK communicates through to an external PC via UDP and TCP for diﬀerent purposes. Thesecommunication ports are outlined in Table 1.Table 1. Communication port streams used by the AR.Drone.Port Number Communication Type Function5554 UDP Navdata stream sent from Drone to client5555 TCP/IP Image stream sent from Drone to client5556 UDP Controlling and conﬁguring the Drone using commands5559 TCP/IP Transferring of critical system informationAlthough very complete, the main drawback to the use of the SDK system is its low level nature. Insteadof sending direct commands such as “move forward with a particular speed”, it is necessary to send a complete2of9American Institute of Aeronautics and Astronautics



Figure 2. The layered architecture of the AR.Drone 2.0 SDK .bit sequence, with sequences using an unsigned 32 bit representation. This means that sending a complexcommand sequence to the drone can become very cumbersome. Moreover, working with the drone on acomplex level (such in the case of SLAM) will involve an incredibly large amount of software developmentfor even the simplest of SLAM algorithms to work. This is deﬁnitely counter-productive for the purposeof engineering research. If students are required to perform and learn large amounts of low level softwareprogramming it will slow down (and perhaps even avoid interest in) the use of drones for research projects.Therefore it was decided to implement the communication to the AR.Drone via the Robot OperatingSystem (ROS) ARDrone autonomy driver .15 This driver provides a higher level abstraction to the SDK viathe ROS system. It is still relatively low level in that it deals with Drone via C++, but the abstractionit provides allows the AR.Drone to move forward through a single velocity command rather than buildingup multiple bit streams. Of course understanding the manner in which ROS works to communicate withthe drone does have an associated learning curve, however ROS is such a well-documented and widely usedenvironment for robotic research development that gaining familiarly for a ROS environment will be anineﬀably easier task compared to learning to work with the even lower level (and poorly documented) SDK.A look into the ROS environment is explored in the following section.III. Development of the Control and Vision InterfaceThis section of the paper explores the development of the control and vision interface (CVI) as a whole.It will ﬁrst look at the C++ systems use build the CVI (through ROS and OpenCV). From there it shallexplore how these components work together to generate a coherent computer system for researchers.A. Understanding Robot Operating SystemThe Robot Operating System (ROS) is an open source platform used to aid communication with researchrobot. It is a ‘so called’ operating system because it provides similar services to an OS. For example it includesmulti-leveled hardware abstraction, low level device control, interprocess communications and distributedpackage management. In addition to this, it is especially venerated within the robotics community due toits ample tools and libraries of a gamut of algorithms, which allows for multi-layered communication withrobots.16 There are several distinct advantages to the uses of ROS, which are brieﬂy explored in the followingsub-sections.3of9American Institute of Aeronautics and Astronautics



1. Distributed computationROS provides an abstraction system to help simplify distributed computations on robots. Distributedcomputation naturally arises throughout the use of multiple computers within the single robot, or whenmultiple robots attempt to cooperate and coordinate their eﬀorts to achieve some overarching goal. Moreover,it is just simpler to subdivide one’s code into multiple blocks which can then be cooperate together viainterprocess communication (as is used in the development of the CVI’s control and vision GUI).2. Software re-usabilityDue to the appeal of ROS for robotics research a number of path planning, vision detection, and generalizedmapping algorithms have been developed using its libraries. Due to the development of these algorithmsunder a standard and stable ROS environment they have become applicable in a relatively large amountof contexts for robots, without the need to re-implement each algorithm from its low level origins for eachsystem. Not surprisingly, ROS’s Message Passing Interface (MPI) is gradually becoming a standard measuremeasure for a robots interoperability. Therefore once the user overcomes the initial learning curve of ROS(which is rather steep), they will be able to focus more on experimenting on innovative ideas rather thancontinually re-implementing standard algorithms to allow for bold ideas to manifest (in a more lower levelenvironment).3. A single, monolithic libraryAlthough ROS contains some core libraries, more often than not the user will be required to download addi-tional libraries to supplement their current library (or to use external code developed by other universities).The compilation and linkage of these libraries can cause signiﬁcant issue if the user is not well acquaintedwith such aspects of software development. Moreover the linking of several external libraries will generallyexponentially increase compilation time for the programB. Understanding OpenCVIn the construction of the CVI, open computer vision (OpenCV) will be used for image processing, suchas with Canny edge detection and image smoothing. Brieﬂy, OpenCV is an open source computer visionproject supplied online by sourceforge. All libraries and systems of OpenCV are written in C and C++,and projects from OpenCV are able to run under Linux, Windows and the Mac OS X operating systems.Recently, there have been substantial eﬀorts to port the OpenCV libraries to Python, Ruby, and MATLABR, although progression is generally slow, and support is very little. Due to OpenCV’s strong dependency onthe low level languages of C and C++, a strong focus of the OpenCV libraries is on computational eciencyand real time applications. It does this by looking at speciﬁc memory allocations on the processor level, andcan in most cases utilize multi core processing (a strong limitation of MATLAB for real time computer visionis that it can only use 1 core at most, unless and external parallel computing toolbox is installed). The maingoal of OpenCV is to provide a simple-to-use computer vision infrastructure which will help people constructfairly sophisticated computer vision processes rapidly. OpenCV does this very successfully through the useof over 500 functions, which are particularly optimized at the compiler level.17C. Development of Control and Vision InterfaceSpeaking from a high level perspective, the CVI development required a large degree of internal multithreading. This is necessary because both of these applications (the control part - Control GUI, and thevision part - Vision GUI) require multiple inﬁnite loops in order to function properly. For example a standardGUI thread will always require an inﬁnite loop to physically show itself on the screen, and to perform checkson the buttons. The running of ROS would deﬁnitely needs it’s own inﬁnite loop to check the global callback queue deﬁned in the ROS API, and ﬁnally the ‘Video Thread’ and ‘Video Processing Thread’ bothrequire their own threads. This is so that as well as the camera GUI showing, the video stream (and itsprocessed version of itself) will require inﬁnite loops in order to continue to compute image data and showit on screen. Thus the GUIs are in fact highly threaded to internalise computations and render them quickenough for a psuedo-real time environment to be used. A summary of this multi-threaded nature is shownin Figure 3.4of9American Institute of Aeronautics and Astronautics



Figure 3. An image of the AR.Drone demonstrating the EPP covering and motor positioning .                 In addition to multi-threaded code, the two GUIs (and a third process labelled as pseudo real time graphin Figure 3which handles plotting) are able to communicate through Inter Process Communications (IPCs),however IPC is not the ideal option to communicate data between one another. If all the GUIs were internallymulti-threaded into one another, then that would be most eﬃcient. However threading all aspects into asingle GUI would make the coding extremely diﬃcult (lots of inherent multi-threading issues such as contextswitching and race conditions will arise), and therefore diﬃcult to conceptually follow for new researchers inthe ﬁeld. Thus two (separate) processes were selected as the time optimal way to code up these applications.Ultimately, The Control GUI aims to facilitate control-based communication with the AR.Drone. Itachieves this through the use of keyboard control, open access to navigation data, and button options toperform a pseudo real time plot of system states. In addition it provides the user with a link to a second GUIfor video processing. The ﬁnal, coded implementation of the control GUI is diagrammatically representedin Figure 4.In Figure 4, it can be seen the the user has the option to manually control the ﬂight of the AR.Dronethrough simple keyboard controls as well as track the state variables deﬁned in Table 2in pseudo real-time.5of9American Institute of Aeronautics and Astronautics



Figure 4. The ﬁnal implementation of the control GUI in it’s ‘oﬀ ’ state.Table 2. Summary of AR.Drone information and corresponding units extracted from the SDK via ROS.Variable Recorded UnitsTimestamp Second since Unix EpochTime between timestamps NanosecondRotation rates Radians/secondAcceleration Metres/second2Quaternions UnitlessEuler Angles DegreesAltitude CentimetresStream sequence number UnitlessIn addition to the ability to perform and track pseudo real-time control operations on the AR.Drone, alink can be directly set-up to the video steam GUI which, can be initiated by clicking the Start Video Streamoption. The completed state of the video GUI portion of the CVI interface is represented in Figure 5.From Figure 5, it can be seen that the user is given the option to toggle between the front, and downfacing cameras for the video stream. This is consequence of the AR.Drone 2.0 only being able to show onevideo stream at one point in time. Moreover, the GUI enables researchers to toggle between various imageﬁltering techniques in pseudo real-time which is an invaluable experience for learning, as well as for research.In particular, Figure 5shows an implementation of an in-house contour-based feature detection algorithm.Although the CVI shown in Figures 4and 5seem extensive, obviously there are a multitude of otheralgorithms which can be implemented for various research purposes. Therefore, in order to accommodatethe possibility of future research the code has been constructed in a modularised fashion,so that it essentiallyacts as API. Therefore future researchers need only use a few basic functions (the same ones used to createdall the other buttons and features) within which new code can be written and dynamically integrated intothe CVI. A few examples of the positive impacts of the CVI for future drone-based research is outlined inthe following sections.6of9American Institute of Aeronautics and Astronautics



Figure 5. The ﬁnal implementation of the control GUI .IV. Application to Real Time Feature DetectionThe framework for the Vision Node GUI has been adapted such that additional functions can be easilybe added or removed into the video stream. The Vision Node GUI has been extended to include realtime feature detection. This is done by adding feature detection algorithm to the video processing threadas shown in Figure 3. For a demonstration of concept, a multi-stage feature detection algorithm has beenimplemented. This algorithm includes a number of stages, ﬁrst a Gaussian smoothing to remove all thebackground noise. This is followed by a Canny edge detection, which creates a binary image which places apixel at regions with a large gradient change. This is then followed by the Suzuki algorithm18 which is usedto ﬁnd closed contours in the image. These algorithms have been implemented in the CVI framework. Theresults are shown in ﬁgure 6.(a) Raw Image (b) Processed ImageFigure 6. Images from the Vision Node GUI.19It can be seen in ﬁgure 6, that the feature detection algorithm converted the raw image into a binaryimage which consists of only the edges of the objects in the images. The green rectangles is placed on theprocessed image when the Suzuki algorithm detects a closed contour on the image. These closed contours arethen matched matched with the database of features, in this case its just simply the triangle, rectangle and a7of9American Institute of Aeronautics and Astronautics



pentagon. By implementing this multi-stage feature detection algorithm, it can be seen that the frameworkis robust enough to be able to support multi-stage algorithms.This has shown that the Vision Node GUI has developed a pipeline which is highly eﬀective in adding andremoving computer vision algorithms. Even with algorithms which require high computational power suchas Canny edge detection and contour detection algorithms, the framework is set up with eﬀectively, whereeach critical section is isolated in its own thread had allow the software to run smoothly. When running in acomputer with lower computational power, the frame work ensures that the video stream is running in realtime by only retrieving the newest video frame.V. Application to Monocular SLAMFollowing the success of the feature detection, the CVI is extended to incorporate SLAM. SLAM is acomputational problem which requires to develop a map of an unknown environment while keeping trackof its location. The CVI has assisted in the development of SLAM by providing the API for mapping, controland video stream. The mapping and feature detection can be seen in ﬁgure 7. The mapping feature usesthe 3D visualisation tool Rviz provided by ROS.Figure 7. Map and feature detected by using the SLAM algorithm.20The feature detection algorithm is run in parallel with the SLAM algorithm in the vision node. Thefull SLAM algorithm has been implemented in the control node, the features and landmarks detected isdone through the vision node. The eﬀective use of multi-threading has allowed the control of the drone andSLAM algorithm to run simultaneously in the control node. Figure 7, shows that SLAM has been eﬀectivelyimplemented into the CVI framework. The detection of features can also be seen to run in parallel as shownin the top right image. The mapping of these features can be seen in the bottom left corner, where the reddots indicate the features, green line showing the dead reckoning and the blue line indicating the predictionby SLAM. The implementation of SLAM has shown the eﬀective communication between the control andvision node and as well as the highly eﬀective usage of using threads to ensure that all features in the CVIrun in real time.8of9American Institute of Aeronautics and Astronautics



VI. ConclusionTherefore this paper has demonstrated the development and application of a control and vision interface(CVI). Using the AR.Drone 2.0 as a simple, low-cost, readily available test-bed it is possible to performa wide-range of control and vision-based research applications. This paper has related how the use of a highlevel abstraction via object orient C++ code was able to combine OpenCV and ROS into a coherent interfaceto interact with the AR.Drone. In particular applications to real time feature detection and monocular SLAMhave been explored, which demonstrates just a few of the exciting applications to research that the CVI canhandle.AcknowledgmentsThe authors would like to thank undergraduate students Andy Ho, and Trevor Wang, of the Universityof Sydney. They were able to assist in appropriating the CVI for the SLAM and feature detectionapplications explored in this paper. Moreover thanks is given to Associate Professor Peter Gibbens forproviding the facilities and funding to carry out the development of the CVI.References1Saska, M., Krajnik, T., Faigl, J., Vonasek, V., and Preucil, L., “Low cost MAV platform AR-drone in experimentalveriﬁcations of methods for vision based autonomous navigation,” IEEE, 2012, pp. 4808–4809.2Michael, M., The AR Drone LabVIEW Toolkit: A Software Framework for the Control of Low-Cost Quadrotor AerialRobots, Ph.D. thesis, Tufts University, 2012.3Yang, S., Scherer, S. A., and Zell, A., “Visual SLAM for autonomous MAVs with dual cameras,” IEEE, 2014, pp.5227–5232.4Sadat, S. A., Chutskoﬀ, K., Jungic, D., Wawerla, J., and Vaughan, R., “Feature-rich path planning for robust navigationof MAVs with Mono-SLAM,” IEEE, 2014, pp. 3870–3875.5Lee, G. H., Fraundorfer, F., and Pollefeys, M., “MAV visual SLAM with plane constraint,” 2011, pp. 3139–3144.6Karlsson, N., di Bernardo, E., Ostrowski, J., Goncalves, L., Pirjanian, P., and Munich, M. E., “The vSLAM Algorithmfor Robust Localization and Mapping,” Vol. 2005, IEEE, 2005, pp. 24–29.7Dijkshroon, N., Simultaneous localization and mapping with AR.Drone, Ph.D. thesis, Universiteit van Amsterdam, 2012.8Blosch, M., Weiss, S., Scaramuzza, D., and Siegwart, R., “Vision Based MAV Navigation in Unknown and UnstructuredEnvironments,” 2010.9Engel, J., Sturm, J., and Cremers, D., “Scale-aware navigation of a low-cost quadrocopter with a monocular camera,”Robotics and Autonomous Systems, Vol. 62, No. 11, 2014, pp. 1646–1656.10Altug, E. and Taylor, C., “Vision-based pose estimation and control of a model helicopter,” IEEE, 2004, pp. 316–321.11Bristeau, P.-J., Callou, F., Vissiere, D., and Petit, N., “The Navigation and Control technology inside the AR.Dronemicro UAV,” Vol. 18, 2011, pp. 1477–1484.12Bills, C., Chen, J., and Saxena, A., “Autonomous MAV ﬂight in indoor environments using single image perspectivecues,” 2011, pp. 5776–5783.13Parrot Drone, “Parrot Drone Images,” www.parrot.com/media/gallery/ardrone2_hd_indoor.jpg, 2015.14Parrot Drone, “Parrot Drone SDK,” www.msh-tools.com/ardrone/ARDrone_Developer_Guide.pdf, 2014.15Monajjemi, M., “ardrone autonomy Package Summary,” www.wiki.ros.org/ardrone_autonomy, 2015.16O’Kane, J.M., “A Gentle Introduction to ROS,” Tech. rep., University of South Carolina, 2014.17OpenCV, “Documentation,” www.opencv.org/documentation.html, 2015.18Suzuki, S., “Topological structural analysis of digitized binary images by border following,” Computer Vision, Graphics,and Image Processing, Vol. 30, No. 1, 1985, pp. 32–46.19Ho, A., Real Time Feature Detection For Feature Assisted Visual Navigation For The Parrot AR Drone, Ph.D. thesis,University of Sydney, 2015.20Wang, T., Monocular Simultaneous Localisation and Mapping for Autonomous Visual Navigation with the parrot ARDrone, Ph.D. thesis, University of Sydney, 2015.9of9American Institute of Aeronautics and Astronautics


Citations (1)References (18)... A Robotic Operating System (ROS) API connects our interfaces to this control system to update the locations of the user's planned path points as well as launch/land commands. This ROS abstraction approach is similar to the one taken by Cheema et al. [2] in their 2D Control and Vision interface. This platform is used for all three test interfaces described in this paper.  ...Improving Usability, Efficiency, and Safety of UAV Path Planning through a Virtual Reality InterfacePreprintApr 2019Jesse PatersonJiwoong HanTom Cheng Allen YangAs the capability and complexity of UAVs continue to increase, the human-robot interface community has a responsibility to design better ways of specifying the complex 3D flight paths necessary for instructing them. Immersive interfaces, such as those afforded by virtual reality (VR), have several unique traits which may improve the user's ability to perceive and specify 3D information. These traits include stereoscopic depth cues which induce a sense of physical space as well as six degrees of freedom (DoF) natural head-pose and gesture interactions. This work introduces an open-source platform for 3D aerial path planning in VR and compares it to existing UAV piloting interfaces. Our study has found statistically significant improvements in safety and subjective usability over a manual control interface, while achieving a statistically significant efficiency improvement over a 2D touchscreen interface. The results illustrate that immersive interfaces provide a viable alternative to touchscreen interfaces for UAV path planning.ViewShow abstractShow moreFeature-rich path planning for robust navigation of MAVs with Mono-SLAMConference PaperMay 2014Seyed Abbas SadatKyle ChutskoffDamir Jungic Richard T. VaughanWe present a path planning method for MAVs with vision-only MonoSLAM that generates safe paths to a goal according to the information richness of the environment. The planner runs on top of monocular SLAM and uses the available information about structure of the environment and features visibility to find trajectories that maintain visual contact with feature-rich areas. The MAV continuously re-plans as it explores and updates the feature-points in the map. In real-world experiments we show that our system is able to avoid paths that lead into visually-poor sections of the environment by considering the distribution of visual features. If the same system ignores the availability of visually-informative regions in the planning, it is unable to estimate its state accurately and fails to reach its goal.ViewShow abstractThe Navigation and Control Technology Inside the AR.Drone Micro UAVConference PaperAug 2011Pierre-Jean BristeauThis paper exposes the Navigation and Control technology embedded in a recently commercialized micro Unmanned Air Vehicle (UAV), the AR.Drone, which cost and performance are unprecedented among any commercial product for mass markets. The system relies on state-of-the-art indoor navigation systems combining low-cost inertial sensors, computer vision techniques, sonar, and accounting for aerodynamics models.ViewShow abstractLow cost MAV platform AR-drone in experimental verifications of methods for vision based autonomous navigationConference PaperFull-text availableOct 2012Rep U S Martin Saska Tomáš Krajník Jan FaiglLibor PřeučilSeveral navigation tasks utilizing a low-cost Micro Aerial Vehicle (MAV) platform AR-drone are presented in this paper to show how it can be used in an experimental verification of scientific theories and developed methodologies. An important part of this paper is an attached video showing a set of such experiments. The presented methods rely on visual navigation and localization using on-board cameras of the AR-drone employed in the control feedback. The aim of this paper is to demonstrate flight performance of this platform in real world scenarios of mobile robotics.ViewShow abstractScale-aware navigation of a low-cost quadrocopter with a monocular cameraArticleNov 2014ROBOT AUTON SYST Jakob Engel Jürgen Sturm Daniel CremersWe present a complete solution for the visual navigation of a small-scale, low-cost quadrocopter in unknown environments. Our approach relies solely on a monocular camera as the main sensor, and therefore does not need external tracking aids such as GPS or visual markers. Costly computations are carried out on an external laptop that communicates over wireless LAN with the quadrocopter. Our approach consists of three components: a monocular SLAM system, an extended Kalman filter for data fusion, and a PID controller. In this paper, we (1) propose a simple, yet effective method to compensate for large delays in the control loop using an accurate model of the quadrocopter’s flight dynamics, and (2) present a novel, closed-form method to estimate the scale of a monocular SLAM system from additional metric sensors. We extensively evaluated our system in terms of pose estimation accuracy, flight accuracy, and flight agility using an external motion capture system. Furthermore, we compared the convergence and accuracy of our scale estimation method for an ultrasound altimeter and an air pressure sensor with filtering-based approaches. The complete system is available as open-source in ROS. This software can be used directly with a low-cost, off-the-shelf Parrot AR.Drone quadrocopter, and hence serves as an ideal basis for follow-up research projects.ViewShow abstractVisual SLAM for autonomous MAVs with dual camerasConference PaperFull-text availableJun 2014 Shaowu YangSebastian A. Scherer Andreas ZellThis paper extends a monocular visual simultaneous localization and mapping (SLAM) system to utilize two cameras with non-overlap in their respective field of views (FOVs). We achieve using it to enable autonomous navigation of a micro aerial vehicle (MAV) in unknown environments. The methodology behind this system can easily be extended to multi-camera rigs, if the onboard computation capability allows this. We analyze the iterative optimizations for pose tracking and map refinement of the SLAM system in multicamera cases. This ensures the soundness and accuracy of each optimization update. Our method is more resistant to tracking failure than conventional monocular visual SLAM systems, especially when MAVs fly in complex environments. It also brings more flexibility to configurations of multiple cameras used onboard of MAVs. We demonstrate its efficiency with both autonomous flight and manual flight of a MAV. The results are evaluated by comparisons with ground truth data provided by an external tracking system.ViewShow abstractTopological structural analysis of digitized binary images by border followingArticleMar 1985Comput Vis Graph Image ProcessSatoshi SuzukiKeiichiA beTwo border following algorithms are proposed for the topological analysis of digitized binary images. The first one determines the surroundness relations among the borders of a binary image. Since the outer borders and the hole borders have a one-to-one correspondence to the connected components of 1-pixels and to the holes, respectively, the proposed algorithm yields a representation of a binary image, from which one can extract some sort of features without reconstructing the image. The second algorithm, which is a modified version of the first, follows only the outermost borders (i.e., the outer borders which are not surrounded by holes). These algorithms can be effectively used in component counting, shrinking, and topological structural analysis of binary images, when a sequential digital computer is used.ViewShow abstractVision based MAV navigation in unknown and unstructured environmentsConference PaperFull-text availableMay 2010 Michael Bloesch Stephan Weiss Davide Scaramuzza Roland SiegwartWithin the research on Micro Aerial Vehicles (MAVs), the field on flight control and autonomous mission execution is one of the most active. A crucial point is the localization of the vehicle, which is especially difficult in unknown, GPS-denied environments. This paper presents a novel vision based approach, where the vehicle is localized using a downward looking monocular camera. A state-of-the-art visual SLAM algorithm tracks the pose of the camera, while, simultaneously, building an incremental map of the surrounding region. Based on this pose estimation a LQG/LTR based controller stabilizes the vehicle at a desired setpoint, making simple maneuvers possible like take-off, hovering, setpoint following or landing. Experimental data show that this approach efficiently controls a helicopter while navigating through an unknown and unstructured environment. To the best of our knowledge, this is the first work describing a micro aerial vehicle able to navigate through an unexplored environment (independently of any external aid like GPS or artificial beacons), which uses a single camera as only exteroceptive sensor.ViewShow abstractThe vSLAM Algorithm for Robust Localization and Mapping.Conference PaperFull-text availableJan 2005 Niklas KarlssonEnrico Di BernardoJ. Ostrowski Mario MunichThis paper presents the Visual Simultaneous Localization and Mapping (vSLAMTM) algorithm, a novel algorithm for simultaneous localization and mapping (SLAM). The algorithm is vision-and odometry-based, and enables low-cost navigation in cluttered and populated environments. No initial map is required, and it satisfactorily handles dynamic changes in the environment, for example, lighting changes, moving objects and/or people. Typically, vSLAM recovers quickly from dramatic disturbances, such as “kidnapping”.ViewShow abstractAutonomous MAV flight in indoor environments using single image perspective cuesConference PaperMay 2011Cooper BillsJoyce ChenAshutosh SaxenaWe consider the problem of autonomously flying Miniature Aerial Vehicles (MAVs) in indoor environments such as home and office buildings. The primary long range sensor in these MAVs is a miniature camera. While previous approaches first try to build a D model in order to do planning and control, our method neither attempts to build nor requires a 3D model. Instead, our method first classifies the type of indoor environment the MAV is in, and then uses vision algorithms based on perspective cues to estimate the desired direction to fly. We test our method on two MAV platforms: a co-axial miniature helicopter and a toy quadrotor. Our experiments show that our vision algorithms are quite reliable, and they enable our MAVs to fly in a variety of corridors and staircases.ViewShow abstractMAV visual SLAM with plane constraintConference PaperFull-text availableMay 2011 Gim Hee Lee Friedrich Fraundorfer Marc PollefeysBundle adjustment (BA) which produces highly accurate results for visual Simultaneous Localization and Map ping (SLAM) could not be used for Micro-Aerial Vehicles (MAVs) with limited processing power because of its O(N<sup>3</sup>) complexity. We observed that a consistent ground plane often exists for MAVs flying in both the indoor and outdoor urban environments. Therefore, in this paper, we propose a visual SLAM algorithm that make use of the plane constraint to reduce the complexity of BA. The reduction of complexity is achieved by refining only the current camera pose and most recent map points with BA that minimizes the reprojection errors and perpendicular distances between the most recent map points and the best fit plane with all the pre-existing map points. As a result, our algorithm is approximately constant time since the number of current camera pose and most recent map points remain approximately constant. In addition, the minimization of the perpendicular distances between the plane and map points would enforce consistency between the reconstructed map points and the actual ground plane.ViewShow abstractShow moreJoin ResearchGate to find the people and research you need to help your work.15+ million members118+ million publications700k+ research projectsJoin for freeRecommended publicationsDiscover more publications, questions and projects in InterfaceArticleTechnology: Machine vision: No failure to communicateJanuary 2009H. HoganGigE Vision Plug Fest, plugging GigE Vision in more ways than one, plug fests test interoperability between different vendors' products is discussed. A single camera programming standard for all the major digital protocols also should finally be realized. The most important development is a prototype of the Camera Link support that is now available. With that extension, the users will enjoy a ... [Show full abstract] standardized GenICam API, application programming interface, and GUI, graphical user interface, for Camera Link devices as they have it already with GigE Vision, 1394 and USB cameras. The next version of the standard will include revisions to cable construction and testing requirements, an update of the Power over Camera Link specification for a medium and full configuration and the introduction of a version of Power over Camera Link in a new 14-pin form suitable for miniaturized applications. The Automated Imaging Association also controls the GigE Vision standard.Read moreChapterMap Maker: Combining Google Maps and the CanvasJanuary 2011 Jeanine MeyerIn this chapter, you will learn how to do the following:
Use the Google Maps API to display a map at a specific location
Draw graphics on a canvas using transparency (also known as the alpha or opacity level) and a customized cursor icon
Provide a graphical user interface (GUI) to your user by combining the use of Google Maps and HTML5 features by managing the events and the z-index ... [Show full abstract] levels
Calculate the distance between two geographical locationsRead moreArticleFull-text availableIRoSim: Industrial Robotics Simulation Design Planning and Optimization Platform based on CAD and Kn...June 2016 · Robotics and Computer-Integrated Manufacturing Khelifa Baizid Saša Ćuković Jamshed Iqbal[...] Ionut Gabriel GhioneaThis paper presents Industrial Robotics Simulation Design Planning and Optimization platform named IRoSim, which is based on SolidWorks Application Programming Interface (API) to offer an intuitive and convertible environment for designing and simulating robotized tasks. The core idea is to integrate features of mechanical CAD and robotics CAD into the same platform to facilitate the development ... [Show full abstract] process through the designed Graphical User Interface (GUI) which permits user friendly interaction. The platform includes various 3D models that are essential for developing any robotized task and offers possibility to integrate new models in simulation. Robotic manipulator library is one such example which contains several types of serial arms with different combinations of revolute and prismatic joints. The platform provides most important steps such as defining the task, CAD learning of the end-effector's trajectory, checking the manipulator's reachability to perform a task, simulating the motion and finally validating the manipulator's trajectory to avoid possible collisions. To demonstrate the efficiency of the proposed approach, two frequent and important tasks (spot welding and painting) using a 6-Degree Of Freedom (DOF) industrial robotic manipulator have been considered. The output of the proposed strategy provides collision-free trajectory of the manipulator's motion which can be directly mapped to a real site. Moreover, the approach permits addressing the problems related with the real implementation of robotized tasks.View full-textChapterIntroducing EclipseDecember 2009Doug AbbottThis chapter is an introduction to what Eclipse is along with its history and current status. Eclipse is more than just an integrated development environment (IDE). It is a framework for building IDEs. Although Eclipse has a lot of built-in functionality, most of that functionality is very generic. Eclipse is largely written in Java, and was originally developed for it. Consequently, it runs on ... [Show full abstract] any machine with a Java Runtime Environment (JRE). The platform's principal role is to provide tool developers with mechanisms to use, and rules to follow, for creating seamlessly integrated tools. These mechanisms are exposed via well defined API interfaces, classes, and methods. The platform also provides useful building blocks and frameworks that facilitate developing new tools. The workbench is the primary user interface for Eclipse and it supplies the structures that allow tools to interact with the user. The Eclipse Platform provides a basic Graphical User Interface (GUI) on top of which plug-ins are added to provide functionality addressing a specific software development problem. The workspace consists of one or more top-level projects, where each project maps to a corresponding directory in the file system. In addition to being accessible from Eclipse, all files in the workspace are directly accessible to the standard programs and tools provided by the underlying operating system. Tools integrated with the Platform are provided with APIs for dealing with workspace resources.Read moreDiscover moreDownload citationWhat type of file do you want? RIS BibTeX Plain TextWhat do you want to download? Citation only Citation and abstractDownloadInterested in research on Interface?Join ResearchGate to discover and stay up-to-date with the latest research from leading experts in Interface and many other scientific topics.Join for freeorDiscover by subject areaRecruit researchersJoin for freeLoginEmail Tip: Most researchers use their institutional email address as their ResearchGate loginPasswordForgot password? Keep me logged inLog inorContinue with LinkedInContinue with GoogleWelcome back! Please log in.Email · HintTip: Most researchers use their institutional email address as their ResearchGate loginPasswordForgot password? Keep me logged inLog inorContinue with LinkedInContinue with GoogleNo account? Sign upAboutNewsCompanyCareersSupportHelp centerFAQBusiness solutionsRecruitingAdvertising© ResearchGate 2019. All rights reserved.ImprintTermsPrivacy